\include{header}

\begin{document}

\lecture{13 --- OpenMP}{\term}{Patrick Lam}

\section*{OpenMP}
\paragraph{Moving on.} Now that we've seen automatic parallelization (and how that works in Solaris Studio
and gcc), let's talk about manual---but compiler-aided---parallelization using OpenMP. 

\paragraph{About OpenMP.} OpenMP (Open Multiprocessing) 
is an API specification which allows you to tell the compiler how you'd 
like your program to be parallelized. Implementations of OpenMP 
include compiler support (present in Intel's compiler, Solaris's 
compiler, {\tt gcc} as of 4.2, and Microsoft Visual C++) as well as a 
runtime library.

You use OpenMP\footnote{More information:
  \url{https://computing.llnl.gov/tutorials/openMP/}} by specifying
directives in the source code. In C and C++, these directives are
pragmas of the form \verb+#pragma omp ...+. There is also OpenMP
syntax for Fortran. 

Here are some benefits of the OpenMP approach:
\begin{itemize}
\item Because OpenMP uses compiler directives, you can easily tell the
  compiler to build a parallel version or a serial version (which it can do by
  ignoring the directives). This can simplify debugging---you
  have some chance of observing differences in behaviour between 
  versions.
\item OpenMP's approach also separates the parallelization
  implementation (inserted by the compiler) from the algorithm
  implementation (which you provide), making the algorithm easier to
  read. Plus, you're not responsible for dealing with thread libraries.
\item The directives apply to limited parts of the code, thus supporting
  incremental parallelization of the program, starting with the hotspots.
\end{itemize}

Let's look at a simple example:
{\small
\begin{verbatim}
  void calc (double *array1, double *array2, int length) {
    #pragma omp parallel for
    for (int i = 0; i < length; i++) {
      array1[i] += array2[i];
    }
  }
\end{verbatim}
}
This \verb+#pragma+ instructs the C compiler to parallelize the
loop. It is the responsibility of the developer to make sure that
the parallelization is safe; for instance, {\tt array1} and {\tt array2}
had better not overlap. You no longer need to supply {\tt restrict}
qualifiers, although it's still not a bad idea. (If you wanted this
to be autoparallelized without OpenMP, you would need to provide
{\tt restrict}.)

OpenMP will always start parallel threads if you tell it to, dividing
the iterations contiguously among the threads.

Let's look at the parts of this \verb+#pragma+.
\begin{itemize}
\item \verb+#pragma omp+ indicates an OpenMP directive;
\item {\tt parallel} indicates the start of a parallel region; and
\item {\tt for} tells OpenMP to run the following {\tt for} loop in parallel.
\end{itemize}
When you run the parallelized program, the runtime library starts
up a number of threads and assigns a subrange of the loop range to 
each of the threads.

\paragraph{Restrictions.} OpenMP places some restrictions on
loops that it's going to parallelize:
\begin{itemize}
\item the loop must be of the form 
\[ \mbox{\tt for (init expression; test expression; increment expression)}; \]
\item the loop variable must be integer (signed or unsigned), pointer, or a C++
random access iterator;
\item the loop variable must be initialized to one end of the range;
\item the loop increment amount must be loop-invariant (constant with respect to the loop body); 
\item the test expression must be one of {\tt >}, {\tt >=}, {\tt <}, or {\tt <=}, and the comparison value (bound) must be loop-invariant.
\end{itemize}

(These restrictions therefore also apply to automatically parallelized
loops.) If you want to parallelize a loop that doesn't meet the 
restriction, restructure it so that it does, as we saw last time.

\paragraph{Runtime effect.} When you compile a program with 
OpenMP directives, the compiler generates code to spawn a \emph{team}
of threads and automatically splits off the worker-thread code into a
separate procedure. The code uses fork-join parallelism, so when the
master thread hits a parallel region, it gives work to the worker
threads, which execute and report back. Then the master thread
continues running, while the worker threads wait for more work.

You can specify the number of threads by setting the
\verb+OMP_NUM_THREADS+ environment variable (adjustable by calling 
\verb+omp_set_num_threads()+), and you can get the
Solaris compiler to tell you what it did by giving it the
options \verb+-xopenmp -xloopinfo+.

\section*{Variable scoping}
When using multiple threads, some variables, like loop counters,
should be thread-local, or \emph{private}, while other variables
should be \emph{shared} between threads. Changes to shared variables
are visible to all threads, while changes to private variables are
visible only to the changing thread. Let's look at the defaults that
OpenMP uses to parallelize the above code.

{ 
\begin{verbatim}
$ er_src parallel-for.o
     1.   void calc (double *array1, double *array2, int length) {
        <Function: calc>
    
    Source OpenMP region below has tag R1
    Private variables in R1: i
    Shared variables in R1: array2, length, array1
     2.     #pragma omp parallel for
    
    Source loop below has tag L1
    L1 autoparallelized
    L1 parallelized by explicit user directive
    L1 parallel loop-body code placed in function _$d1A2.calc along with 0 inner loops
    L1 multi-versioned for loop-improvement:dynamic-alias-disambiguation. 
        Specialized version is L2
     3.     for (int i = 0; i < length; i++) {
     4.       array1[i] += array2[i];
     5.     }
     6.   }
\end{verbatim}
}

We can see that the loop variable {\tt i} is private, while the {\tt
  array1}, {\tt array2} and {\tt length} variables are shared.
Actually, it would be fine for the {\tt length} variable to be either
shared or private, but if it was private, then you would have to copy
in the appropriate initial value. The {\tt array} variables, though, 
need to be shared.

\paragraph{Summary of default rules.} Loop variables are private; 
variables defined in parallel code are private; and variables defined
outside the parallel region are shared.

You can disable the default rules by specifying {\tt default(none)}
on the {\tt parallel} pragma, or you can give explicit scoping:

\verb+   #pragma omp parallel for private(i) shared(length, array1, array2)+


\input{bibliography.tex}

\end{document}
