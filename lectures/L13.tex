\include{header}

\begin{document}

\lecture{13 --- OpenMP}{\term}{Patrick Lam}

\section*{OpenMP}


OpenMP (Open Multiprocessing) 
is an API specification which allows you to tell the compiler how you'd 
like your program to be parallelized. Implementations of OpenMP 
include compiler support (present in Intel's compiler, Solaris's 
compiler, {\tt gcc} as of 4.2, and Microsoft Visual C++) as well as a 
runtime library.

You use OpenMP~\cite{omptutorial} by specifying
directives in the source code. In C and C++, these directives are
pragmas of the form \verb+#pragma omp ...+. There is also OpenMP
syntax for Fortran. 

There are {\bf 16} OpenMP directives. Each directive applies to the immediately-following statement, which is either a single statement or a compound statement {\bf \{  $\cdots$ \}}. Most clauses have a \emph{list} as an argument. 
A list is a comma-separated list of list items. For C and C++, a list item 
is simply a variable name.


Here are some benefits of the OpenMP approach:
\begin{itemize}
\item Because OpenMP uses compiler directives, you can easily tell the
  compiler to build a parallel version or a serial version (which it can do by
  ignoring the directives). This can simplify debugging---you
  have some chance of observing differences in behaviour between 
  versions.
\item OpenMP's approach also separates the parallelization
  implementation (inserted by the compiler) from the algorithm
  implementation (which you provide), making the algorithm easier to
  read. Plus, you're not responsible for dealing with thread libraries.
\item The directives apply to limited parts of the code, thus supporting
  incremental parallelization of the program, starting with the hotspots.
\end{itemize}

Let's look at a simple example of a parallel for loop:
{\small
\begin{lstlisting}[language=C,morekeywords={foreach,pragma,omp,parallel,single,nowait,task,untied,barrier,taskyield}]
  void calc (double *array1, double *array2, int length) {
    #pragma omp parallel for
    for (int i = 0; i < length; i++) {
      array1[i] += array2[i];
    }
  }
\end{lstlisting}
}
This \verb+#pragma+ instructs the C compiler to parallelize the
loop. It is the responsibility of the developer to make sure that
the parallelization is safe; for instance, {\tt array1} and {\tt array2}
had better not overlap. You no longer need to supply {\tt restrict}
qualifiers, although it's still not a bad idea. (If you wanted this
to be autoparallelized without OpenMP, you would need to provide
{\tt restrict}.)

OpenMP will always start parallel threads if you tell it to, dividing
the iterations contiguously among the threads.

Let's look at the parts of this \verb+#pragma+.
\begin{itemize}
\item \verb+#pragma omp+ indicates an OpenMP directive;
\item {\tt parallel} indicates the start of a parallel region; and
\item {\tt for} tells OpenMP to run the following {\tt for} loop in parallel.
\end{itemize}
When you run the parallelized program, the runtime library starts
up a number of threads and assigns a subrange of the loop range to 
each of the threads.

\paragraph{Restrictions.} OpenMP places some restrictions on
loops that it's going to parallelize:
\begin{itemize}
\item the loop must be of the form 
\[ \mbox{\tt for (init expression; test expression; increment expression)}; \]
\item the loop variable must be integer (signed or unsigned), pointer, or a \CPP~ 
random access iterator;
\item the loop variable must be initialized to one end of the range;
\item the loop increment amount must be loop-invariant (constant with respect to the loop body); 
\item the test expression must be one of {\tt >}, {\tt >=}, {\tt <}, or {\tt <=}, and the comparison value (bound) must be loop-invariant.
\end{itemize}

(These restrictions therefore also apply to automatically parallelized
loops.) If you want to parallelize a loop that doesn't meet the 
restriction, restructure it so that it does, as we  did for autoparallelization

\paragraph{Runtime effect.} When you compile a program with 
OpenMP directives, the compiler generates code to spawn a \emph{team}
of threads and automatically splits off the worker-thread code into a
separate procedure. The code uses fork-join parallelism, so when the
master thread hits a parallel region, it gives work to the worker
threads, which execute and report back. Then the master thread
continues running, while the worker threads wait for more work.

You can specify the number of threads by setting the
\verb+OMP_NUM_THREADS+ environment variable (adjustable by calling 
\verb+omp_set_num_threads()+), and you can get the
Solaris compiler to tell you what it did by giving it the
options \verb+-xopenmp -xloopinfo+.

\section*{Variable scoping}
When using multiple threads, some variables, like loop counters,
should be thread-local, or \emph{private}, while other variables
should be \emph{shared} between threads. Changes to shared variables
are visible to all threads, while changes to private variables are
visible only to the changing thread. Let's look at the defaults that
OpenMP uses to parallelize the above code.


OpenMP includes three keywords for variable scope and storage:
\begin{itemize}
        \item private;
        \item shared; and
        \item threadprivate.
\end{itemize}

\paragraph{Private Variables.} You can declare private variables
with the {\tt private} clause. This creates new storage, on a per-thread
basis, for the variable---it does not copy variables. The scope
of the variable extends from the start of the region where the variable
exists to the end of that region; the variable is destroyed afterwards.

Some Pthread pseudocode for private variables:

  \begin{lstlisting}[language=C]
void* run(void* arg) {
    int x;
    // use x
}
  \end{lstlisting}

\paragraph{Shared Variables.} The opposite of a private variable
is a {\tt shared} variable. All threads have access to the same block
of data when accessing such a variable.

The relevant Pthread pseudocode is:
  \begin{lstlisting}[language=C]
int x;

void* run(void* arg) {
    // use x
}
  \end{lstlisting}

\paragraph{Thread-Private Variables.} Finally, OpenMP supports 
{\tt threadprivate} variables. This is like a {\tt private} variable
in that each thread makes a copy of the variable. However, the scope 
is different. Such variables are accessible to the thread in any parallel region.

This example will make things clearer. The OpenMP code:
  \begin{lstlisting}[language=C]
int x;
#pragma omp threadprivate(x)
  \end{lstlisting}
  maps to this Pthread pseudocode:
  \begin{lstlisting}[language=C]
int x;
int x[NUM_THREADS];

void* run(void* arg) {
  // int* id = (int*) arg;
  // use x[*id]
}
  \end{lstlisting}

A variable may not appear in {\bf more than one clause} on the same
directive. (There's an exception for {\tt firstprivate} and {\tt
  lastprivate}, which we'll see later.) By default, variables
declared in regions are private; those outside regions are
shared. (An exception: anything with dynamic storage is
shared).


We can ask the Solaris compiler what it did:
{ 
\begin{lstlisting}[language=C,morekeywords={foreach,pragma,omp,parallel,single,nowait,task,untied,barrier,taskyield}]
$ er_src parallel-for.o
     1.   void calc (double *array1, double *array2, int length) {
        <Function: calc>
    
    Source OpenMP region below has tag R1
    Private variables in R1: i
    Shared variables in R1: array2, length, array1
     2.     #pragma omp parallel for
    
    Source loop below has tag L1
    L1 autoparallelized
    L1 parallelized by explicit user directive
    L1 parallel loop-body code placed in function _$d1A2.calc along with 0 inner loops
    L1 multi-versioned for loop-improvement:dynamic-alias-disambiguation. 
        Specialized version is L2
     3.     for (int i = 0; i < length; i++) {
     4.       array1[i] += array2[i];
     5.     }
     6.   }
\end{lstlisting}
}

We can see that the loop variable {\tt i} is private, while the {\tt
  array1}, {\tt array2} and {\tt length} variables are shared.
Actually, it would be fine for the {\tt length} variable to be either
shared or private, but if it was private, then you would have to copy
in the appropriate initial value. The {\tt array} variables, though, 
need to be shared.

\paragraph{Summary of default rules.} Loop variables are private; 
variables defined in parallel code are private; and variables defined
outside the parallel region are shared.

You can disable the default rules by specifying {\tt default(none)}
on the {\tt parallel} pragma, or you can give explicit scoping:

\verb+   #pragma omp parallel for private(i) shared(length, array1, array2)+

\section*{Directives}
We'll talk about the different OpenMP directives next. These are the key
language features you'll use to tell OpenMP what to parallelize.

\subsection*{Parallel}
  \begin{center}
    {\tt \#pragma omp }{\bf parallel} {\it [clause [[,] clause]*]}
  \end{center}

This is the most basic directive in OpenMP.
It tells OpenMP to form a team of threads and start parallel execution.
The thread that enters the region becomes the {\bf master} (thread 0).

Allowed Clauses: {\bf if, num\_threads, default, private, firstprivate,
    shared, copyin, reduction}.

The figure below illustrates what {\tt parallel} does.
By default, the number of threads used is set globally, either automatically
or manually. After the parallel block, the thread team sleeps until it's needed again.

  \tikzstyle{brace} = [decorate,decoration={brace},thick]
  \tikzstyle{transition} = [rectangle, draw=blue!50, fill=blue!20, thick,
                            text width=6.75cm, align=center]
  \tikzstyle{thread} = [->, very thick]
  \tikzstyle{master} = [->, very thick, red]


\begin{center}
  \begin{tikzpicture}[decoration={brace}]

    \node[transition] (begin) {{\tt \#pragma omp parallel}\\begin block};
    \node[transition] (end) [below=of begin] {end block};

    \draw[master]   (   0,   1.55) to (   0,   0.55);

    \draw[master]   (-3.5, -0.525) to (-3.5, -1.525);
    \draw[thread]   (-2.5, -0.525) to (-2.5, -1.525);
    \draw[thread]   (-1.5, -0.525) to (-1.5, -1.525);
    \draw[thread]   (-0.5, -0.525) to (-0.5, -1.525);
    \draw[thread]   ( 0.5, -0.525) to ( 0.5, -1.525);
    \draw[thread]   ( 1.5, -0.525) to ( 1.5, -1.525);
    \draw[thread]   ( 2.5, -0.525) to ( 2.5, -1.525);
    \draw[thread]   ( 3.5, -0.525) to ( 3.5, -1.525);

    \draw[master]   (   0,  -2.08) to (   0,  -3.08);

    \draw[decorate, thick, red] (-4,  -3.08) -- (-4,  -2.08)
      node[red, anchor=east, midway, outer sep=0.1cm] {Master Thread};
    \draw[decorate, thick]      (-4, -1.525) -- (-4, -0.525)
      node[anchor=east, midway, outer sep=0.1cm] {Thread Team};
    \draw[decorate, thick, red] (-4,   0.55) -- (-4,   1.55)
      node[red, anchor=east, midway, outer sep=0.1cm] {Master Thread};
\end{tikzpicture}
\end{center}


\paragraph{Code Example: Parallel.}
  \begin{lstlisting}[language=C]
#pragma omp parallel
{
    printf("Hello!");
}
  \end{lstlisting}

  If the number of threads is 4, this produces:
  \begin{lstlisting}[language=C]
Hello!
Hello!
Hello!
Hello!
  \end{lstlisting}

\paragraph{{\tt if} and {\tt num\_threads} Clauses.} Directives take clauses.
The first one we'll see (besides the variable scope clauses) are {\tt if} and
{\tt num\_threads}.

  \begin{center}
    {\bf if}({\it primitive-expression})    
  \end{center}
The {\tt if} clause allows you to control at runtime whether or not to run
multiple threads or just one thread in its associated parallel section.
If {\it primitive-expression} evaluates to {\tt 0}, i.e. {\tt false}, then
only one thread will execute in the parallel section. (It's what you would expect.)


If the parallel section is going to run multiple threads (e.g. {\tt if} expression
is true; or if there is no {\tt if} expression), we can then specify how many threads.
  \begin{center}
    {\bf num\_threads}({\it integer-expression})    
  \end{center}

This spawns at most {\bf num\_threads}, depending on the number of
threads available.  It can only guarantee the number of threads
requested if {\bf dynamic adjustment} for number of threads is off and
enough threads aren't busy.

\subsection*{Reductions}
Recall that we introduced the concept of a reduction, e.g.
{\small
\begin{lstlisting}[language=C,morekeywords={foreach,pragma,omp,parallel,single,nowait,task,untied,barrier,taskyield}]
  for (int i = 0; i < length; i++) total += array[i];
\end{lstlisting}
}
What is the appropriate scope for {\tt total}? We want each thread
to be able to write to it, but we don't want race conditions.
Fortunately, OpenMP can deal with reductions as a special case:

\verb!   #pragma omp parallel for reduction (+:total)!

\noindent
specifies that the {\tt total} variable is the accumulator for a
reduction over {\tt +}. OpenMP will create local copies of {\tt total} and 
combine them at the end of the parallel region.

{\bf Operators (and their associated initial value)}
  \begin{center}
    \begin{tabular}{l r | l r | l r | l r | l r}
      + & (0) & -  &  (0) &    $\mid$ & (0) & \&\& & (1) & max & MAX\\
      * & (1) & \& & (\~{}0) & \^{} & (0) &   $\mid\mid$ & (0) & min & MIN\\ 
    \end{tabular}
  \end{center}

\subsection*{(For) Loop Directive} Inside a parallel section, we can
specify a for loop clause, as follows.

  \begin{center}
    {\tt \#pragma omp }{\bf for} {\it [clause [[,] clause]*]}
  \end{center}

    Iterations of the loop will be distributed among the
      current team of threads.
    This clause only supports simple ``for'' loops with invariant bounds (bounds do
      not change during the loop).
    Loop variable is implicitly private; OpenMP sets the
      correct values.

  Allowed Clauses: {\bf private, firstprivate, lastprivate, reduction, schedule,
    collapse, ordered, nowait}.

\paragraph{The {\tt schedule} Clause.}

  \begin{center}
    {\bf schedule}({\it kind[, chunk\_size]})
  \end{center}

    The {\bf chunk\_size} is the number of iterations a thread
      should handle at a time. {\bf kind} is one of:
      \begin{itemize}
        \item {\bf static}:  divides the number of iterations into chunks and assigns each thread
        a chunk in round-robin fashion (before the loop executes).

        \item {\bf dynamic}: divides the number of iterations into chunks and assigns each
        available thread a chunk, until there are no chunks left.
        \item {\bf guided}:  same as dynamic, except {\bf chunk\_size} represents the minimum
        size. This starts by dividing the loop into large chunks, and decreases the
        chunk size as fewer iterations remain.
        \item {\bf auto}: obvious (OpenMP decides what's best for you).
        \item {\bf runtime}: also obvious; we'll see how to adjust this later.
      \end{itemize}

OpenMP tries to guess how many iterations to distribute to each thread
in a team. The default mode is called \emph{static scheduling}; in
this mode, OpenMP looks at the number of iterations it needs to run,
assumes they all take the same amount of time, and distributes them
evenly. So for 100 iterations and 2 threads, the first thread gets 50
iterations and the second thread gets 50 iterations.

This assumption doesn't always hold; consider, for instance, the 
following (contrived) code:
{\small
\begin{lstlisting}[language=C,morekeywords={foreach,pragma,omp,parallel,single,nowait,task,untied,barrier,taskyield}]
  double calc(int count) {
    double d = 1.0;
    for (int i = 0; i < count*count; i++) d += d;
    return d;
  }

  int main() {
    double data[200][200];
    int i, j;
    #pragma omp parallel for private(i, j) shared(data)
    for (int i = 0; i < 200; i++) {
      for (int j = 0; j < 200; j++) {
        data[i][j] = calc(i+j);
      }
    }
    return 0;
  }
\end{lstlisting}
}
This code gives sublinear scaling, because the earlier iterations 
finish faster than the later iterations, and the program needs to wait
for all iterations to complete.

Telling OpenMP to use a \emph{dynamic schedule} can enable better
parallelization: the runtime distributes work to each thread in
chunks, which results in less waiting. Just add {\tt
  schedule(dynamic)} to the pragma. Of course, this has more overhead,
since the threads need to solicit the work, and there is a potential
serialization bottleneck in soliciting work from the single work
queue. 

The default chunk size is 1, but you can specify it yourself, either
using a constant or a value computed at runtime, e.g. 
{\tt schedule(dynamic, n/50)}. Static scheduling also accepts a
chunk size.

OpenMP has an even smarter work distribution mode, {\tt guided}, where
it changes the chunk size according to the amount of work remaining.
You can specify a minimum chunk size, which defaults to 1. There
are also two meta-modes, {\tt auto}, which leaves it up to OpenMP, and
{\tt runtime}, which leaves it up to the \verb+OMP_SCHEDULE+ environment
variable.



\paragraph{{\tt collapse} and {\tt ordered} Clauses.}

  \begin{center}
    {\bf collapse}({\it n})
  \end{center}

    This collapses {\it n} levels of loops. Obviously, this only has
    an effect if $n \ge 2$; otherwise, nothing happens. Collapsed loop
    variables are also made private.

  \begin{center}
    {\bf ordered}
  \end{center}
  
    This enables the use of {\tt ordered} directives inside loop, which we'll see below.

\subsection*{Ordered directive}
  \begin{center}
    {\tt \#pragma omp }{\bf ordered}
  \end{center}

    To use this directive, the containing loop must have an {\bf ordered} clause.
    OpenMP will ensure that the ordered directives are executed the same
      way the sequential loop would (one at a time).
    Each iteration of the loop may execute {\bf at most one} ordered
      directive.

Let's see what that means by way of two examples.

\paragraph{Invalid Use of Ordered.} This doesn't work.

{\small
  \begin{lstlisting}
void work(int i) {
  printf("i = %d\n", i);
}
...
int i;
#pragma omp for ordered
for (i = 0; i < 20; ++i) {

  #pragma omp ordered
  work(i);

  // Each iteration of the loop has 2 "ordered" clauses!
  #pragma omp ordered 
  work(i + 100);
}
  \end{lstlisting}
}

\paragraph{Valid Use of Ordered.} This does.
{\small  \begin{lstlisting}
void work(int i) {
  printf("i = %d\n", i);
}
...
int i;
#pragma omp for ordered
for (i = 0; i < 20; ++i) {
  if (i <= 10) {
    #pragma omp ordered
    work(i);
  }
  if (i > 10) {
    // two ordered clauses are mutually-exclusive
    #pragma omp ordered
    work(i+100);
  }
}
  \end{lstlisting}
}

{\bf Note:} if we change {\tt i \textgreater\- 10} to {\tt i \textgreater\- 9}, 
the use becomes invalid because the iteration $i=9$ contains two {\tt ordered}
directives.

  \paragraph{Tying It All Together.} Here's a larger example.

  \begin{lstlisting}
#include <omp.h>
#include <stdio.h>

int main(int argc, char *argv[])
{
    int j, k, a;
    #pragma omp parallel num_threads(2)
    {
        #pragma omp for collapse(2) ordered private(j,k) \
                        schedule(static,3)
        for (k = 1; k <= 3; ++k)
            for (j = 1; j <= 2; ++j) {
                #pragma omp ordered
                printf("t[%d] k=%d j=%d\n",
                       omp_get_thread_num(),
                       k, j);
            }
    }
    return 0;
}
  \end{lstlisting}

\paragraph{Output of Previous Example.} And here's what it does.

{\small
  \begin{lstlisting}
    t[0] k=1 j=1
    t[0] k=1 j=2
    t[0] k=2 j=1
    t[1] k=2 j=2
    t[1] k=3 j=1
    t[1] k=3 j=2
  \end{lstlisting}
}

  {\bf Note:} the output will be determinstic; still, the program will run two threads as
  long as the thread limit is at least 2.

This directive is shorthand:

  \begin{center}
    {\tt \#pragma omp }{\bf parallel for} {\it [clause [[,] clause]*]}
  \end{center}

We could equally well write:
{\small
  \begin{lstlisting}
#pragma omp parallel
{
    #pragma omp for
    {

    }
}
  \end{lstlisting}
}
but the single directive is shorter; this idiom happens a lot. But if you forget the parallel, you don't get the behaviour you expect: no team of threads!

Allowed Clauses: everything allowed by {\tt parallel} and {\tt for}, except
  {\bf nowait}.

\subsection*{Sections}
Another OpenMP parallelism idiom is sections.
  \begin{center}
    {\tt \#pragma omp }{\bf sections} {\it [clause [[,] clause]*]}
  \end{center}

  Allowed Clauses: {\bf private, firstprivate, lastprivate, reduction, nowait}.

Each {\bf sections} directive must contain one or more {\bf section} directive:
  \begin{center}
    {\tt \#pragma omp }{\bf section}
  \end{center}
  
Sections distributed among current team of threads.
They statically limit parallelism to the number of
      sections which are lexically in the code.

\paragraph{Parallel Sections.} Another common idiom.
  \begin{center}
    {\tt \#pragma omp }{\bf parallel sections} {\it [clause [[,] clause]*]}
  \end{center}

  As with parallel for, this is basically shorthand for:
  
  \begin{lstlisting}
#pragma omp parallel
{
    #pragma omp sections
    {

    }
}
  \end{lstlisting}

  Allowed Clauses: everything allowed by {\tt parallel} and {\tt sections}, except
  {\bf nowait}.

  \subsection*{Single} When we'd be otherwise running many threads, we can state
that some particular code block should be run by only one method.

  \begin{center}
    {\tt \#pragma omp }{\bf single}
  \end{center}
    Only a single thread executes the region following the directive.
    The thread is not guaranteed to be the master thread.

  Allowed Clauses: {\bf private, firstprivate, copyprivate, nowait}.
 Must not use {\bf copyprivate} with {\bf nowait}


  \subsection*{Master} Sometimes we do want to guarantee that only the master
thread runs some code.
  \begin{center}
    {\tt \#pragma omp }{\bf master}
  \end{center}

    This is similar to the {\bf single} directive, except that the master thread (and only the master thread) is guaranteed to enter this region.
    No implied barriers. 

Also, no clauses.

 \subsection*{Barrier}

  \begin{center}
    {\tt \#pragma omp }{\bf barrier}
  \end{center}

     Waits for all the threads in the team to reach the barrier before
      continuing. In other words, this constitutes a synchronization point.
     Loops, sections, and single all have an implicit barrer at the end of their
      region (unless you use {\bf nowait}).
     Note that, although it's always good practice to put statements following a {\tt \#pragma omp} inside a compound block ({\tt \{ \ldots \}}), a barrier inside an {\tt if} statement must be {\tt \{  \}} separated.

     This mechanism is analogous to {\tt pthread\_barrier} in pthreads.

  \subsection*{Critical} We turn our attention to synchronization constructs.
First, let's examine critical.
  \begin{center}
    {\tt \#pragma omp }{\bf critical} {\it [(name)]}
  \end{center}

    The enclosed region is guaranteed to only run one thread at a time
      (on a per-name basis).
    This is the same as a block of code in Pthreads surrounded by a {\tt mutex} lock
      and unlock.

\subsection*{Atomic}
We can also request atomic operations:

    {\tt \#pragma omp }{\bf atomic} [{\bf read $\mid$ write $\mid$ update $\mid$ capture}]\\
    {\it expression-stmt}
    
This ensures that a specific storage location is updated
    atomically.  Atomics should be more efficient than using critical
    sections (or else why would they include it?)

\section*{A Warning About Using OpenMP Directives}
  Write your code so that simply eliding OpenMP directives gives a valid program. For instance, this is wrong:
  \begin{lstlisting}[language=C,morekeywords={foreach,pragma,omp,parallel,single,nowait,task,untied,barrier,taskyield}]
if (a != 0)
    #pragma omp barrier // wrong!
if (a != 0)
    #pragma omp taskyield // wrong!
  \end{lstlisting}

  Use this instead:
  \begin{lstlisting}[language=C,morekeywords={foreach,pragma,omp,parallel,single,nowait,task,untied,barrier,taskyield}]
if (a != 0) {
    #pragma omp barrier
}
if (a != 0) {
    #pragma omp taskyield
}
  \end{lstlisting}



\input{bibliography.tex}

\end{document}
