\include{header}

\begin{document}

\lecture{22 --- GPU Programming Continued }{\term}{Patrick Lam \& Jeff Zarnett}

\section*{GPUs: Heterogeneous Programming}

\section*{Host Code}
We've learned about how a kernel works and a bit about how to write one. The next part is the host code, Now, fortunately, we don't have to write the whole program in \CPP~ or C, even though the kernel has to be written in the CUDA variant. We're going to use the Rustacuda library from \url{https://github.com/bheisler/RustaCUDA}. 


We'll look at a quick example of launching a very simple kernel from the Rustacuda examples\footnote{\url{https://github.com/bheisler/RustaCUDA/blob/master/examples/launch.rs}}: 
\begin{lstlisting}[language=Rust]
#[macro_use]
extern crate rustacuda;

use rustacuda::prelude::*;
use std::error::Error;
use std::ffi::CString;

fn main() -> Result<(), Box<dyn Error>> {
    // Set up the context, load the module, and create a stream to run kernels in.
    rustacuda::init(CudaFlags::empty())?;
    let device = Device::get_device(0)?;
    let _ctx = Context::create_and_push(ContextFlags::MAP_HOST | ContextFlags::SCHED_AUTO, device)?;

    let ptx = CString::new(include_str!("../resources/add.ptx"))?;
    let module = Module::load_from_string(&ptx)?;
    let stream = Stream::new(StreamFlags::NON_BLOCKING, None)?;

    // Create buffers for data
    let mut in_x = DeviceBuffer::from_slice(&[1.0f32; 10])?;
    let mut in_y = DeviceBuffer::from_slice(&[2.0f32; 10])?;
    let mut out_1 = DeviceBuffer::from_slice(&[0.0f32; 10])?;
    let mut out_2 = DeviceBuffer::from_slice(&[0.0f32; 10])?;

    // This kernel adds each element in `in_x` and `in_y` and writes the result into `out`.
    unsafe {
        // Launch the kernel with one block of one thread, no dynamic shared memory on `stream`.
        let result = launch!(module.sum<<<1, 1, 0, stream>>>(
            in_x.as_device_ptr(),
            in_y.as_device_ptr(),
            out_1.as_device_ptr(),
            out_1.len()
        ));
        result?;

        // Launch the kernel again using the `function` form:
        let function_name = CString::new("sum")?;
        let sum = module.get_function(&function_name)?;
        // Launch with 1x1x1 (1) blocks of 10x1x1 (10) threads, to show that you can use tuples to
        // configure grid and block size.
        let result = launch!(sum<<<(1, 1, 1), (10, 1, 1), 0, stream>>>(
            in_x.as_device_ptr(),
            in_y.as_device_ptr(),
            out_2.as_device_ptr(),
            out_2.len()
        ));
        result?;
    }

    // Kernel launches are asynchronous, so we wait for the kernels to finish executing.
    stream.synchronize()?;

    // Copy the results back to host memory
    let mut out_host = [0.0f32; 20];
    out_1.copy_to(&mut out_host[0..10])?;
    out_2.copy_to(&mut out_host[10..20])?;

    for x in out_host.iter() {
        assert_eq!(3.0 as u32, *x as u32);
    }

    println!("Launched kernel successfully.");
    Ok(())
}
\end{lstlisting}

\paragraph{Walk-through.} Let's look at all of the code in the example and
explain the terms. For a more detailed explanation of all the steps, see~\cite{cuda}.

 1) First, we request a \emph{platform}.
Platforms, also known as hosts, contain 2) OpenCL \emph{compute devices},
which may in turn contain multiple compute units. Note that we could
also request a CPU device in step 2, without changing the rest of the code.

Next, in step 3, we request an OpenCL \emph{context} (representing all
OpenCL state) and create a \emph{command-queue}. We will request that
OpenCL do work by telling it to run a kernel in the queue.

In step 4, we create an OpenCL \emph{program}. This is a confusing
term; an OpenCL program is what runs on the compute unit, and includes
kernels, functions, and declarations. Your application can contain
more than one OpenCL program. In this case, we create a program
from the C string {\tt source}, which contains the kernel
{\tt memset}. OpenCL can also create programs from binaries, which may be
in an intermediate representation, or already compiled for a particular
device. We get a pointer to the kernel in this step, as the return
value from {\tt clCreateKernel}.

There's one more step before launching the kernel; in step 5, we
create a \emph{data buffer}, which enables communication between
devices. Recall that OpenCL requires explicit communication,
which we'll see later. Since this example doesn't have input, we 
don't need to put anything into the buffer initially.

Finally, we can launch the kernel in step 6. In this case, we don't
specify anything about workgroups, but enqueue the entire
1-dimensional index space, starting at $(0)$. We also state that the
index space has {\tt NWITEMS} elements, and not to subdivide the
problem into work-items. The last three parameters are about events.
We call {\tt clFinish()} to wait for the command-queue to empty.

Finally, in step 7, we copy the results back from the shared buffer
using {\tt clEnqueueMapBuffer}. This copy is blocking (first {\tt
  CL\_TRUE} argument), so we don't need an explicit {\tt clFinish()}
call. We also indicate the details of the command we'd like to
run: in particular, a read of {\tt NWITEMS} from the buffer.

You might also want to consider cleaning up the objects you've
allocated; I haven't shown that here. The code also doesn't contain
any error-handling.



\input{bibliography.tex}

\end{document}
