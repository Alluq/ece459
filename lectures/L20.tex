\include{header}

\begin{document}

\lecture{20 --- Compiler Optimizations}{\term}{Patrick Lam}

\section*{Compiler Optimizations}

\hfill ``Is there any such thing as a free lunch?''

Compiler optimizations really do feel like a free lunch.
But what does it really mean when you say {\tt -O2}?
We'll see some representative compiler optimizations and discuss how
they can improve program performance. Because we're talking about
Programming for Performance, I'll point out cases that stop compilers
from being able to optimize your code. In general, it's better if the
compiler automatically does a performance-improving transformation
rather than you doing it manually; it's probably a waste of time for
you and it also makes your code less readable.

Many pages on the Internet describe
optimizations. Here's one that contains good examples:

$\qquad \qquad$ \url{http://www.digitalmars.com/ctg/ctgOptimizer.html}

You can find a full list of {\tt gcc} options here:

$\qquad \qquad$ \url{http://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html}

\paragraph{About Compiler Optimizations.} First of all, ``optimization'' is
a bit of a misnomer, since compilers generally do not generate ``optimal'' code.
They just generate \emph{better} code.

Often, what happens is that the program you literally wrote is too slow. The
contract of the compiler (working with the architecture) is to actually execute
a program with the same behaviour as yours, but which runs faster.

\paragraph{{\tt gcc} optimization levels.} Here's what {\tt -On} means for
{\tt gcc}. Other compilers have similar (but not identical) optimization flags.

\begin{itemize}[noitemsep]
\item {\tt -O0} (default): Fastest compilation time. Debugging works as expected.
\item {\tt -O1} ({\tt -O}): Reduce code size and execution time.
 No optimizations that increase compiliation time.
\item {\tt -O2}: All optimizations except space vs. speed tradeoffs.
\item {\tt -O3}: All optimizations.
\item {\tt -Ofast}: All {\tt -O3} optimizations, plus non-standards compliant optimizations,
      particularly {\tt -ffast-math}. (Like {\tt -fast} on the Solaris compiler.)\\[1em]
  This flag turns off exact implementations of IEEE or ISO rules/specifications for math
  functions. Generally, if you don't care about the exact result, you can use this for
  a speedup.
\end{itemize}

\subsection*{Scalar Optimizations}
By scalar optimizations, I mean optimizations
which affect scalar (non-array) operations. Here are some examples of scalar
optimizations.

\paragraph{Constant folding.} Probably the simplest optimization one can think of.
Tag line: ``Why do later something you can do now?'' We simply translate:

\begin{center}
\vspace*{-1em}
\begin{tabular}{lll}
i = 1024 * 1024 &
$\Longrightarrow$ &
i = 1048576
\end{tabular}
\end{center}

\noindent \emph{Enabled at all optimization levels.} The compiler will not emit
code that does the multiplication at runtime. It will simply use the
computed value.

\paragraph{Common subexpression elimination.} We can do common subexpression elimination
when the same expression {\tt x op y} is computed more than once, and
neither {\tt x} nor {\tt y} change between the two computations. In the
below example, we need to compute {\tt c + d} only once.


\begin{lstlisting}[language=C]
   a = (c + d) * y;
   b = (c + d) * z;

   w = 3;
   x = f(); y = x;
   z = w + y;

\end{lstlisting}


\noindent \emph{Enabled at {\tt -O2}, {\tt -O3} or with {\tt -fgcse}.} 
These flags actually enable a global (i.e. across-basic-blocks) CSE pass.
This also enables global constant and copy propagation.

\paragraph{Constant propagation.} Moves constant values from definition to
use. The transformation is valid if there are no redefinitions of the
variable between the definition and its use. In the above example,
we can propagate the constant value 3 to its use in {\tt z = w + y},
yielding {\tt z = 3 + y}.

\paragraph{Copy propagation.} A bit more sophisticated than constant
propagation---telescopes copies of variables from their definition to
their use. This usually runs after CSE. Using it, we can replace the
last statement with {\tt z = w + x}. If we run both constant and copy
propagation together, we get {\tt z = 3 + x}.

These scalar optimizations are more complicated in the presence
of pointers, e.g. {\tt z = *w + y}. More next time.

%\paragraph{Scalar Replacement of Aggregates.} Censored. Too many people misunderstood it last year.

\paragraph{Redundant Code Optimizations.} In some sense, most optimizations
remove redundant code, but one particular optimization is \emph{dead code
elimination}, which removes code that is guaranteed to not execute.
For instance:

{\scriptsize
\begin{center}
\vspace*{-2em}
\begin{minipage}{.3\textwidth}
\begin{lstlisting}[language=C]
  int f(int x) {
    return x * 2;
  }
  \end{lstlisting}
  \end{minipage} \begin{minipage}{.3\textwidth}
\begin{lstlisting}[language=C]
  int g() {
    if (f(5) % 2 == 0) {
      // do stuff...
    } else {
      // do other stuff
    }
  }
\end{lstlisting}
\end{minipage}
\end{center}
}
We see that the then-branch in {\tt g()} is always going to execute, and the
else-branch is never going to execute.

The general problem, as with many other compiler problems, is undecidable. Let's not get too caught up in the semantics of the \textit{Entscheidungsproblem}, even if you do speak German and like to show it off by pronouncing that word correctly.

\subsection*{Loop Optimizations}
Loop optimizations are particularly profitable when loops execute
often. This is often a win, because programs spend a lot of time looping.
The trick is to find which loops are going to be the important ones.
Profiling is helpful.

A loop induction variable is a variable that varies on each iteration
of the loop; the loop variable is definitely a loop induction variable,
but there may be others. \emph{Induction variable elimination} gets
rid of extra induction variables.

\emph{Scalar replacement} replaces an array read {\tt a[i]}
occuring multiple times with a single read {\tt temp = a[i]} and references
to {\tt temp} otherwise. It needs to know that {\tt a[i]} won't change
between reads.

Sane languages include array bounds checks, and loop optimizations
can eliminate array bounds checks if they can prove that the loop
never iterates past the array bounds.

\paragraph{Loop unrolling.} This optimization
lets the processor run more code without having to branch
as often. \emph{Software pipelining} is a synergistic optimization,
which allows multiple iterations of a loop to proceed in parallel.
This optimization is also useful for SIMD. Here's an example.
\begin{center}
\vspace*{-1em}
\begin{minipage}{.3\textwidth}
  \begin{lstlisting}[language=C]
for (int i = 0; i < 4; ++i)
    f(i)
  \end{lstlisting}
  \end{minipage} $\Longrightarrow \hspace*{2em}$ \begin{minipage}{.4\textwidth}
  \begin{lstlisting}[language=C]
f(0); f(1); f(2); f(3);
  \end{lstlisting}
  \end{minipage}
  \end{center}
\noindent \emph{Enabled with {\tt -funroll-loops}.}

\paragraph{Loop interchange.} This optimization can give big wins
for caches (which are key); it changes the nesting of loops to
coincide with the ordering of array elements in memory. For instance,
in C:
\begin{center}
\vspace*{-1em}
\begin{minipage}{.3\textwidth}
  \begin{lstlisting}[language=C]
for (int i = 0; i < N; ++i)
    for (int j = 0; j < M; ++j)
        a[j][i] = a[j][i] * c
  \end{lstlisting}
  \end{minipage} $\Longrightarrow \hspace*{2em}$ \begin{minipage}{.4\textwidth}
  \begin{lstlisting}[language=C]
for (int j = 0; j < M; ++j)
    for (int i = 0; i < N; ++i)
        a[j][i] = a[j][i] * c
  \end{lstlisting}
  \end{minipage}
  \end{center}
  since C is \emph{row-major} (meaning a[1][1] is beside a[1][2]),
rather than \emph{column-major}.

\noindent
\emph{Enabled with {\tt -floop-interchange}.}

Strangely enough, sometimes you want to do things the column-major way even though it's ``wrong''. If your two dimensional array is of an appropriate size then by intentionally hitting things in the ``wrong'' order, you'll trigger all your page faults up front and load all your pages into cache and then you can go wild. This was suggested as a way to make matrix multiplication faster for a sufficiently large matrix...

\paragraph{Loop fusion.} This optimization is like the OpenMP collapse
construct; we transform
\begin{center}
\vspace*{-1em}
\begin{minipage}{.3\textwidth}
  \begin{lstlisting}[language=C]
for (int i = 0; i < 100; ++i)
    a[i] = 4

for (int i = 0; i < 100; ++i)
    b[i] = 7
  \end{lstlisting}
  \end{minipage} $\Longrightarrow \hspace*{2em}$ \begin{minipage}{.4\textwidth}
  \begin{lstlisting}[language=C]
for (int i = 0; i < 100; ++i) {
    a[i] = 4
    b[i] = 7
}
  \end{lstlisting}
  \end{minipage}
  \end{center}
There's a trade-off between data locality and loop overhead; hence,
sometimes the inverse transformation, \emph{loop fission}, will
improve performance.

\paragraph{Loop-invariant code motion.} Also known as \emph{Loop hoisting},
this optimization moves calculations out of a loop. 
\begin{center}
\vspace*{-1em}
\begin{minipage}{.3\textwidth}
  \begin{lstlisting}[language=C]
for (int i = 0; i < 100; ++i) {
    s = x * y;
    a[i] = s * i;
}
  \end{lstlisting}
  \end{minipage} $\Longrightarrow \hspace*{2em}$ \begin{minipage}{.4\textwidth}
  \begin{lstlisting}[language=C]
s = x * y;
for (int i = 0; i < 100; ++i) {
    a[i] = s * i;
}
  \end{lstlisting}
  \end{minipage}
  \end{center}

This reduces the amount of work we have to do for each iteration of the loop.


\subsection*{Miscellaneous Low-Level Optimizations}
Some optimizations affect low level code generation; here are two examples.

\paragraph{Branch Prediction.} {\tt gcc} attempts to guess the probability of each branch to
best order the code. (For an {\tt if}, fall-through is most efficient. Why?)

This isn't quite an optimization, but you can use {\tt
  \_\_builtin\_expect(expr, value)} to help GCC, if you know the
run-time characteristics of your program. An example, from the
  Linux kernel:

  \begin{lstlisting}[language=C]
#define likely(x)       __builtin_expect((x),1)
#define unlikely(x)     __builtin_expect((x),0)
  \end{lstlisting}

\paragraph{Architecture-Specific.} {\tt gcc} can also generate code tuned to particular
processors and processor variants. You can specify this using {\tt
  -march} and {\tt -mtune}. ({\tt -march} implies {\tt -mtune}).
This will enable specific instructions that not all CPUs support (e.g. SSE4.2).
For example, {\tt -march=corei7}.

\noindent
Good to use on your local machine or your cloud servers, not ideal for code you ship to others.


\section*{Interprocedural Analysis and Link-Time Optimizations}
\hfill ``Are economies of scale real?''

In this context, does a
whole-program optimization really improve your program?
We'll start by first talking about some information that is critical for
whole-program optimizations.

\subsection*{Alias and Pointer Analysis}
As we've seen in the above analyses, compiler optimizations often need
to know about what parts of memory each statement reads to.  This is
easy when talking about scalar variables which are stored on the
stack. This is much harder when talking about pointers or arrays
(which can alias). \emph{Alias analysis} helps by declaring that a
given variable {\tt p} does not alias another variable {\tt q}; that
is, they point to different heap locations. \emph{Pointer analysis}
abstractly tracks what regions of the heap each variable points to.
A region of the heap may be the memory allocated at a particular
program point.

When we know that two pointers don't alias, then we know that their
effects are independent, so it's correct to move things around.
This also helps in reasoning about side effects and enabling reordering.

We've talked about automatic parallelization previously in this course.
At this point, I'll remind you that we used {\tt restrict} so that the
compiler wouldn't have to do as much pointer analysis. Shape analysis
builds on pointer analysis to determine that data structures are indeed
trees rather than lists.

\paragraph{Call Graphs.} Many interprocedural analyses require accurate
call graphs. A call graph is a directed graph showing relationships between
functions. It's easy to compute a call graph when you have C-style
function calls. It's much harder when you have virtual methods, as in
C++ or Java, or even C function pointers. In particular, you need pointer
analysis information to construct the call graph.

\paragraph{Devirtualization.} This optimization attempts to convert
virtual function calls to direct calls.  Virtual method calls have the
potential to be slow, because there is effectively a branch to
predict. If the branch prediction goes well, then it doesn't impose
more runtime cost. However, the branch prediction might go poorly.  (In
general for C++, the program must read the object's vtable.) Plus, virtual
calls impede other optimizations. Compilers can help by doing
sophisticated analyses to compute the call graph and by replacing
virtual method calls with nonvirtual method calls.  Consider the
following code:
  \begin{lstlisting}[language=C]
class A {
  public:
    virtual void m();
};

class B : public A {
  public:
    virtual void m() {}
};

int main(int argc, char *argv[]) {
    std::unique_ptr<A> t(new B);
    t->m();
}
  \end{lstlisting}
Devirtualization could eliminate vtable access; instead, we could just call B's {\tt m} method
directly. By the way, ``Rapid Type Analysis'' analyzes the entire program, observes that
only {\tt B} objects are ever instantiated, and enables devirtualization
of the {\tt b.m()} call.

\noindent \emph{Enabled with {\tt -O2}, {\tt -O3}, or with {\tt -fdevirtualize}.}

\paragraph{Inlining.} We have seen the notion of inlining:
  \begin{itemize}
    \item Instructs the compiler to just insert the function code in-place,
      instead of calling the function.
    \item Hence, no function call overhead!
    \item Compilers can also do better---context-sensitive---operations they couldn't
      have done before.
  \end{itemize}

OK, so inlining removes overhead. Sounds like better performance! Let's inline everything!
There are two ways of inlining in C++.

Implicit Inlining (defining a function inside a class definition):
  \begin{lstlisting}[language=C]
class P {
public:
    int get_x() const { return x; }
...
private:
    int x;
};
  \end{lstlisting}

Explicit Inlining:
  \begin{lstlisting}[language=C]
inline max(const int& x, const int& y) {
    return x < y ? y : x;
}
  \end{lstlisting}

\paragraph{The Other Side of Inlining.}
Inlining has one big downside:
  \begin{itemize}
    \item Your program size is going to increase.
  \end{itemize}
   This is worse than you think:
      \begin{itemize}
        \item Fewer cache hits.
        \item More trips to memory.
      \end{itemize}
   Some inlines can grow very rapidly (C++ extended constructors).
  Just from this your performance may go down easily.

  Note also that inlining is merely a suggestion to compilers~\cite{gcc:inlining}.
  They may ignore you.
  For example:
  \begin{itemize}
    \item taking the address of an ``inline'' function and using it; or
    \item virtual functions (in C++),
  \end{itemize}
  will get you ignored quite fast.

\paragraph{Implications of inlining.} Inlining can make your life worse in two ways.
First, debugging is more difficult (e.g. you can't set a breakpoint in a function that
  doesn't actually exist).
 Most compilers simply won't inline code with debugging symbols on.
 Some do, but typically it's more of a pain.

Second, it can be a problem for library design:
  \begin{itemize}
    \item If you change any inline function in your library, any users
      of that library have to {\bf recompile} their program if the
      library updates. (Congratulations, you made a non-binary-compatible change!)
  \end{itemize}
This would not be a problem for non-inlined functions---programs execute the new function
dynamically at runtime.

\noindent \emph{Enabled with {\tt -O2} and {\tt -O3}.}

Obviously, inlining and devirtualization require call graphs. But so
does any analysis that needs to know about the heap effects of
functions that get called; for instance, consider this code:

{\small
\begin{lstlisting}[language=C]
  int n;

  int f() { /* opaque */ }

  int main() {
    n = 5;
    f();
    printf("%d\n", n);
  }
\end{lstlisting}
}
We could propagate the constant value 5, as long as we know that {\tt
  f()} does not write to {\tt n}.

\paragraph{Tail Recursion Elimination.} This optimization is mandatory
in some functional languages; we replace a call by a {\tt goto} at the
compiler level. Consider this example, courtesy of Wikipedia:

{\small
\begin{lstlisting}[language=C]
  int bar(int N) {
    if (A(N))
      return B(N);
    else
      return bar(N);
  }
\end{lstlisting}
}

For both calls, to {\tt B} and {\tt bar}, we don't need to return control
to the calling {\tt bar()} before returning to its caller (because {\tt bar()}
is done anyway). This avoids
function call overhead and reduces call stack use.

\noindent \emph{Enabled with {\tt -foptimize-sibling-calls}.} Also supports
sibling calls as well as tail-recursive calls.

\section*{Link-Time Optimizations}
Next up: mechanics of interprocedural optimizations in modern open-source
compilers. Conceptually, interprocedural optimizations have been well-understood
for a while. But practical implementations in open-source compilers are still
relatively new; Hubi\v{c}ka~\cite{hubicka14:_linkt_gcc} summarizes recent history.
In 2004, the only real interprocedural optimization in gcc was inlining, and it was
quite ad-hoc.

The biggest challenge for interprocedural optimizations is scalability, so 
it fits right in as a topic of discussion for this course.
Here's an outline of how it works:
\begin{itemize}[noitemsep]
\item local generation (parallelizable): compile to Intermediate Representation. Must generate compact
IR for whole-program analysis phase.
\item whole-program analysis (hard to parallelize!): create call graph, make transformation decisions. Possibly partition
the program.
\item local transformations (parallelizable): carry out transformations to local IRs, generate object code.
Perhaps use call graph partitions to decide optimizations. 
\end{itemize}
There were a number of conceptually-uninteresting implementation
challenges to be overcome before gcc could have its intermediate code available for
interprocedural analysis (i.e. there was no stable on-disk IR format). The transformations look like this:
\begin{itemize}[noitemsep]
\item global decisions, local transformations:
\begin{itemize}[noitemsep]
\item devirtualization
\item dead variable elimination/dead function elimination
\item field reordering, struct splitting/reorganization
\end{itemize}
\item global decisions, global transformations:
\begin{itemize}[noitemsep]
\item cross-module inlining
\item virtual function inlining
\item interprocedural constant propagation
\end{itemize}
\end{itemize}
The interesting issues arise from making the whole-program analysis scalable. Firefox, the Linux kernel,
and Chromium contain tens of millions of lines of code. Whole-program analysis requires that all of 
this code (in IR) be available to the analysis and that at least some summary of the code be in memory, 
along with the call graph.
(Since it's a whole-program analysis, any part of the program may affect other parts). The first problem
is getting it into memory; loading the IR for tens of millions of lines of code is a non-starter.
Clearly, anything that is more expensive than linear time can cause problems. Partitioning the program
can help.

How did gcc get better?
Hubi\v{c}ka~\cite{hubicka15:_link_gcc} explains how. In line with what I've said earlier, it's
avoiding unnecessary work.
\begin{itemize}[noitemsep]
\item gcc 4.5: initial version of LTO;
\item gcc 4.6: parallelization; partitioning of the call graph (put closely-related functions together, approximate functions in other partitions); the bottleneck: streaming in types and declarations;
\item gcc 4.7--4.9: improve build times, memory usage [``chasing unnecessary data away''.]
\end{itemize}
As far as I can tell, today's gcc, with {\tt -flto}, does work and includes
optimizations including constant propagation and function
specialization.

\paragraph{Impact.} gcc LTO appears to give 3--5\% improvements in performance, which compiler experts consider good.
Like we discussed last time, this allows developers to shift their attention from 
manual factoring of translation units to letting the compiler do it. (This is kind of like going
from manual transmissions to automatic transmissions for cars\ldots).

The LLVM project provides more details at~\cite{project17:_llvm_link_time_optim}, while gcc details
can be found at~\cite{novillo09:_linkt}.

%\url{https://gcc.gnu.org/wiki/LightweightIpo}

\input{bibliography.tex}

\end{document}
