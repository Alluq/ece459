\include{header}

\begin{document}

\lecture{20 --- Compiler Optimizations}{\term}{Patrick Lam \& Jeff Zarnett}

\section*{Compiler Optimizations}
What does it really mean when you say {\tt -O2}?
We'll see some representative compiler optimizations and discuss how
they can improve program performance. Because we're talking about
Programming for Performance, I'll point out cases that stop compilers
from being able to optimize your code. In general, it's better if the
compiler automatically does a performance-improving transformation
rather than you doing it manually; it's probably a waste of time for
you and it also makes your code less readable.

There are a lot of pages on the Internet with information about
optimizations. Here's one that contains good examples:

$\qquad \qquad$ \url{http://www.digitalmars.com/ctg/ctgOptimizer.html}

You can find a full list of {\tt gcc} options here:

$\qquad \qquad$ \url{http://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html}

\paragraph{About Compiler Optimizations.} First of all, ``optimization'' is
a bit of a misnomer, since compilers generally do not generate ``optimal'' code.
They just generate \emph{better} code.

Often, what happens is that the program you literally wrote is too slow. The
contract of the compiler (working with the architecture) is to actually execute
a program with the same behaviour as yours, but which runs faster.

\paragraph{{\tt gcc} optimization levels.} Here's what {\tt -On} means for
{\tt gcc}. Other compilers have similar (but not identical) optimization flags.

\begin{itemize}
\item {\tt -O0} (default): Fastest compilation time. Debugging works as expected.
\item {\tt -O1} ({\tt -O}): Reduce code size and execution time.
 No optimizations that increase compiliation time.
\item {\tt -O2}: All optimizations except space vs. speed tradeoffs.
\item {\tt -O3}: All optimizations.
\item {\tt -Ofast}: All {\tt -O3} optimizations, plus non-standards compliant optimizations,
      particularly {\tt -ffast-math}. (Like {\tt -fast} on the Solaris compiler.)\\[1em]
  This flag turns off exact implementations of IEEE or ISO rules/specifications for math
  functions. Generally, if you don't care about the exact result, you can use this for
  a speedup.
\end{itemize}

\subsection*{Scalar Optimizations} 
By scalar optimizations, I mean optimizations
which affect scalar (non-array) operations. Here are some examples of scalar
optimizations.

\paragraph{Constant folding.} Probably the simplest optimization one can think of.
Tag line: ``Why do later something you can do now?'' We simply translate:

\begin{center}
\begin{tabular}{lll}
i = 1024 * 1024 &
$\Longrightarrow$ &
i = 1048576
\end{tabular}
\end{center}

\noindent \emph{Enabled at all optimization levels.} The compiler will not emit
code that does the multiplication at runtime. It will simply use the
computed value.

\paragraph{Common subexpression elimination.} We can do common subexpression elimination
when the same expression {\tt x op y} is computed more than once, and
neither {\tt x} nor {\tt y} change between the two computations. In the
below example, we need to compute {\tt c + d} only once.


\begin{verbatim}
   a = (c + d) * y;
   b = (c + d) * z;   

   w = 3;
   x = f();
   y = x;
   z = w + y;

\end{verbatim}


\noindent \emph{Enabled at {\tt -O2}, {\tt -O3} or with {\tt -fgcse}.} Note that
these flags actually enable a global CSE pass, where global means across-basic-blocks.
This also enables global constant and copy propagation.

\paragraph{Constant propagation.} Moves constant values from definition to
use. The transformation is valid if there are no redefinitions of the
variable between the definition and its use. In the above example, 
we can propagate the constant value 3 to its use in {\tt z = w + y},
yielding {\tt z = 3 + y}.

\paragraph{Copy propagation.} A bit more sophisticated than constant
propagation---telescopes copies of variables from their definition to
their use. This usually runs after CSE. Using it, we can replace the
last statement with {\tt z = w + x}. If we run both constant and copy
propagation together, we get {\tt z = 3 + x}.

These scalar optimizations are more complicated in the presence 
of pointers, e.g. {\tt z = *w + y}. More below.

%\paragraph{Scalar Replacement of Aggregates.} Censored. Too many people misunderstood it last year.

\paragraph{Redundant Code Optimizations.} In some sense, most optimizations
remove redundant code, but one particular optimization is \emph{dead code
elimination}, which removes code that is guaranteed to not execute.
For instance:

{\scriptsize
\begin{verbatim}
  int f(int x) {
    return x * 2;
  }

  int g() {
    if (f(5) % 2 == 0) {
      // do stuff...
    } else {
      // do other stuff
    }
  }
\end{verbatim}
}
By looking at the code, you can tell that the then-branch of the 
{\tt if} statement in {\tt g()} is always going to execute, and the
else-branch is never going to execute.

The general problem, as with many other compiler problems, is undecidable. Let's not get too caught up in the semantics of the \textit{Entscheidungsproblem}, even if you do speak German and like to show it off by pronouncing that word correctly.  

\subsection*{Loop Optimizations}
Loop optimizations are particularly profitable when loops execute
often. This is often a win, because programs spend a lot of time looping.
The trick is to find which loops are going to be the important ones.
Profiling is helpful there.

A loop induction variable is a variable that varies on each iteration
of the loop; the loop variable is definitely a loop induction variable,
but there may be others. \emph{Induction variable elimination} gets
rid of extra induction variables.

\emph{Scalar replacement} replaces an array read {\tt a[i]}
occuring multiple times with a single read {\tt temp = a[i]} and references
to {\tt temp} otherwise. It needs to know that {\tt a[i]} won't change
between reads.

Some languages include array bounds checks, and loop optimizations
can eliminate array bounds checks if they can prove that the loop
never iterates past the array bounds.

\paragraph{Loop unrolling.} This optimization 
lets the processor run more code without having to branch 
as often. \emph{Software pipelining} is a synergistic optimization,
which allows multiple iterations of a loop to proceed in parallel.
This optimization is also useful for SIMD. Here's an example.
  \begin{verbatim}
for (int i = 0; i < 4; ++i)
    f(i)
  \end{verbatim}
could be transformed to:
  \begin{verbatim}
f(0)
f(1)
f(2)
f(3)
  \end{verbatim}
\noindent \emph{Enabled with {\tt -funroll-loops}.}

\paragraph{Loop interchange.} This optimization can give big wins
for caches (which are key); it changes the nesting of loops to
coincide with the ordering of array elements in memory. For instance,
in C, we could change this:
  \begin{verbatim}
for (int i = 0; i < N; ++i)
    for (int j = 0; j < M; ++j)
        a[j][i] = a[j][i] * c
  \end{verbatim}
to this:
  \begin{verbatim}
for (int j = 0; j < M; ++j)
    for (int i = 0; i < N; ++i)
        a[j][i] = a[j][i] * c
  \end{verbatim}
  since C is \emph{row-major} (meaning a[1][1] is beside a[1][2]),
rather than \emph{column-major}.

\noindent
\emph{Enabled with {\tt -floop-interchange}.}

Strangely enough, sometimes you want to do things the column-major way even though it's ``wrong''. If your two dimensional array is of an appropriate size then by intentionally hitting things in the ``wrong'' order, you'll trigger all your page faults up front and load all your pages into cache and then you can go wild. This was suggested as a way to make matrix multiplication faster for a sufficiently large matrix...

\paragraph{Loop fusion.} This optimization is like the OpenMP collapse
construct; we transform 
  \begin{verbatim}
for (int i = 0; i < 100; ++i)
    a[i] = 4

for (int i = 0; i < 100; ++i)
    b[i] = 7
  \end{verbatim}
into this:
  \begin{verbatim}
for (int i = 0; i < 100; ++i) {
    a[i] = 4
    b[i] = 7
}
  \end{verbatim}
There's a trade-off between data locality and loop overhead; hence,
sometimes the inverse transformation, \emph{loop fission}, will
improve performance.

\paragraph{Loop-invariant code motion.} Also known as \emph{Loop hoisting},
this optimization moves calculations out of a loop. For instance,
  \begin{verbatim}
for (int i = 0; i < 100; ++i) {
    s = x * y;
    a[i] = s * i;
}
  \end{verbatim}
  would be transformed to this:
  \begin{verbatim}
s = x * y;
for (int i = 0; i < 100; ++i) {
    a[i] = s * i;
}
  \end{verbatim}

This reduces the amount of work we have to do for each iteration of the loop.

\subsection*{Alias and Pointer Analysis}
As we've seen in the above analyses, compiler optimizations often need
to know about what parts of memory each statement reads to.  This is
easy when talking about scalar variables which are stored on the
stack. This is much harder when talking about pointers or arrays
(which can alias). \emph{Alias analysis} helps by declaring that a
given variable {\tt p} does not alias another variable {\tt q}; that
is, they point to different heap locations. \emph{Pointer analysis}
abstractly tracks what regions of the heap each variable points to.
A region of the heap may be the memory allocated at a particular
program point.

When we know that two pointers don't alias, then we know that their
effects are independent, so it's correct to move things around.
This also helps in reasoning about side effects and enabling reordering.

We've talked about automatic parallelization previously in this course.
At this point, I'll remind you that we used {\tt restrict} so that the
compiler wouldn't have to do as much pointer analysis. Shape analysis
builds on pointer analysis to determine that data structures are indeed
trees rather than lists.

\paragraph{Call Graphs.} Many interprocedural analyses require accurate
call graphs. A call graph is a directed graph showing relationships between
functions. It's easy to compute a call graph when you have C-style
function calls. It's much harder when you have virtual methods, as in 
C++ or Java, or even C function pointers. In particular, you need pointer
analysis information to construct the call graph.

\paragraph{Devirtualization.} This optimization attempts to convert
virtual function calls to direct calls.  Virtual method calls have the
potential to be slow, because there is effectively a branch to
predict. If the branch prediction goes well, then it doesn't impose
more runtime cost. However, the branch prediction might go poorly.  (In
general for C++, the program must read the object's vtable. Plus, virtual
calls impede other optimizations. Compilers can help by doing
sophisticated analyses to compute the call graph and by replacing
virtual method calls with nonvirtual method calls.  Consider the
following code:
  \begin{verbatim}
class A {
    virtual void m();
};

class B : public A {
    virtual void m();
}

int main(int argc, char *argv[]) {

    std::unique_ptr<A> t(new B);
    t.m();
}
  \end{verbatim}
Devirtualization could eliminate vtable access; instead, we could just call B's {\tt m} method
directly. By the way, ``Rapid Type Analysis'' analyzes the entire program, observes that
only {\tt B} objects are ever instantiated, and enables devirtualization
of the {\tt b.m()} call.

\noindent \emph{Enabled with {\tt -O2}, {\tt -O3}, or with {\tt -fdevirtualize}.}

Obviously, inlining and devirtualization require call graphs. But so
does any analysis that needs to know about the heap effects of
functions that get called; for instance, consider this code:

{\small
\begin{verbatim}
  int n;

  int f() { /* opaque */ }

  int main() {
    n = 5;
    f();
    printf("%d\n", n);
  }
\end{verbatim}
}
We could propagate the constant value 5, as long as we know that {\tt
  f()} does not write to {\tt n}.

\paragraph{Tail Recursion Elimination.} This optimization is mandatory
in some functional languages; we replace a call by a {\tt goto} at the
compiler level. Consider this example, courtesy of Wikipedia:

{\small
\begin{verbatim}
  int bar(int N) {
    if (A(N))
      return B(N);
    else
      return bar(N);
  }
\end{verbatim}
}

For both calls, to {\tt B} and {\tt bar}, we don't need to return control
to the calling {\tt bar()} before returning to its caller. This avoids
function call overhead and reduces call stack use.

\noindent \emph{Enabled with {\tt -foptimize-sibling-calls}.} Also supports
sibling calls as well as tail-recursive calls.

\subsection*{Miscellaneous Low-Level Optimizations}
Some optimizations affect low level code generation; here are two examples.

\paragraph{Branch Prediction.} {\tt gcc} attempts to guess the probability of each branch to 
best order the code. (For an {\tt if}, fall-through is most efficient. Why?)

This isn't quite an optimization, but you can use {\tt
  \_\_builtin\_expect(expr, value)} to help GCC, if you know the
run-time characteristics of your program. An example, from the
  Linux kernel:

  \begin{verbatim}
#define likely(x)       __builtin_expect((x),1)
#define unlikely(x)     __builtin_expect((x),0)
  \end{verbatim}

\paragraph{Architecture-Specific.} {\tt gcc} can also generate code tuned to particular
processors and processor variants. You can specify this using {\tt
  -march} and {\tt -mtune}. ({\tt -march} implies {\tt -mtune}).
This will enable specific instructions that not all CPUs support (e.g. SSE4.2).
For example, {\tt -march=corei7}. 

\noindent
Good to use on your local machine, not ideal for shipped code.


\input{bibliography.tex}

\end{document}
