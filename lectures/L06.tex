\include{header}

\begin{document}

\lecture{6 --- Race Conditions \& Synchronization}{\term}{Patrick Lam and Jeff Zarnett}


\section*{Race Conditions}
Previous courses (ECE~254 or equivalent) should have introduced the concept of a race condition. We'll be talking about them in greater detail in this course.

\begin{quote}
\textit{
	``Knock knock.''\\
	``Race Condition.''\\
	``Who's there?''
	}
\end{quote}

\paragraph{Definition.} A race occurs when you have two concurrent accesses to the
same memory location, at least one of which is a {\bf write}. In earlier courses you probably just considered any shared accesses or shared data at all. 

This definition is a little bit strict. We could also say that there is a race condition if there is some form of output, such as writing to the console. If one thread is going to write ``1'' to the console and another is going to write ``2'', then we could have a race condition. If there is no co-ordination, we could get output of ``12'' or ``21''. If the order here is unimportant, there's no issue; but if one order is correct, then the appearance of the other is a bug.

When there's a race, the final state may not be the same as running
one access to completion and then the other. But it ``usually'' is. It's nondeterministic. The fact that the output is often ``12'' and only very occasionally ``21'' may make it very difficult to track down the source of the problem. Furthermore, if we end up adding additional logging statements or use the debugger or anything to that effect, we will change the timing and possibly suppress (cover up) the behaviour of the bug.

In other situations (e.g., processor design) these are sometimes referred to as data hazards or dependencies. The problem there is that we have something that needs to wait for something else. There are four basic possibilities to consider:

\begin{enumerate}
	\item \textbf{RAW} (Read After Write) - The classic form of dependency. The read has to take place after the write, otherwise there's nothing to read, or an incorrect value will be read.
	\item \textbf{WAR} (Write After Read) - A write cannot take place until the read has happened, to ensure the read takes the correct value.
	\item \textbf{WAW} (Write After Write) - A write cannot take place because an earlier write needs to happen first. If we do them out of order, the final value may be out of date or otherwise incorrect.
	\item \textbf{RAR} (Read After Read) - No such hazard! 
\end{enumerate}

\begin{center}
\begin{tabular}{l|ll}
& {\bf Read 2nd} & {\bf Write 2nd} \\ \hline
{\bf Read 1st} & Read after read (RAR) & Write after read (WAR) \\
& No dependency & Antidependency \\
{\bf Write 1st} & Read after write (RAW) & Write after write (WAW) \\
& True dependency & Output dependency
\end{tabular}
\end{center}

The no-dependency case (RAR) is clear. Declaring data immutable 
in your program is a good way to ensure no dependencies.


Race conditions typically arise between variables which are shared
between threads.

\paragraph{Example.}
\begin{lstlisting}[language=C]
#include <stdlib.h>
#include <stdio.h>
#include <pthread.h>

void* run1(void* arg)
{
    int* x = (int*) arg;
    *x += 1;
}

void* run2(void* arg)
{
    int* x = (int*) arg;
    *x += 2;
}

int main(int argc, char *argv[])
{
    int* x = malloc(sizeof(int));
    *x = 1;
    pthread_t t1, t2;
    pthread_create(&t1, NULL, &run1, x);
    pthread_join(t1, NULL);
    pthread_create(&t2, NULL, &run2, x);
    pthread_join(t2, NULL);
    printf("%d\n", *x);
    free(x);
    return EXIT_SUCCESS;
}
\end{lstlisting}

\noindent
Question: Do we have a data race? Why or why not?
\vspace*{2em}
%No, we don't. Only one thread is active at a time.

\paragraph{Example 2.} Here's another example; keep the same thread definitions.
\begin{lstlisting}[language=C]
int main(int argc, char *argv[])
{
    int* x = malloc(sizeof(int));
    *x = 1;
    pthread_t t1, t2;
    pthread_create(&t1, NULL, &run1, x);
    pthread_create(&t2, NULL, &run2, x);
    pthread_join(t1, NULL);
    pthread_join(t2, NULL);
    printf("%d\n", *x);
    free(x);
    return EXIT_SUCCESS;
}
\end{lstlisting}

Now do we have a data race? Why or why not?
\vspace*{2em}

% Yes, we do. We have 2 threads concurrently accessing the same data.

\paragraph{Tracing our Example Data Race.} 
What are the possible outputs? (Assume that initially {\tt *x} is 1.)
We'll look at compiler intermediate code (three-address code) to tell.

\hspace*{.2\textwidth}\begin{minipage}{.8\textwidth}
\begin{lstlisting}[language=C]
run1                          run2   
D.1 = *x;                     D.1 = *x;
D.2 = D.1 + 1;                D.2 = D.1 + 2
*x = D.2;                     *x = D.2;
  \end{lstlisting}
\end{minipage}

Memory reads and writes are key in data~races.

Let's call the read and write from {\tt run1} R1 and W1; R2 and W2
from {\tt run2}. Assuming a sane\footnote{sequentially consistent; sadly, many
widely-used models are wilder than this.}
memory model, $R_n$ must precede $W_n$. {\bf C and C++ do not guarantee
  such a memory model in the presence of races.} This reasoning would
actually only work if we declared {\tt x} as {\tt atomic} and did the
individual three-address code operations. Or, you could avoid this whole
mess by using read-modify-write instructions.

Here are all possible orderings:
  \begin{center}
    \begin{tabular}{llll|l}
\multicolumn{4}{c|}{Order} & {\tt *x}\\
\hline
R1 & W1 & R2 & W2 & 4 \\
R1 & R2 & W1 & W2 & 3 \\
R1 & R2 & W2 & W1 & 2 \\
R2 & W2 & R1 & W1 & 4 \\
R2 & R1 & W2 & W1 & 2 \\
R2 & R1 & W1 & W2 & 3 \\
    \end{tabular}
  \end{center}


Let's look at an antidependency (WAR) example.

{\small \begin{center}
\begin{tabular}{ll}
\begin{minipage}{.4\textwidth}
\begin{lstlisting}[language=C]
void antiDependency(int z) {
  int y = f(x);
  x = z + 1;
}
\end{lstlisting}
\end{minipage} &
\begin{minipage}{.4\textwidth}
\begin{lstlisting}[language=C]
void fixedAntiDependency(int z) {
  int x_copy = x;
  int y = f(x_copy);
  x = z + 1;
}
\end{lstlisting}
\end{minipage} 
\end{tabular}
\end{center} }
{\sf Why is there a problem?}\\[2em]

Finally, WAWs can also inhibit parallelization:

{\small \begin{center}
\begin{tabular}{ll}
\begin{minipage}{.45\textwidth}
\begin{lstlisting}[language=C]
void outputDependency(int x, int z) {
  y = x + 1;
  y = z + 1;
}
\end{lstlisting}
\end{minipage} &
\begin{minipage}{.4\textwidth}
\begin{lstlisting}[language=C]
void fixedOutputDependency(int x, int z) {
  y_copy = x + 1;
  y = z + 1;
}
\end{lstlisting}
\end{minipage} 
\end{tabular}
\end{center} }

In both of these cases, renaming or copying data can
eliminate the dependence and enable parallelization. Of course,
copying data also takes time and uses cache, so it's not free. One
might change the output locations of both statements and then copy in
the correct output. These are usually more useful when it's not just one
access, but some sort of longer computation.




\section*{Synchronization}
You'll need some sort of synchronization to get sane results from
multithreaded programs. We'll start by talking about how to use
mutual exclusion in Pthreads.


\paragraph{Mutual Exclusion.} Mutexes are the most basic type of synchronization.
As a reminder:
    \begin{itemize}
    \item Only one thread can access code protected by a mutex at a time.
    \item All other threads must wait until the mutex is free before they can
      execute the protected code.
    \end{itemize}

    Here's an example of using mutexes:
    
    \begin{tabular}{ll}
      \begin{minipage}{.65\textwidth}
        {\bf PThreads}
  \begin{lstlisting}[language=C]
pthread_mutex_t m1_static = PTHREAD_MUTEX_INITIALIZER;
pthread_mutex_t m2_dynamic;

pthread_mutex_init(&m2_dynamic, NULL);
...
pthread_mutex_destroy(&m1_static);
pthread_mutex_destroy(&m2_dynamic);
  \end{lstlisting}
      \end{minipage}
      \begin{minipage}{.35\textwidth}
        {\bf C++11}
  \begin{lstlisting}[language=C]
mutex m1;
mutex * m2;

m2 = new mutex();
// ...

delete (m2);
  \end{lstlisting}
      \end{minipage}
    \end{tabular}

You can initialize mutexes statically (as with {\tt m1\_static}) or
dynamically ({\tt m2\_dynamic}). If you want to include attributes,
you need to use the dynamic version.

\paragraph{Mutex Attributes.} Both threads and mutexes use the notion of attributes.
We won't talk about mutex attributes in any detail, but here are the three standard ones.
  \begin{itemize}
    \item {\bf Protocol}: specifies the protocol used to prevent priority
      inversions for a mutex.
    \item {\bf Prioceiling}: specifies the priority ceiling of a mutex.
    \item {\bf Process-shared}: specifies the process sharing of a mutex.
  \end{itemize}
  You can specify a mutex as {\it process shared} so that you can access it
  between processes. In that case, you need to use shared memory and {\tt mmap},
  which we won't get into.

  \paragraph{Mutex Example.} Let's see how this looks in practice. It is fairly simple:
  
    \begin{tabular}{ll}
      \begin{minipage}{.5\textwidth}
        {\bf PThreads}
  \begin{lstlisting}[language=C]
// code
pthread_mutex_lock(&m1);
// protected code
pthread_mutex_unlock(&m1);
// more code
  \end{lstlisting}
      \end{minipage}&
      \begin{minipage}{.35\textwidth}
        {\bf C++11 Threads}
  \begin{lstlisting}[language=C]
// code
m1.lock();
// protected code
m1.unlock();
// more code
  \end{lstlisting}
      \end{minipage}
    \end{tabular}
  \begin{itemize}
    \item Everything within the {\tt lock} and {\tt unlock} is protected.
    \item Be careful to avoid deadlocks if you are using multiple mutexes (always
acquire locks in the same order across threads).
    \item Another useful primitive is {\tt pthread\_mutex\_trylock}.
  \end{itemize}

  \subsection*{Data Races}
  \vspace*{-1em}
Why are we bothering with locks? Data races. A data race occurs when
two concurrent actions access the same variable and at least one of
them is a {\bf write}. (This shows up on Assignment 1!)

  \begin{lstlisting}[language=C]
static int counter = 0;

void* run(void* arg) {
    for (int i = 0; i < 100; ++i) {
        ++counter;
    }
}

int main(int argc, char *argv[]) {
    // Create 8 threads
    // Join 8 threads
    printf("counter = %i\n", counter);
}
  \end{lstlisting}

Is there a datarace in this example? If so, how would we fix it?

\paragraph{Solution: use mutexes.}~

  \begin{lstlisting}[language=C]
static pthread_mutex_t mutex = PTHREAD_MUTEX_INITIALIZER;
static int counter = 0;

void* run(void* arg) {
    for (int i = 0; i < 100; ++i) {
        pthread_mutex_lock(&mutex);
        ++counter;
        pthread_mutex_unlock(&mutex);
    }
}

int main(int argc, char *argv[]) {
    // Create 8 threads
    // Join 8 threads
    pthread_mutex_destroy(&mutex);
    printf("counter = %i\n", counter);
}
  \end{lstlisting}
  
  
\paragraph{Recap: Mutexes.} Recall that our goal in this course is
to be able to use mutexes correctly. 
You should have seen how they work and how to use them in your operating systems course. Here we only care about using them properly.

\begin{itemize}
\item Call {\tt lock} on mutex $\ell_1$. Upon return from
      {\tt lock}, your thread has exclusive access to $\ell_1$ until it
      {\tt unlock}s it.
\item Other calls to {\tt lock} $\ell_1$ will not return
      until {\tt m1} is available.
\end{itemize}

 For background on implementing mutual exclusion, see
 \url{http://en.wikipedia.org/wiki/Lamport\%27s_bakery_algorithm}
      {Lamport's bakery algorithm}. Implementation details are not in
      scope for this course. You did take ECE 254, right?

Key idea: locks protect resources; only one thread
can hold a lock at a time. A second thread trying to obtain the lock
(i.e. \emph{contending} for the lock) has to wait, or \emph{block},
until the first thread releases the lock. So only one thread has
access to the protected resource at a time. The code between the lock
acquisition and release is known as the \emph{critical region} or \emph{critical section}.

Some mutex implementations also provide a ``try-lock'' primitive,
which grabs the lock if it's available, or returns control to the
thread if it's not, thus enabling the thread to do something else. (Kind of
like non-blocking I/O!)

Excessive use of locks can serialize programs. Consider two resources
$A$ and $B$ protected by a single lock $\ell$. Then a thread that's
just interested in $B$ still has acquire $\ell$, which requires it to
wait for any other thread working with $A$. (The Linux kernel used to
rely on a Big Kernel Lock protecting lots of resources in the 2.0 era,
and Linux 2.2 improved performance on SMPs by cutting down on the use
of the BKL.) Mac OS also used to have problems with this, using the
big and small kernel locks (but this is something they got from using
the Mach microkernel, which is a whole other story.).

Note: in Windows, the term ``mutex'' refers to an inter-process
communication mechanism. ``Critical sections'' are the mutexes we're
talking about above.

\paragraph{Spinlocks.} Spinlocks are a variant of mutexes, where the
waiting thread repeatedly tries to acquire the lock instead of sleeping.
Use spinlocks when you expect critical sections to finish 
quickly\footnote{For more information on spinlocks in the Linux
kernel, see \url{http://lkml.org/lkml/2003/6/14/146}.}. Spinning
for a long time consumes lots of CPU resources. Many lock
implementations use both sleeping and spinlocks: spin for a bit,
then sleep for longer. 

When would we ever want to use a spinlock? After all, we spend so much time talking about how we would never ever want to wait in a busy loop. Well. What we normally expect is to block until the lock becomes available. But that means a process switch, and then a switch back in the future when the lock is available. This takes nonzero time so it's optimal to use a spinlock if the amount of time we expect to wait for the lock is less than the amount of time it would take to do two process switches. As long as we have a multicore system.

\paragraph{Reader/Writer Locks.} Recall that data races only happen when
one of the concurrent accesses is a write. So, if you have read-only
(``immutable'') data, as often occurs in functional programs, you don't need
to protect access to that data. For instance, your program might
have an initialization phase, where you write some data, and then a 
query phase, where you use multiple threads to read the data.

Unfortunately, sometimes your data is not read-only. It might, for instance,
be rarely updated. Locking the data every time would be inefficient.
The answer is to instead use a \emph{reader/writer} lock. Multiple
threads can hold the lock in read mode, but only one thread can hold the
lock in write mode, and it will block until all the readers are done 
reading.

\begin{lstlisting}[language=C]
  int readData(int c1, int c2) {              // glib usage example
    g_static_rw_lock_reader_lock (&rwlock);
    int result = data[c1] + data[c2];
    g_static_rw_lock_reader_unlock (&rwlock);
  }

  void writeData(int c1, int c2, int value) {
    g_static_rw_lock_writer_lock (&rwlock);
    data[c1] += value; data[c2] -= value;
    g_static_rw_lock_writer_unlock (&rwlock);
  }
\end{lstlisting}

\paragraph{Semaphores/condition variables.} 
While semaphores can keep track of a counter and can implement
mutexes, you should use them to support signalling between threads or
processes.

In pthreads, semaphores can also be used for inter-process communication,
while condition variables are like Java's {\tt wait()}/{\tt notify()}.

\paragraph{Barriers.} This synchronization primitive allows you 
to make sure that a collection of threads all reach the
barrier before finishing. In pthreads, each thread should call
\verb+pthread_barrier_wait()+, which will proceed when enough threads
have reached the barrier. Enough means a number you specify upon
barrier creation.

\paragraph{Lock-Free Code.} We'll talk more about this soon.
Modern CPUs support atomic operations, such as compare-and-swap, which
enable experts to write lock-free code. A recent research 
result~\cite{mckenney11:_concur,Attiya:2011:LOE:1926385.1926442} states the requirements for correct implementations: basically,
such implementations must contain certain synchronization constructs.

\section*{Semaphores}
As you learned in previous courses, semaphores have a {\tt value} and
can be used for signalling between threads. When you create a semaphore,
you specify an initial value for that semaphore. Here's how they work.

\begin{itemize}
\item The {\tt value} can be understood to represent the number of resources available.
\item A semaphore has two fundamental operations: {\tt wait} and 
{\tt post}.
\item {\tt wait} reserves one instance of the protected resource, if currently
available---that is, if {\tt value} is currently above 0. If {\tt value} 
is 0, then {\tt wait} suspends the thread until some other thread makes
the resource available.
\item {\tt post} releases one instance of the protected resource,
incrementing {\tt value}.
\end{itemize}

\paragraph{Semaphore Usage.} Here are the relevant API calls.
  \begin{lstlisting}[language=C]
#include <semaphore.h>

int sem_init(sem_t *sem, int pshared, unsigned int value);
int sem_destroy(sem_t *sem);
int sem_post(sem_t *sem);
int sem_wait(sem_t *sem);
int sem_trywait(sem_t *sem);
  \end{lstlisting}

This API is a lot like the mutex API:
  \begin{itemize}
    \item must link with {\tt -pthread} (or {\tt -lrt} on Solaris);
    \item all functions return {\tt 0} on success;
    \item same usage as mutexes in terms of passing pointers.
  \end{itemize}

\noindent
How could you use a {\tt semaphore} as a {\tt mutex}?
\vspace*{3em}

% If the initial {\tt value} is 1 and you use {\tt wait} to lock
% and {\tt post} to unlock, it's equivalent to a {\tt mutex}

\paragraph{Semaphores for Signalling.}
Here's an example from the recommended book. How would you make this always print
``Thread 1'' then ``Thread 2'' using semaphores?

\begin{lstlisting}[language=C]
#include <pthread.h>
#include <stdio.h>
#include <semaphore.h>
#include <stdlib.h>

void* p1 (void* arg) { printf("Thread 1\n"); }

void* p2 (void* arg) { printf("Thread 2\n"); }

int main(int argc, char *argv[])
{
    pthread_t thread[2];
    pthread_create(&thread[0], NULL, p1, NULL);
    pthread_create(&thread[1], NULL, p2, NULL);
    pthread_join(thread[0], NULL);
    pthread_join(thread[1], NULL);
    return EXIT_SUCCESS;
}
\end{lstlisting}

\paragraph{Proposed Solution.} Is it actually correct?

\begin{lstlisting}[language=C]
sem_t sem;
void* p1 (void* arg) {
  printf("Thread 1\n");
  sem_post(&sem);
}
void* p2 (void* arg) {
  sem_wait(&sem);
  printf("Thread 2\n");
}

int main(int argc, char *argv[])
{
    pthread_t thread[2];
    sem_init(&sem, 0, /* value: */ 1);
    pthread_create(&thread[0], NULL, p1, NULL);
    pthread_create(&thread[1], NULL, p2, NULL);
    pthread_join(thread[0], NULL);
    pthread_join(thread[1], NULL);
    sem_destroy(&sem);
}
\end{lstlisting}

Well, let's reason through it.  
  \begin{itemize}
    \item {\tt value} is initially 1.
    \item Say {\tt p2} hits its {\tt sem\_wait} first and succeeds.
    \item {\tt value} is now 0 and {\tt p2} prints ``Thread 2'' first.
    \item It would be OK if {\tt p1} happened first. That would just increase
      {\tt value} to 2.
    \end{itemize}

Fix: set the initial {\tt value} to 0. Then, if {\tt p2} hits
its {\tt sem\_wait} first, it will not print until {\tt p1} posts, which is after 
{\tt p1} prints ``Thread 1''.


\subsection*{The {\tt volatile} qualifier}
We'll continue by discussing C language features and how they affect
the compiler. The {\tt volatile} qualifier in C notifies the compiler that
a variable may be changed by ``external forces''. It therefore ensures
that the compiled code does an actual read from a variable every time
a read appears (i.e. the compiler can't optimize away the read). It
does not prevent re-ordering nor does it protect against races. This is different from the Java {\tt volatile}.

Here's an example.
  \begin{lstlisting}[language=C]
int i = 0;

while (i != 255) { ... }
  \end{lstlisting}

{\tt volatile} prevents this from being optimized to:

  \begin{lstlisting}[language=C]
int i = 0;

while (true) { ... }
  \end{lstlisting}

Most of the time, {\tt volatile} only prevents useful
optimizations. {\tt volatile} is usually wrong unless there is a
\emph{very} good reason for it.

The ``typical'' use case for volatile is having some variable like \texttt{quit} as the condition of your infinite loop (\texttt{while (!quit)}... ) and when something happens, say, you catch the Ctrl-C signal, you change the value of \texttt{quit} so your infinite loop will exit and the program will clean itself up nicely. The compiler doesn't necessarily notice that some other thread or signal handler or what have you will make changes to the value of \texttt{quit} and so it will probably conclude that it can optimize something like \texttt{!quit} to \texttt{true}, which is right the vast majority of the time, but wrong in that important scenario...

  
\input{bibliography.tex}

\end{document}
