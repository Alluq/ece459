\include{header}

\begin{document}

\lecture{11 --- Autoparallelization }{\term}{Patrick Lam}

\section*{The Compiler and You}
Making the compiler work for you is critical to programming for
performance. We'll therefore see some compiler implementation details
in this class. Understanding these details will help you reason about
how your code gets translated into machine code and thus executed.

\paragraph{Three Address Code.} Compiler analyses are much easier to
perform on simple expressions which have two operands and a
result---hence three addresses---rather than full expression trees.
Any good compiler will therefore convert a program's abstract syntax
tree into an intermediate, portable, three-address code before going
to a machine-specific backend.

Each statement represents one fundamental operation; we'll consider
these operations to be atomic. A typical statement looks like this:

\[ \qquad \mbox{result} := \mbox{operand$_1$}\:\mbox{operator}\:\mbox{operand$_2$} \]

Three-address code is useful for reasoning about data races. It is
also easier to read than assembly, as it separates out memory reads
and writes.

\paragraph{GIMPLE: \texttt{gcc}'s three-address code.} To see the GIMPLE representation 
of your code, pass {\tt gcc} the {\tt -fdump-tree-gimple} flag. You
can also see all of the three address code generated by the compiler;
use {\tt -fdump-tree-all}. You'll probably just be interested in the
optimized version.  

I suggest using GIMPLE to reason about your code at a low level
without having to read assembly. Let's take a few minutes to look at a few examples, focusing on some code we have already created.


\subsection*{The {\tt restrict} qualifier} 
The {\tt restrict} qualifier on pointer {\tt p} tells
the compiler~\cite{cellperf} that it may assume that, in the scope of {\tt p},
the program will not use any other pointer {\tt q} to access the
data at {\tt *p}.

The {\tt restrict} qualifier is a feature introduced in C99: ``The
restrict type qualifier allows programs to be written so that
translators can produce significantly faster executables.''
  \begin{itemize}
    \item To request C99 in {\tt gcc}, use the {\tt -std=c99} flag.
  \end{itemize}

{\tt restrict} means: you are promising the
compiler that the pointer will never alias (another pointer will not
point to the same data) for the lifetime of the pointer.  Hence, two
pointers declared {\tt restrict} must never point to the same data.

In fact~\cite{cellperf} includes a contract that goes with the use of restrict:

\begin{quote}
I, [insert your name], a PROFESSIONAL or AMATEUR [circle one] programmer recognize that there are limits to what a compiler can do. I certify that, to the best of my knowledge, there are no magic elves or monkeys in the compiler which through the forces of fairy dust can always make code faster. I understand that there are some problems for which there is not enough information to solve. I hereby declare that given the opportunity to provide the compiler with sufficient information, perhaps through some key word, I will gladly use said keyword and not bitch and moan about how "the compiler should be doing this for me."

In this case, I promise that the pointer declared along with the restrict qualifier is not aliased. I certify that writes through this pointer will not effect the values read through any other pointer available in the same context which is also declared as restricted.

* Your agreement to this contract is implied by use of the restrict keyword ;)
\end{quote}

Of course, I highly recommend that you have your personal legal expert review this contract before you sign it. As I would for any contract. Contracts are serious business.

An example from Wikipedia:
\begin{lstlisting}[language=C]
  void updatePtrs(int* ptrA, int* ptrB, int* val) {
    *ptrA += *val;
    *ptrB += *val;
  }
\end{lstlisting}
Would declaring all these pointers as {\tt restrict} generate better code?

Well, let's look at the GIMPLE.

\begin{lstlisting}[language=C]
void updatePtrs(int* ptrA, int* ptrB, int* val) {
 D.1609 = *ptrA;
 D.1610 = *val;
 D.1611 = D.1609 + D.1610;
 *ptrA = D.1611;
 D.1612 = *ptrB;
 D.1610 = *val;
 D.1613 = D.1612 + D.1610;
 *ptrB = D.1613;
}
\end{lstlisting}

Now we can answer the question: ``Could any operation be left out if
all the pointers didn't overlap?''

\begin{itemize}
\item If {\tt ptrA} and {\tt val} are not equal, you don't have to
      reload the data on {\bf line 7}.
\item Otherwise, you would: there might be a call, somewhere:\\\verb+    updatePtrs(&x, &y, &x);+
\end{itemize}

Hence, this set of annotations allows optimization:
\begin{lstlisting}[language=C]
    void updatePtrs(int* restrict ptrA, 
                    int* restrict ptrB,
                    int* restrict val)
\end{lstlisting}
Note: you can get the optimization by just declaring {\tt ptrA} and
      {\tt val} as {\tt restrict}; {\tt ptrB} isn't needed for this optimization

\paragraph{Summary of {\tt restrict}.}
Use {\tt restrict} whenever you know the pointer will not alias
another pointer (also declared {\tt restrict}).

It's hard for the compiler to infer pointer aliasing information;
it's easier for you to specify it. If the compiler has this information,
it can better optimize your code; in the body of a critical loop, that
can result in better performance.

A caveat: don't lie to the compiler, or you will get undefined behaviour.

Aside: {\tt restrict} is not the same as {\tt const}. {\tt const} data can still be
changed through an alias.



\section*{Automatic Parallelization}

We'll now talk about automatic parallelization. The vision is that the compiler will take
your standard sequential C program and convert it into a parallel C
program which leverages multiple cores, CPUs, machines, etc.  This was
an active area of research in the 1990s, then tapered off in the 2000s
(because it's a hard problem!); it is enjoying renewed interest now (but
it's still hard!)

\paragraph{What can we parallelize?} The
easiest kind of program to parallelize is the classic Fortran program
which performs a computation over a huge array. C code---if it's the
right kind---is a bit worse, but still tractable, given enough hints
to the compiler. For us, the right kind of code is going to be array
codes. Some production compilers, like the non-free Intel C compiler
{\tt icc}, the free-as-in-beer Solaris Studio
compiler~\cite{solarisstudio}
and the free GNU C compiler {\tt gcc}, include support for
parallelization, with different maturity levels.


Following Gove, we'll parallelize the following code:

\begin{lstlisting}[language=C]
#include <stdlib.h>

void setup(double *vector, int length) {
    int i;
    for (i = 0; i < length; i++) {
        vector[i] += 1.0;
    }
}

int main() {
    double *vector;
    vector = (double*) malloc (sizeof (double) * 1024 * 1024);
    for (int i = 0; i < 1000; i++) {
        setup (vector, 1024*1024);
    }
}
\end{lstlisting}


\paragraph{Automatic Parallelization.} Let's first see what compilers can do automatically.
The Solaris Studio compiler yields the following output:
{\small 
\begin{lstlisting}[language=C]
$ cc -O3 -xloopinfo -xautopar omp_vector.c 
"omp_vector.c", line 5: PARALLELIZED, and serial version generated
"omp_vector.c", line 15: not parallelized, call may be unsafe
\end{lstlisting}
} 

{\bf Note:} The Solaris compiler generates two versions of the code,
and decides, at runtime, if the parallel code would be faster, depending on
whether the loop bounds, at runtime, are large enough to justify spawning
threads.

Under the hood, most parallelization frameworks use {\tt OpenMP},
which we'll see next time. For now, you can control the number of
threads with the {\tt OMP\_NUM\_THREADS} environment variable.

\paragraph{Autoparallelization in {\tt gcc}.} 
{\tt gcc} 4.3+ can also parallelize loops, but there are a couple of
problems: 1) the loop parallelization doesn't seem very stable yet; 2)
I can't figure out how to make {\tt gcc} tell you what it did in a comprehensible
way (you can try {\tt -fdump-tree-parloops-details}); and,
perhaps most importantly for performance, 3) {\tt gcc} doesn't have
many heuristics yet for guessing which loops are profitable (since 4.8, it can
use profiling data and tries to infer the number of loop iterations
happen)~\cite{autopar}.

The BSD and Mac OS X default C compiler {\tt clang} also has the {\tt polly} parallelization framework, but we'll leave that aside for now. If you have significant experience with it, make a pull request for this lecture and it will be added!

One way to inspect {\tt gcc}'s output is by giving it the {\tt
  -S} option and looking at the resulting assembly code yourself. This
is obviously not practical for production software.
\begin{lstlisting}[language=C]
$ gcc -std=c99 omp_vector.c -O2 -floop-parallelize-all -ftree-parallelize-loops=2 -S           
\end{lstlisting}

The resulting {\tt .s} file contains the following code:

\begin{lstlisting}[language=C]
        call    GOMP_parallel_start
        movl    %edi, (%esp)
        call    setup._loopfn.0
        call    GOMP_parallel_end
\end{lstlisting}
{\tt gcc} code appears to ignore \verb+OMP_NUM_THREADS+.  Here's
some potential output from a parallelized program:
\begin{lstlisting}[language=C]
        $ export OMP_NUM_THREADS=2
        $ time ./a.out
        real    0m5.167s
        user    0m7.872s
        sys    0m0.016s
\end{lstlisting}


(When you use multiple (virtual) CPUs, CPU usage can increase beyond
100\% in {\tt top}, and real time can be less than user time in
the {\tt time} output, since user time counts the time used by all CPUs.)

Let's look at some gcc examples from~\cite{autopar:related}.

\paragraph{Loops That gcc's Automatic Parallelization Can Handle.}~\\

  Single loop:
  \begin{lstlisting}[language=C]
for (i = 0; i < 1000; i++)
    x[i] = i + 3;
  \end{lstlisting}

  Nested loops with simple dependency:
  \begin{lstlisting}[language=C]
for (i = 0; i < 100; i++)
    for (j = 0; j < 100; j++)
        X[i][j] = X[i][j] + Y[i-1][j];
  \end{lstlisting}

  Single loop with not-very-simple dependency:
  \begin{lstlisting}[language=C]
for (i = 0; i < 10; i++)
    X[2*i+1] = X[2*i];
  \end{lstlisting}

\paragraph{Loops That gcc's Automatic Parallelization Can't Handle.}~\\

  Single loop with if statement:
  \begin{lstlisting}[language=C]
for (j = 0; j <= 10; j++)
    if (j > 5) X[i] = i + 3;
  \end{lstlisting}

  Triangle loop:
  \begin{lstlisting}[language=C]
for (i = 0; i < 100; i++)
    for (j = i; j < 100; j++)
        X[i][j] = 5;
  \end{lstlisting}

\paragraph{Manual Parallelization.} Let's first think about how we could 
manually parallelize this code.
\begin{itemize}
\item {\bf Option 1:} horizontal, \begin{minipage}{7em} --- --- --- ---\\[-.8em] --- --- --- ---\\[-.8em] --- --- --- --- \end{minipage} \\
Create 4 threads; each thread does 1000 iterations on its own sub-array.

\item {\bf Option 2:} bad horizontal, \begin{minipage}{7em} --- --- --- ---\\[-.8em] --- --- --- ---\\[-.8em] --- --- --- --- \end{minipage} \\
1000 times, create 4 threads which each operate once on the sub-array.

\item {\bf Option 3:} vertical $ \quad \: \qquad \mid \mid \mid\mid \:\: \mid \mid \mid \mid \:\: \mid \mid \mid \mid\:\: \mid \mid \mid \mid$\\
Create 4 threads; for each element, the owning thread does 1000 iterations on that element.
\end{itemize}
We can try these and empirically see which works better. As you might expect, bad horizontal
does the worst. Horizontal does best. Let's take a minute to look at the results from~\cite{autopar:related}
\subsection*{Case study: Multiplying a Matrix by a Vector.}
Next, we'll see how automatic parallelization does on a more complicated
program. We will progressively remove barriers to parallelization for
this program:

\begin{lstlisting}[language=C]
void matVec (double **mat, double *vec, double *out,
             int *row, int *col) 
{
  int i, j;
  for (i = 0; i < *row; i++)
  {
    out[i] = 0;
    for (j = 0; j < *col; j++)
    {
      out[i] += mat[i][j] * vec[j];
    }
  }
}
\end{lstlisting}

The Solaris C compiler refuses to parallelize this code:
{\small 
\begin{lstlisting}[language=C]
$ cc -O3 -xloopinfo -xautopar fploop.c 
"fploop.c", line 5: not parallelized, not a recognized for loop
"fploop.c", line 8: not parallelized, not a recognized for loop
\end{lstlisting} 
}
For definitive documentation about Sun's automatic parallelization, see
Chapter 10 of their \emph{Fortran Programming Guide} and do the analogy to C:

\url{http://download.oracle.com/docs/cd/E19205-01/819-5262/index.html}

In this case, the loop bounds are not constant, and the write to {\tt
  out} might overwrite either {\tt row} or {\tt col}. So, let's modify
the code and make the loop bounds {\tt int}s rather than {\tt int *}s.

\begin{lstlisting}[language=C]
void matVec (double **mat, double *vec, double *out,
             int row, int col) 
{
  int i, j;
  for (i = 0; i < row; i++)
  {
    out[i] = 0;
    for (j = 0; j < col; j++)
    {
      out[i] += mat[i][j] * vec[j];
    }
  }
}
\end{lstlisting}
 This changes the error message:


\begin{lstlisting}[language=C]
$ cc -O3 -xloopinfo -xautopar fploop1.c 
"fploop1.c", line 5: not parallelized, unsafe dependence
"fploop1.c", line 8: not parallelized, unsafe dependence
\end{lstlisting} 

Now the problem is that {\tt out} might alias {\tt mat} or {\tt vec};
as I've mentioned previously, parallelizing
in the presence of aliases could change the run-time behaviour.

\paragraph{{\tt restrict} qualifier.} 
Recall that the {\tt restrict} qualifier on pointer {\tt p} tells
the compiler that it may assume that, in the scope of {\tt p},
the program will not use any other pointer {\tt q} to access the
data at {\tt *p}~\cite{cellperf}.

{
\begin{lstlisting}[language=C]
void matVec (double **mat, double *vec, double * restrict out,
             int row, int col) 
{
  int i, j;
  for (i = 0; i < row; i++)
  {
    out[i] = 0;
    for (j = 0; j < col; j++)
    {
      out[i] += mat[i][j] * vec[j];
    }
  }
}
\end{lstlisting}
}

Now Solaris {\tt cc} is happy to parallelize the outer loop:

{\small 
\begin{lstlisting}[language=C]
$ cc -O3 -xloopinfo -xautopar fploop2.c 
"fploop2.c", line 5: PARALLELIZED, and serial version generated
"fploop2.c", line 8: not parallelized, unsafe dependence
\end{lstlisting} 
}

There's still a dependence in the inner loop. This dependence is because
all inner loop iterations write to the same location, {\tt out[i]}. We'll
discuss that problem below.

In any case, the outer loop is the one that can actually improve performance,
since parallelizing it imposes much less barrier synchronization cost 
waiting for all threads to finish. So, even if we tell the compiler to ignore
the reduction issue, it will generally refuse to parallelize inner loops:
{\small 
\begin{lstlisting}[language=C]
$ cc -g -O3 -xloopinfo -xautopar -xreduction fploop2.c 
"fploop2.c", line 5: PARALLELIZED, and serial version generated
"fploop2.c", line 8: not parallelized, not profitable
\end{lstlisting} 
}


\paragraph{Summary of conditions for automatic parallelization.} Here's what I 
can figure out; you may also refer to Chapter 3 of the Solaris Studio
\emph{C User's Guide}, but it doesn't spell out the exact conditions
either. To parallelize a loop, it must:
\begin{itemize}
\item have a recognized loop style, e.g. {\tt for} loops with
bounds that don't vary per iteration;
\item have no dependencies between data accessed in loop bodies for
  each iteration;
\item not conditionally change scalar variables read after the loop
  terminates, or change any scalar variable across iterations;
\item have enough work in the loop body to make parallelization profitable.
\end{itemize}

\paragraph{Reductions.} The concept behind a 
reduction (as made ``famous'' in MapReduce, which we'll talk about
later) is reducing a set of data to a smaller set which somehow
summarizes the data. For us, reductions are going to reduce
arrays to a single value. Consider, for instance, this function, which
calculates the sum of an array of numbers:

{
\begin{lstlisting}[language=C]
double sum (double *array, int length)
{
  double total = 0;

  for (int i = 0; i < length; i++)
    total += array[i];
  return total;
}
\end{lstlisting}
}

There are two barriers: 1) the value of {\tt total} depends on what
gets computed in previous iterations; and 2) addition is actually
non-associative for floating-point values. ({\sf Why? When is it
appropriate to parallelize non-associative operations?})

Nevertheless, the Solaris C compiler will explicitly recognize
some reductions and can parallelize them for you:

{
\begin{lstlisting}[language=C]
$ cc -O3 -xautopar -xreduction -xloopinfo sum.c
"sum.c", line 5: PARALLELIZED, reduction, and serial version generated
\end{lstlisting}
}

{\bf Note:}  If we try to do the reduction on {\tt fploop.c} with {\tt restrict}s added, we'll get the following:

\begin{lstlisting}[language=C]
$ cc -O3 -xautopar -xloopinfo  -xreduction -c fploop.c
"fploop.c", line 5: PARALLELIZED, and serial version generated
"fploop.c", line 8: not parallelized, not profitable
\end{lstlisting}

\paragraph{Dealing with function calls.} Generally, function calls
can have arbitrary side effects. Production compilers will usually
avoid parallelizing loops with function calls; research compilers try
to ensure that functions are pure and then parallelize them.
(This is why functional languages are nice for parallel
programming: impurity is visible in type signatures.)

For builtin functions, like {\tt sin()}, you can promise to the 
compiler that you didn't replace them with your own implementations
({\tt -xbuiltin}), and then the compiler will parallelize the loop.

Another option is to crank up the optimization level ({\tt -xO4}), or
to explicitly tell the compiler to inline certain functions ({\tt
  -xinline=}), thereby enabling parallelization. This doesn't work as
well as one might hope; using macros will always work, but is less maintainable.

\paragraph{Helping the compiler parallelize.} Let's summarize what we've
seen. To help the compiler, we can use the {\tt restrict} qualifier on
pointers (possibly copying a pointer to a {\tt restrict}-qualified
pointer: {\tt int * restrict p = s->p;}); and, we can make sure that
loop bounds don't change in the loop (e.g. by using temporary
variables). Some compilers can automatically create different versions
for the alias-free case and the (parallelized) aliased case; at
runtime, the program runs the aliased case if the inputs permit.

\input{bibliography.tex}

\end{document}
