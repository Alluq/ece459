\include{header}

\begin{document}

\lecture{11 --- Autoparallelization }{\term}{Patrick Lam}

\section*{Automatic Parallelization}

We'll now talk about automatic parallelization. The vision is to take
your standard sequential C program and convert it into a parallel C
program which leverages multiple cores, CPUs, machines, etc.  This was
an active area of research in the 1990s, then tapered off in the 2000s
(because it's a hard problem!); it is enjoying renewed interest now (but
it's still hard!)

\paragraph{What can we parallelize?} The
easiest kind of program to parallelize is the classic Fortran program
which performs a computation over a huge array. C code---if it's the
right kind---is a bit worse, but still tractable, given enough hints
to the compiler. For us, the right kind of code is going to be array
codes. Some production compilers, like the non-free Intel C compiler
{\tt icc}, the free-as-in-beer Solaris Studio
compiler~\cite{solarisstudio}
and the free GNU C compiler {\tt gcc}, include support for
parallelization, with different maturity levels.


Following Gove, we'll parallelize the following code:

\begin{verbatim}
#include <stdlib.h>

void setup(double *vector, int length) {
    int i;
    for (i = 0; i < length; i++) {
        vector[i] += 1.0;
    }
}

int main() {
    double *vector;
    vector = (double*) malloc (sizeof (double) * 1024 * 1024);
    for (int i = 0; i < 1000; i++) {
        setup (vector, 1024*1024);
    }
}
\end{verbatim}


\paragraph{Automatic Parallelization.} Let's first see what compilers can do automatically.
The Solaris Studio compiler yields the following output:
{\small 
\begin{verbatim}
$ cc -O3 -xloopinfo -xautopar omp_vector.c 
"omp_vector.c", line 5: PARALLELIZED, and serial version generated
"omp_vector.c", line 15: not parallelized, call may be unsafe
\end{verbatim}
} 

{\bf Note:} The Solaris compiler generates two versions of the code,
and decides, at runtime, if the parallel code would be faster, depending on
whether the loop bounds, at runtime, are large enough to justify spawning
threads.

Under the hood, most parallelization frameworks use {\tt OpenMP},
which we'll see next time. For now, you can control the number of
threads with the {\tt OMP\_NUM\_THREADS} environment variable.

\paragraph{Autoparallelization in {\tt gcc}.} 
{\tt gcc} 4.3+ can also parallelize loops, but there are a couple of
problems: 1) the loop parallelization doesn't seem very stable yet; 2)
I can't figure out how to make {\tt gcc} tell you what it did in a comprehensible
way (you can try {\tt -fdump-tree-parloops-details}); and,
perhaps most importantly for performance, 3) {\tt gcc} doesn't have
many heuristics yet for guessing which loops are profitable (since 4.8, it can
use profiling data and tries to infer the number of loop iterations
happen)\footnote{\url{https://gcc.gnu.org/wiki/AutoParInGCC}}.

{\tt clang} also has the {\tt polly} parallelization framework, but I couldn't
figure out how to try it.

One way to inspect {\tt gcc}'s output is by giving it the {\tt
  -S} option and looking at the resulting assembly code yourself. This
is obviously not practical for production software.
\begin{verbatim}
$ gcc -std=c99 omp_vector.c -O2 -floop-parallelize-all -ftree-parallelize-loops=2 -S           
\end{verbatim}

The resulting {\tt .s} file contains the following code:

\begin{verbatim}
        call    GOMP_parallel_start
        movl    %edi, (%esp)
        call    setup._loopfn.0
        call    GOMP_parallel_end
\end{verbatim}
{\tt gcc} code appears to ignore \verb+OMP_NUM_THREADS+.  Here's
some potential output from a parallelized program:
\begin{verbatim}
        $ export OMP_NUM_THREADS=2
        $ time ./a.out
        real    0m5.167s
        user    0m7.872s
        sys    0m0.016s
\end{verbatim}


(When you use multiple (virtual) CPUs, CPU usage can increase beyond
100\% in {\tt top}, and real time can be less than user time in
the {\tt time} output, since user time counts the time used by all CPUs.)

Let's look at some gcc examples from: \url{http://gcc.gnu.org/wiki/AutoparRelated}.

\paragraph{Loops That gcc's Automatic Parallelization Can Handle.}~\\

  Single loop:
  \begin{verbatim}
for (i = 0; i < 1000; i++)
    x[i] = i + 3;
  \end{verbatim}

  Nested loops with simple dependency:
  \begin{verbatim}
for (i = 0; i < 100; i++)
    for (j = 0; j < 100; j++)
        X[i][j] = X[i][j] + Y[i-1][j];
  \end{verbatim}

  Single loop with not-very-simple dependency:
  \begin{verbatim}
for (i = 0; i < 10; i++)
    X[2*i+1] = X[2*i];
  \end{verbatim}

\paragraph{Loops That gcc's Automatic Parallelization Can't Handle.}~\\

  Single loop with if statement:
  \begin{verbatim}
for (j = 0; j <= 10; j++)
    if (j > 5) X[i] = i + 3;
  \end{verbatim}

  Triangle loop:
  \begin{verbatim}
for (i = 0; i < 100; i++)
    for (j = i; j < 100; j++)
        X[i][j] = 5;
  \end{verbatim}

\paragraph{Manual Parallelization.} Let's first think about how we could 
manually parallelize this code.
\begin{itemize}
\item {\bf Option 1:} horizontal, \begin{minipage}{7em} --- --- --- ---\\[-.8em] --- --- --- ---\\[-.8em] --- --- --- --- \end{minipage} \\
Create 4 threads; each thread does 1000 iterations on its own sub-array.

\item {\bf Option 2:} bad horizontal, \begin{minipage}{7em} --- --- --- ---\\[-.8em] --- --- --- ---\\[-.8em] --- --- --- --- \end{minipage} \\
1000 times, create 4 threads which each operate once on the sub-array.

\item {\bf Option 3:} vertical $ \quad \: \qquad \mid \mid \mid\mid \:\: \mid \mid \mid \mid \:\: \mid \mid \mid \mid\:\: \mid \mid \mid \mid$\\
Create 4 threads; for each element, the owning thread does 1000 iterations on that element.
\end{itemize}
We can try these and empirically see which works better. As you might expect, bad horizontal
does the worst. Horizontal does best. See L11 notes for benchmark results and further discussion.

\subsection*{Case study: Multiplying a Matrix by a Vector.}
Next, we'll see how automatic parallelization does on a more complicated
program. We will progressively remove barriers to parallelization for
this program:

\begin{verbatim}
void matVec (double **mat, double *vec, double *out,
             int *row, int *col) 
{
  int i, j;
  for (i = 0; i < *row; i++)
  {
    out[i] = 0;
    for (j = 0; j < *col; j++)
    {
      out[i] += mat[i][j] * vec[j];
    }
  }
}
\end{verbatim}

The Solaris C compiler refuses to parallelize this code:
{\small 
\begin{verbatim}
$ cc -O3 -xloopinfo -xautopar fploop.c 
"fploop.c", line 5: not parallelized, not a recognized for loop
"fploop.c", line 8: not parallelized, not a recognized for loop
\end{verbatim} 
}
For definitive documentation about Sun's automatic parallelization, see
Chapter 10 of their \emph{Fortran Programming Guide} and do the analogy to C:

\url{http://download.oracle.com/docs/cd/E19205-01/819-5262/index.html}

In this case, the loop bounds are not constant, and the write to {\tt
  out} might overwrite either {\tt row} or {\tt col}. So, let's modify
the code and make the loop bounds {\tt int}s rather than {\tt int *}s.

\begin{verbatim}
void matVec (double **mat, double *vec, double *out,
             int row, int col) 
{
  int i, j;
  for (i = 0; i < row; i++)
  {
    out[i] = 0;
    for (j = 0; j < col; j++)
    {
      out[i] += mat[i][j] * vec[j];
    }
  }
}
\end{verbatim}
 This changes the error message:


\begin{verbatim}
$ cc -O3 -xloopinfo -xautopar fploop1.c 
"fploop1.c", line 5: not parallelized, unsafe dependence
"fploop1.c", line 8: not parallelized, unsafe dependence
\end{verbatim} 

Now the problem is that {\tt out} might alias {\tt mat} or {\tt vec};
as I've mentioned previously, parallelizing
in the presence of aliases could change the run-time behaviour.

\paragraph{{\tt restrict} qualifier.} 
Recall that the {\tt restrict} qualifier on pointer {\tt p} tells
the compiler\footnote{\url{http://cellperformance.beyond3d.com/articles/2006/05/demystifying-the-restrict-keyword.html}} that it may assume that, in the scope of {\tt p},
the program will not use any other pointer {\tt q} to access the
data at {\tt *p}.

{
\begin{verbatim}
void matVec (double **mat, double *vec, double * restrict out,
             int row, int col) 
{
  int i, j;
  for (i = 0; i < row; i++)
  {
    out[i] = 0;
    for (j = 0; j < col; j++)
    {
      out[i] += mat[i][j] * vec[j];
    }
  }
}
\end{verbatim}
}

Now Solaris {\tt cc} is happy to parallelize the outer loop:

{\small 
\begin{verbatim}
$ cc -O3 -xloopinfo -xautopar fploop2.c 
"fploop2.c", line 5: PARALLELIZED, and serial version generated
"fploop2.c", line 8: not parallelized, unsafe dependence
\end{verbatim} 
}

There's still a dependence in the inner loop. This dependence is because
all inner loop iterations write to the same location, {\tt out[i]}. We'll
discuss that problem below.

In any case, the outer loop is the one that can actually improve performance,
since parallelizing it imposes much less barrier synchronization cost 
waiting for all threads to finish. So, even if we tell the compiler to ignore
the reduction issue, it will generally refuse to parallelize inner loops:
{\small 
\begin{verbatim}
$ cc -g -O3 -xloopinfo -xautopar -xreduction fploop2.c 
"fploop2.c", line 5: PARALLELIZED, and serial version generated
"fploop2.c", line 8: not parallelized, not profitable
\end{verbatim} 
}




\input{bibliography.tex}

\end{document}
