\input{configuration}

\title{Lecture 13 --- OpenMP}

\author{Patrick Lam \\ \small \texttt{patrick.lam@uwaterloo.ca}}
\institute{Department of Electrical and Computer Engineering \\
  University of Waterloo}
\date{\today}


\begin{document}

\begin{frame}
  \titlepage

 \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[containsverbatim]
  \frametitle{From pthreads to OpenMP}


    So far: Pthreads and automatic parallelization.
    
    Next: ``Manual'' parallelization using OpenMP (Open Multiprocessing).
          
    You specify parallelization; compiler implements.
    
    \begin{center}
    \includegraphics[width=0.4\textwidth]{images/makeitso.jpg}
    \end{center}
    
    ``I want 10 turnstiles.''


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Make It So, Number One}

You use OpenMP by specifying \alert{directives} in the code.

Directives look like \texttt{\#pragma omp ...}

There are 16 directives; each applies to the immediately-following statement.

Some have a list as an argument (a list of variable names)

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[containsverbatim]
  \frametitle{Modern compilers support OpenMP}

All major compilers have OpenMP: (gcc, clang, Solaris, Intel, Microsoft).

Benefits:
\begin{itemize}
	\item Switch parallelism on and off via compile option
	\item Make the compiler do hard work
	\item Makes algorithms easier to read
	\item Apply them only in parts where needed, incrementally
\end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\begin{frame}[fragile]
\frametitle{Simple For Loop Exampe}

\begin{lstlisting}[language=C,morekeywords={foreach,pragma,omp,parallel,single,nowait,task,untied,barrier,taskyield}]
  void calc (double *array1, double *array2, int length) {
    #pragma omp parallel for
    for (int i = 0; i < length; i++) {
      array1[i] += array2[i];
    }
  }
\end{lstlisting}

The directive instructs the compiler to parallelize the loop.

Developer's responsibility to make sure this is safe!

You don't need to declare {\tt restrict}, but it's a good idea.

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[containsverbatim]
  \frametitle{{\tt \#pragma omp parallel for}}

  Let's look at the parts of this \verb+#pragma+.\\[1em]

  \begin{itemize}
    \item \verb+#pragma omp+ indicates an OpenMP directive;
    \vfill
    \item {\tt parallel} indicates the start of a parallel region.
    \vfill
    \item {\tt for} tells OpenMP: run the next {\tt for} loop in parallel.
  \end{itemize}


The runtime library starts
  a number of threads \&  assigns a subrange of the range to 
  each of the threads.


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{Loops of a Certain Kind}

  
\begin{verbatim}
    for (int i = 0; i < length; i++) { ... }
\end{verbatim}~\\[1em]

    Can only parallelize loops which satisfy these conditions:
\begin{itemize}
\item form:~{\tt for (init expr; test expr; increment expr)};
\item loop variable must be integer (signed or unsigned), pointer, or a C++
random access iterator;
\item loop variable must be initialized to one end of the range;
\item loop increment amount must be loop-invariant (constant with respect to the loop body); and
\item test expression must be one of {\tt >}, {\tt >=}, {\tt <}, or {\tt <=}, and the comparison value (bound) must be loop-invariant.
\end{itemize}

{\bf Note:} these restrictions thus also apply to automatically parallelized
loops.


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[containsverbatim]
  \frametitle{Parallelized Code Generated}

  \begin{enumerate}
    \item Compiler generates code to spawn a \alert{team}
of threads; automatically splits off worker-thread code into a
separate procedure.
    \item Generated code uses {\tt fork-join} parallelism; when the
master thread hits a parallel region, it gives work to the worker
threads, which execute and report back.
    \item Afterwards, the master thread
continues running, while the worker threads wait for more work .
  \end{enumerate}



You can specify the number of threads by setting the
\verb+OMP_NUM_THREADS+ environment variable. \\

(You can also adjust by calling 
\verb+omp_set_num_threads()+).

\begin{itemize}
  \item Solaris compiler tells you what it did if you use the flags \verb+-xopenmp -xloopinfo+, or \verb+er_src+.
\end{itemize}

  


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{Variable Scoping}
OpenMP includes three keywords for variable scope and storage:
\begin{itemize}
        \item private
        \item shared
        \item threadprivate
\end{itemize}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}[fragile]
\frametitle{Private Variable}


Private: new storage, on a per-thread basis, for the variable.

The scope from the start of the region to the end of that region.

The variable is destroyed afterwards.

Thread equivalent:

  \begin{lstlisting}[language=C]
void* run(void* arg) {
    int x;
    // use x
}
  \end{lstlisting}

\end{frame}


\begin{frame}[fragile]
\frametitle{Shared Variable}

Shared: the opposite of a private variable.
 
All threads have access to the same block of data when accessing such a variable.

Thread equivalent:

  \begin{lstlisting}[language=C]
int x;

void* run(void* arg) {
    // use x
}
  \end{lstlisting}

\end{frame}


\begin{frame}[fragile]
\frametitle{Threadprivate}

Threadprivate: This is like private, in that each thread makes a copy of the variable. 

However, the scope 
is different. 

Such variables are accessible to the thread in any parallel region.

This example will make things clearer. The OpenMP code:
  \begin{lstlisting}[language=C]
int x;
#pragma omp threadprivate(x)
  \end{lstlisting}
  maps to this Pthread pseudocode:
  \begin{lstlisting}[language=C]
int x;
int x[NUM_THREADS];

void* run(void* arg) {
  // int* id = (int*) arg;
  // use x[*id]
}
  \end{lstlisting}



\end{frame}


\begin{frame}
\frametitle{I Do Declare...}

A variable may not appear in {\bf more than one clause} on the same
directive. 
(There's an exception for {\tt firstprivate} and {\tt
  lastprivate}) 
  
  By default, variables
declared in regions are private; those outside regions are
shared. 

(An exception: anything with dynamic storage is
shared).

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{Solaris, why...}


Let's look at the defaults that OpenMP uses to parallelize the {\tt parallel-for} code:
\begin{lstlisting}
% er_src parallel-for.o
     1.   <Function: calc>
    
    Source OpenMP region below has tag R1
    Private variables in R1: i
    Shared variables in R1: array2, length, array1
     2.     #pragma omp parallel for

    Source loop below has tag L1
    L1 autoparallelized
    L1 parallelized by explicit user directive
    L1 parallel loop-body code placed in function _$d1A2.calc 
                     along with 0 inner loops
    L1 multi-versioned for loop-improvement:
                     dynamic-alias-disambiguation. 
        Specialized version is L2
     3.     for (int i = 0; i < length; i++) {
     4.       array1[i] += array2[i];
     5.     }
     6.   }
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{Override!}
  
You can disable the default rules
by specifying {\tt default(none)} on the {\tt parallel},
or you can give explicit scoping:

\begin{lstlisting}
#pragma omp parallel for private(i) 
                         shared(length, array1, array2)
\end{lstlisting}



\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}
\frametitle{Directives: Parallel}

  \begin{center}
    {\tt \#pragma omp }{\bf parallel} {\it [clause [[,] clause]*]}
  \end{center}

This is the most basic directive in OpenMP.
It tells OpenMP to form a team of threads and start parallel execution.
The thread that enters the region becomes the {\bf master} (thread 0).

Allowed Clauses: {\bf if, num\_threads, default, private, firstprivate,
    shared, copyin, reduction}.

\end{frame}


\begin{frame}
\frametitle{Parallel}

 \tikzstyle{brace} = [decorate,decoration={brace},thick]
  \tikzstyle{transition} = [rectangle, draw=blue!50, fill=blue!20, thick,
                            text width=6.75cm, align=center]
  \tikzstyle{thread} = [->, >=stealth', very thick]
  \tikzstyle{master} = [->, >=stealth', very thick, red]


\begin{center}
  \begin{tikzpicture}[decoration={brace}]

    \node[transition] (begin) {{\tt \#pragma omp parallel}\\begin block};
    \node[transition] (end) [below=of begin] {end block};

    \draw[master]   (   0,   1.55) to (   0,   0.55);

    \draw[master]   (-3.5, -0.525) to (-3.5, -1.525);
    \draw[thread]   (-2.5, -0.525) to (-2.5, -1.525);
    \draw[thread]   (-1.5, -0.525) to (-1.5, -1.525);
    \draw[thread]   (-0.5, -0.525) to (-0.5, -1.525);
    \draw[thread]   ( 0.5, -0.525) to ( 0.5, -1.525);
    \draw[thread]   ( 1.5, -0.525) to ( 1.5, -1.525);
    \draw[thread]   ( 2.5, -0.525) to ( 2.5, -1.525);
    \draw[thread]   ( 3.5, -0.525) to ( 3.5, -1.525);

    \draw[master]   (   0,  -2.08) to (   0,  -3.08);

    \draw[decorate, thick, red] (-4,  -3.08) -- (-4,  -2.08)
      node[red, anchor=east, midway, outer sep=0.1cm] {Master Thread};
    \draw[decorate, thick]      (-4, -1.525) -- (-4, -0.525)
      node[anchor=east, midway, outer sep=0.1cm] {Thread Team};
    \draw[decorate, thick, red] (-4,   0.55) -- (-4,   1.55)
      node[red, anchor=east, midway, outer sep=0.1cm] {Master Thread};
\end{tikzpicture}
\end{center}

\end{frame}


\begin{frame}[fragile]
\frametitle{Parallel Code Example}

 \begin{lstlisting}[language=C]
#pragma omp parallel
{
    printf("Hello!");
}
  \end{lstlisting}

  If the number of threads is 4, this produces:
  \begin{lstlisting}[language=C]
Hello!
Hello!
Hello!
Hello!
  \end{lstlisting}

\end{frame}


\begin{frame}
\frametitle{If...}

  \begin{center}
    {\bf if}({\it primitive-expression})    
  \end{center}
The {\tt if} clause allows you to control at runtime whether or not to run
multiple threads or just one thread in its associated parallel section.


If {\it primitive-expression} evaluates to {\tt 0}, i.e. {\tt false}, then
only one thread will execute in the parallel section. 

(It's what you would expect.)


\end{frame}


\begin{frame}
\frametitle{Be Specific}

If the parallel section is going to run multiple threads (e.g. {\tt if} expression
is true; or if there is no {\tt if} expression), we can then specify how many threads.

  \begin{center}
    {\bf num\_threads}({\it integer-expression})    
  \end{center}

This spawns at most {\bf num\_threads}, depending on the number of
threads available.  

It can only guarantee the number of threads
requested if {\bf dynamic adjustment} for number of threads is off and
enough threads aren't busy.


\end{frame}


\begin{frame}[fragile]
\frametitle{Directives: Reduction}

Recall that we introduced the concept of a reduction, e.g.

\begin{lstlisting}[language=C,morekeywords={foreach,pragma,omp,parallel,single,nowait,task,untied,barrier,taskyield}]
  for (int i = 0; i < length; i++) total += array[i];
\end{lstlisting}

What is the appropriate scope for {\tt total}? 

We want each thread
to be able to write to it, but we don't want race conditions.


Fortunately, OpenMP can deal with reductions as a special case:

\verb!   #pragma omp parallel for reduction (+:total)!

\end{frame}


\begin{frame}
\frametitle{Reduction Operators}

The {\tt total} variable is the accumulator for a
reduction over {\tt +}. 

OpenMP will create local copies of {\tt total} and 
combine them at the end of the parallel region.

{\bf Operators (and their associated initial value)}
  \begin{center}
    \begin{tabular}{l r | l r | l r | l r | l r}
      + & (0) & -  &  (0) &    $\mid$ & (0) & \&\& & (1) & max & MAX\\
      * & (1) & \& & (\~{}0) & \^{} & (0) &   $\mid\mid$ & (0) & min & MIN\\ 
    \end{tabular}
  \end{center}
\end{frame}


\begin{frame}
\frametitle{Directives: For}

Inside a parallel section, we can
specify a for loop clause, as follows.

  \begin{center}
    {\tt \#pragma omp }{\bf for} {\it [clause [[,] clause]*]}
  \end{center}

    Iterations of the loop will be distributed among the
      current team of threads.
      
    Loop variable is implicitly private; OpenMP sets the
      correct values.

  Allowed Clauses: {\bf private, firstprivate, lastprivate, reduction, schedule,
    collapse, ordered, nowait}.

\end{frame}


\begin{frame}
\frametitle{More Control: schedule}

  \begin{center}
    {\bf schedule}({\it kind[, chunk\_size]})
  \end{center}

    The {\bf chunk\_size} is the number of iterations a thread
      should handle at a time. 
      
{\bf kind} is one of:
      \begin{itemize}
        \item {\bf static}
		\item {\bf dynamic}
        \item {\bf guided}
        \item {\bf auto}
        \item {\bf runtime}
      \end{itemize}

OpenMP tries to guess how many iterations to distribute to each thread
in a team.

\end{frame}

\begin{frame}[fragile]
\frametitle{Schedule Example}

The default of equal number of iterations isn't always optimal:

\begin{lstlisting}[language=C,morekeywords={foreach,pragma,omp,parallel,single,nowait,task,untied,barrier,taskyield}]
  double calc(int count) {
    double d = 1.0;
    for (int i = 0; i < count*count; i++) d += d;
    return d;
  }

  int main() {
    double data[200][200];
    int i, j;
    #pragma omp parallel for private(i, j) shared(data)
    for (int i = 0; i < 200; i++) {
      for (int j = 0; j < 200; j++) {
        data[i][j] = calc(i+j);
      }
    }
    return 0;
  }
\end{lstlisting}

Telling OpenMP to use a \emph{dynamic schedule} can enable better
parallelization.

\end{frame}


\begin{frame}
\frametitle{Collapse}

  \begin{center}
    {\bf collapse}({\it n})
  \end{center}

    This collapses {\it n} levels of loops. 
    
    Obviously, this only has
    an effect if $n \ge 2$; otherwise, nothing happens. 
    
    Collapsed loop
    variables are also made private.


\end{frame}


\begin{frame}
\frametitle{Ordered}

 \begin{center}
    {\tt \#pragma omp }{\bf ordered}
  \end{center}

    To use this directive, the containing loop must have an {\bf ordered} clause.
   
   
    OpenMP will ensure that the ordered directives are executed the same
      way the sequential loop would (one at a time).
  
  
    Each iteration of the loop may execute {\bf at most one} ordered
      directive.

\end{frame}


\begin{frame}[fragile]
\frametitle{Ordnung muss sein}

This use of Ordered does not workL

  \begin{lstlisting}
void work(int i) {
  printf("i = %d\n", i);
}
...
int i;
#pragma omp for ordered
for (i = 0; i < 20; ++i) {

  #pragma omp ordered
  work(i);

  // Each iteration of the loop has 2 "ordered" clauses!
  #pragma omp ordered 
  work(i + 100);
}
\end{lstlisting}


\end{frame}


\begin{frame}[fragile]
\frametitle{Ordnung muss wirklich sein}

Here is a valid use of the ordered clause:
 \begin{lstlisting}
void work(int i) {
  printf("i = %d\n", i);
}
...
int i;
#pragma omp for ordered
for (i = 0; i < 20; ++i) {
  if (i <= 10) {
    #pragma omp ordered
    work(i);
  }
  if (i > 10) {
    // two ordered clauses are mutually-exclusive
    #pragma omp ordered
    work(i+100);
  }
}
  \end{lstlisting}

{\bf Note:} if we change {\tt i \textgreater\- 10} to {\tt i \textgreater\- 9}, 
the use becomes invalid because the iteration $i=9$ contains two {\tt ordered}
directives.

\end{frame}


\begin{frame}[fragile]
\frametitle{Larger Example}

  \begin{lstlisting}
#include <omp.h>
#include <stdio.h>

int main(int argc, char *argv[])
{
    int j, k, a;
    #pragma omp parallel num_threads(2)
    {
        #pragma omp for collapse(2) ordered private(j,k) \
                        schedule(static,3)
        for (k = 1; k <= 3; ++k)
            for (j = 1; j <= 2; ++j) {
                #pragma omp ordered
                printf("t[%d] k=%d j=%d\n",
                       omp_get_thread_num(),
                       k, j);
            }
    }
    return 0;
}
  \end{lstlisting}

\end{frame}

\begin{frame}[fragile]
\frametitle{Output of Larger Example}

\begin{lstlisting}
    t[0] k=1 j=1
    t[0] k=1 j=2
    t[0] k=2 j=1
    t[1] k=2 j=2
    t[1] k=3 j=1
    t[1] k=3 j=2
  \end{lstlisting}

{\bf Note:} the output will be determinstic; still, the program will run two threads as long as the thread limit is at least 2.


\end{frame}


\begin{frame}[fragile]
\frametitle{Remember Parallel}

This directive is shorthand:

  \begin{center}
    {\tt \#pragma omp }{\bf parallel for} {\it [clause [[,] clause]*]}
  \end{center}

We could equally well write:

  \begin{lstlisting}
#pragma omp parallel
{
    #pragma omp for
    {

    }
}
  \end{lstlisting}


Allowed Clauses: everything allowed by {\tt parallel} and {\tt for}, except
  {\bf nowait}.



\end{frame}


\begin{frame}[fragile]
\frametitle{Directives: Sections}

 \begin{center}
    {\tt \#pragma omp }{\bf parallel sections} {\it [clause [[,] clause]*]}
  \end{center}

  As with parallel for, this is basically shorthand for:
  
  \begin{lstlisting}
#pragma omp parallel
{
    #pragma omp sections
    {

    }
}
  \end{lstlisting}

  Allowed Clauses: everything allowed by {\tt parallel} and {\tt sections}, except
  {\bf nowait}.


\end{frame}

\begin{frame}
\frametitle{All the Single Threads}

When we'd be otherwise running many threads, we can state
that some particular code block should be run by only one method.

  \begin{center}
    {\tt \#pragma omp }{\bf single}
  \end{center}
    Only a single thread executes the region following the directive.
    The thread is not guaranteed to be the master thread.

  Allowed Clauses: {\bf private, firstprivate, copyprivate, nowait}.
 Must not use {\bf copyprivate} with {\bf nowait}



\end{frame}


\begin{frame}
\frametitle{Master... Apprentice... Heartborne... 7th Seeker}

  \subsection*{Master} Sometimes we do want to guarantee that only the master
thread runs some code.
  \begin{center}
    {\tt \#pragma omp }{\bf master}
  \end{center}

    This is similar to the {\bf single} directive, except that the master thread (and only the master thread) is guaranteed to enter this region.
    No implied barriers. 

Also, no clauses.

\end{frame}


\begin{frame}
\frametitle{Man the Barricades}


  \begin{center}
    {\tt \#pragma omp }{\bf barrier}
  \end{center}

     Waits for all the threads in the team to reach the barrier before
      continuing.
      
     Loops, sections, and single all have an implicit barrer at the end of their
      region (unless you use {\bf nowait}).
      
      
     A barrier inside an {\tt if} statement must be {\tt \{  \}} separated.

     This mechanism is analogous to {\tt pthread\_barrier} in pthreads.


\end{frame}


\begin{frame}
\frametitle{Critical}

 \begin{center}
    {\tt \#pragma omp }{\bf critical} {\it [(name)]}
  \end{center}

    The enclosed region is guaranteed to only run one thread at a time
      (on a per-name basis).
    
    This is the same as a block of code in pthreads surrounded by a {\tt mutex} lock
      and unlock.

\end{frame}


\begin{frame}
\frametitle{Atomic}

We can also request atomic operations:

    {\tt \#pragma omp }{\bf atomic} [{\bf read $\mid$ write $\mid$ update $\mid$ capture}]\\
    {\it expression-stmt}
    
This ensures that a specific storage location is updated
    atomically.  
    
    Atomics should be more efficient than using critical
    sections (or else why would they include it?)


\end{frame}



\begin{frame}[fragile]
\frametitle{Oh, Think Twice...}

 Write your code so that simply eliding OpenMP directives gives a valid program. For instance, this is wrong:
  \begin{lstlisting}[language=C,morekeywords={foreach,pragma,omp,parallel,single,nowait,task,untied,barrier,taskyield}]
if (a != 0)
    #pragma omp barrier // wrong!
if (a != 0)
    #pragma omp taskyield // wrong!
  \end{lstlisting}

  Use this instead:
  \begin{lstlisting}[language=C,morekeywords={foreach,pragma,omp,parallel,single,nowait,task,untied,barrier,taskyield}]
if (a != 0) {
    #pragma omp barrier
}
if (a != 0) {
    #pragma omp taskyield
}
  \end{lstlisting}


\end{frame}



\end{document}

