\input{configuration}

\title{Lecture 19 --- Single Thread Performance, Compiler Optimizations}

\author{Patrick Lam \small \texttt{patrick.lam@uwaterloo.ca}}
\institute{Department of Electrical and Computer Engineering \\
  University of Waterloo}
\date{\today}


\begin{document}

\begin{frame}
  \titlepage

\end{frame}

\part{Single-Thread Performance}


\begin{frame}
\partpage
\end{frame}



\begin{frame}
\frametitle{Single-Thread Performance}

``Can you run faster just by trying harder?''

\begin{center}
\includegraphics[width=0.7\textwidth]{images/theflash.jpg}
\end{center}

\end{frame}


\begin{frame}
\frametitle{Single-Thread Performance}

The performance improvements we've seen to date have been leveraging parallelism
to improve throughput. 

Decreasing latency is trickier---it often requires domain-specific
tweaks. 

We'll look at one example of decreasing latency today, Stream VByte.

\end{frame}


\begin{frame}
\frametitle{I have a cunning plan...}

Even this example leverages parallelism---it uses vector instructions. 

But there
are some sequential improvements, e.g. Stream VByte takes care to be predictable
for the branch predictor.


\end{frame}
\begin{frame}
\frametitle{Problem Context}

We can abstract the problem to that of storing a sequence of small integers.


Such sequences are important in the context of inverted indexes, which allow fast lookups by term, and support boolean queries which combine terms.

Here is a list of documents and some terms that they contain:
\begin{center}
\begin{tabular}{r|l}
docid & terms \\ \hline
1 & dog, cat, cow\\
2 & cat\\
3 & dog, goat\\
4 & cow, cat, goat\\
\end{tabular}
\end{center}


\end{frame}


\begin{frame}
\frametitle{Problem Context}

The inverted index looks like this:
\begin{center}
\begin{tabular}{r|l}
term & docs \\ \hline
dog & 1, 3 \\
cat & 1, 2, 4 \\
cow & 1, 4 \\
goat & 3, 4
\end{tabular}
\end{center}

Inverted indexes contain many small integers in their lists.

The deltas are typically small if the list of doc ids is sorted.

\end{frame}


\begin{frame}
\frametitle{VByte}

VByte is one of a number of schemes that use a variable number of
bytes to store integers.  

This makes sense when most integers are
small, and especially on today's 64-bit processors.

\end{frame}


\begin{frame}
\frametitle{VByte}

VByte works like this:

\begin{itemize}
\item $x$ between 0 and $2^7-1$, e.g. $17 = 0b10001$: $0xxx xxxx$, e.g. $0001 0001$;
\item $x$ between $2^7$ and $2^{14}-1$, e.g. $1729 = 0b110 11000001$:
                   $1xxx xxxx/0xxx xxxx$, e.g. $1100 0001/0000 1101$;
\item $x$ between $2^{14}$ and $2^{21}-1$: $0xxx xxxx/1xxx xxxx/1xxx xxxx$;
\item etc.
\end{itemize}

That is, the control bit, or high-order bit, is 0 if you have finished representing the integer, and 1 if more bits remain.


\end{frame}



\begin{frame}
\frametitle{VByte}

It might seem that dealing with variable-byte integers might be
harder than dealing fixed-byte integers, and it is. 

But there are performance benefits: because we are using fewer bits! 

We can fit more information into our limited RAM and
cache, and even get higher throughput. 

Storing and reading 0s isn't an effective use of resources. 

However, a naive algorithm to decode VByte also gives lots of branch mispredictions.

\end{frame}

\begin{frame}
\frametitle{Stream VByte}

Stream VByte is a variant of VByte which works using SIMD instructions.


Science is incremental, and Stream VByte builds on earlier work---masked VByte
as well as {\sc varint}-GB and {\sc varint}-G8IU. 

The innovation in Stream VByte is to store the control and data streams separately.

\end{frame}

\begin{frame}
\frametitle{Stream VByte}

Stream VByte's control stream uses two bits per integer to represent the size of the integer:
\begin{center}
\vspace*{-1em}
\begin{tabular}{ll@{~~~~~~~~}ll}
00 & 1 byte & 10 & 3 bytes\\
01 & 2 bytes & 11 & 4 bytes
\end{tabular}
\end{center}


\end{frame}


\begin{frame}
\frametitle{Stream VByte}

Each decode iteration reads a byte from the control stream and 16 bytes of data from memory.


It uses a lookup table over the possible values of the control stream to decide how many
bytes it needs out of the 16 bytes it has read.

It then uses SIMD instructions to shuffle
the bits each into their own integers. 

Note that, unlike VByte, Stream VByte uses all 8 bits
of each data byte as data.


\end{frame}



\begin{frame}
\frametitle{Stream VByte Example}

For instance, if the control stream contains $0b1000~1100$, then the data stream
contains the following sequence of integer sizes: $3, 1, 4, 1$. 

Out of the 16 bytes read,
this iteration will use 9 bytes; it advances the data pointer by 9. 

It then uses the SIMD
``shuffle'' instruction to put the decoded integers from the data stream at known positions in the
128-bit SIMD register. 

In this case, it pads the first 3-byte integer with 1 byte, then
the next 1-byte integer with 3 bytes, etc. 

\end{frame}

\begin{frame}
\frametitle{Stream VByte Example}

Let's say that the input is
{\tt 0xf823~e127~2524~9748~1b..~....~....~....}. 

The 128-bit output is
{\tt 0x00f8~23e1/0000~0027/2524 9748/0000/001b}, with the /s denoting separation
between outputs. 

The shuffle mask is precomputed and, at
execution time, read from an array.

\end{frame}

\begin{frame}[fragile]
\frametitle{SIMD Instructions}

The core of the implementation uses three SIMD instructions:
\begin{lstlisting}[language=C]
  uint8_t C = lengthTable[control];
  __m128i Data = _mm_loadu_si128 ((__m128i *) databytes);
  __m128i Shuf = _mm_loadu_si128(shuffleTable[control]);
  Data = _mm_shuffle_epi8(Data, Shuf);
  databytes += C; control++;
\end{lstlisting}

\end{frame}


\begin{frame}
\frametitle{But Does It Work?!}

Stream VByte performs better than previous techniques on a realistic input.


Let's discuss how it achieves this performance.

\begin{itemize}
\item control bytes are sequential: the processor can always prefetch the next control byte, because
its location is predictable;
\item data bytes are sequential and loaded at high throughput;
\item shuffling exploits the instruction set so that it takes 1 cycle;
\item control-flow is regular (executing only the tight loop which retrieves/decodes control
and data; there are no conditional jumps).
\end{itemize}

\end{frame}

\part{Compiler Optimizations}


\begin{frame}
\partpage
\end{frame}



\begin{frame}
\frametitle{Lunch Time, Already?}

``Is there any such thing as a free lunch?''

Compiler optimizations really do feel like a free lunch.


But what does it really mean when you say {\tt -O2}?


We'll see some representative compiler optimizations and discuss how
they can improve program performance. 

\end{frame}

\begin{frame}
\frametitle{You Do Your Job...}

I'll point out cases that stop compilers
from being able to optimize your code. 

In general, it's better if the
compiler automatically does a performance-improving transformation
rather than you doing it manually.

It's probably a waste of time for
you and it also makes your code less readable.

\end{frame}


\begin{frame}
\frametitle{References}

Many pages on the Internet describe
optimizations. 

Here's one that contains good examples:

$\qquad \qquad$ \url{http://www.digitalmars.com/ctg/ctgOptimizer.html}

You can find a full list of {\tt gcc} options here:

$\qquad \qquad$ \url{http://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html}


\end{frame}

\begin{frame}
\frametitle{About Compiler Optimizations}

First of all, ``optimization'' is
a bit of a misnomer. 

Compilers generally don't generate ``optimal'' code. They generate \emph{better} code.

\begin{center}
\includegraphics[width=0.7\textwidth]{images/bender.jpg}
\end{center}


\end{frame}


\begin{frame}
\frametitle{Optimization Levels}

Here's what {\tt -On} means for
{\tt gcc}. Other compilers have similar (but not identical) optimization flags.

\begin{itemize}
\item {\tt -O0} (default): Fastest compilation time. Debugging works as expected.
\item {\tt -O1} ({\tt -O}): Reduce code size and execution time.
 No optimizations that increase compiliation time.
\item {\tt -O2}: All optimizations except space vs. speed tradeoffs.
\item {\tt -O3}: All optimizations.
\item {\tt -Ofast}: All {\tt -O3} optimizations, plus non-standards compliant optimizations,
      particularly {\tt -ffast-math}.
\end{itemize}

\end{frame}


\begin{frame}
\frametitle{Scalar Optimizations: Constant Folding}

Tag line: ``Why do later something you can do now?''

\begin{center}
\vspace*{-1em}
\begin{tabular}{lll}
i = 1024 * 1024 &
$\Longrightarrow$ &
i = 1048576
\end{tabular}
\end{center}

\noindent \emph{Enabled at all optimization levels.} 

The compiler will not emit code that does the multiplication at runtime.

\end{frame}


\begin{frame}[fragile]
\frametitle{Common Subexpression Elimination}

We can do common subexpression elimination
when the same expression {\tt x~op~y} is computed more than once. 

Neither {\tt x} nor {\tt y} may change between the two computations. 

\begin{lstlisting}[language=C]
   a = (c + d) * y;
   b = (c + d) * z;

   w = 3;
   x = f(); y = x;
   z = w + y;

\end{lstlisting}

\emph{Enabled at {\tt -O2}, {\tt -O3} or with {\tt -fgcse}}

\end{frame}

\begin{frame}
\frametitle{Constant Propagation}

Moves constant values from definition to
use. 

The transformation is valid if there are no redefinitions of the
variable between the definition and its use.

 In the above example,
we can propagate the constant value 3 to its use in {\tt z = w + y},
yielding {\tt z = 3 + y}.



\end{frame}

\begin{frame}
\frametitle{Copy Propagation}

A bit more sophisticated than constant
propagation---telescopes copies of variables from their definition to
their use. 

Using it, we can replace the
last statement with {\tt z = w + x}. 

If we run both constant and copy
propagation together, we get {\tt z = 3 + x}.

These scalar optimizations are more complicated in the presence
of pointers, e.g. {\tt z = *w + y}.

\end{frame}


\begin{frame}[fragile]
\frametitle{Redundant Code Optimizations}

Dead code elimination: removes code that is guaranteed to not execute.

\begin{center}
\vspace*{-2em}
\begin{minipage}{.3\textwidth}
\begin{lstlisting}[language=C]
  int f(int x) {
    return x * 2;
  }
  \end{lstlisting}
  \end{minipage} \begin{minipage}{.3\textwidth}
\begin{lstlisting}[language=C]
  int g() {
    if (f(5) % 2 == 0) {
      // do stuff...
    } else {
      // do other stuff
    }
  }
\end{lstlisting}
\end{minipage}
\end{center}

The general problem, as with many other compiler problems, is undecidable.


\end{frame}


\begin{frame}
\frametitle{Loop Optimizations}

Loop optimizations are particularly profitable when loops execute
often. 

This is often a win, because programs spend a lot of time looping.


The trick is to find which loops are going to be the important ones.

A loop induction variable is a variable that varies on each iteration
of the loop. 

The loop variable is definitely a loop induction variable
but there may be others. 

\emph{Induction variable elimination} gets
rid of extra induction variables.



\end{frame}

\begin{frame}
\frametitle{Scalar Replacement}

\emph{Scalar replacement} replaces an array read {\tt a[i]}
occuring multiple times with a single read {\tt temp = a[i]} and references
to {\tt temp} otherwise. 

It needs to know that {\tt a[i]} won't change
between reads.

Sane languages include array bounds checks. 

Loop optimizations
can eliminate array bounds checks if they can prove that the loop
never iterates past the array bounds.


\end{frame}

\begin{frame}[fragile]
\frametitle{Loop Unrolling}

This
lets the processor run more code without having to branch
as often. 

\emph{Software pipelining} is a synergistic optimization,
which allows multiple iterations of a loop to proceed in parallel.


This optimization is also useful for SIMD. Here's an example.
\begin{center}
\vspace*{-1em}
\begin{minipage}{.3\textwidth}
  \begin{lstlisting}[language=C]
for (int i = 0; i < 4; ++i)
    f(i)
  \end{lstlisting}
  \end{minipage} $\Longrightarrow \hspace*{2em}$ \begin{minipage}{.4\textwidth}
  \begin{lstlisting}[language=C]
f(0); f(1); f(2); f(3);
  \end{lstlisting}
  \end{minipage}
  \end{center}
\noindent \emph{Enabled with {\tt -funroll-loops}.}

\end{frame}

\begin{frame}[fragile]
\frametitle{Loop Interchange}

This optimization can give big wins
for caches (which are key); it changes the nesting of loops to
coincide with the ordering of array elements in memory. 


\begin{center}
\vspace*{-1em}
\begin{minipage}{.4\textwidth}
  \begin{lstlisting}[language=C]
for (int i = 0; i < N; ++i)
    for (int j = 0; j < M; ++j)
        a[j][i] = a[j][i] * c
  \end{lstlisting}
  \end{minipage} $\Longrightarrow \hspace*{2em}$ \begin{minipage}{.4\textwidth}
  \begin{lstlisting}[language=C]
for (int j = 0; j < M; ++j)
    for (int i = 0; i < N; ++i)
        a[j][i] = a[j][i] * c
  \end{lstlisting}
  \end{minipage}
  \end{center}
  since C is \emph{row-major} (meaning a[1][1] is beside a[1][2]),
rather than \emph{column-major}.

\noindent
\emph{Enabled with {\tt -floop-interchange}.}

\end{frame}

\begin{frame}[fragile]
\frametitle{Loop Fusion}

This optimization is like the OpenMP collapse
construct; we transform
\begin{center}
\vspace*{-1em}
\begin{minipage}{.3\textwidth}
  \begin{lstlisting}[language=C]
for (int i = 0; i < 100; ++i)
    a[i] = 4

for (int i = 0; i < 100; ++i)
    b[i] = 7
  \end{lstlisting}
  \end{minipage} $\Longrightarrow \hspace*{2em}$ \begin{minipage}{.4\textwidth}
  \begin{lstlisting}[language=C]
for (int i = 0; i < 100; ++i) {
    a[i] = 4
    b[i] = 7
}
  \end{lstlisting}
  \end{minipage}
  \end{center}
There's a trade-off between data locality and loop overhead.

Sometimes the inverse transformation, \emph{loop fission}, will
improve performance.

\end{frame}

\begin{frame}[fragile]
\frametitle{Loop-Invariant Code Motion}

 Also known as \emph{Loop hoisting},
this optimization moves calculations out of a loop. 
\begin{center}
\vspace*{-1em}
\begin{minipage}{.3\textwidth}
  \begin{lstlisting}[language=C]
for (int i = 0; i < 100; ++i) {
    s = x * y;
    a[i] = s * i;
}
  \end{lstlisting}
  \end{minipage} $\Longrightarrow \hspace*{2em}$ \begin{minipage}{.4\textwidth}
  \begin{lstlisting}[language=C]
s = x * y;
for (int i = 0; i < 100; ++i) {
    a[i] = s * i;
}
  \end{lstlisting}
  \end{minipage}
  \end{center}

This reduces the amount of work we have to do for each iteration of the loop.

\end{frame}


\begin{frame}[fragile]
\frametitle{Miscellaneous Low-Level Optimizations}

Some optimizations affect low level code generation; here are two examples.

{\tt gcc} attempts to guess the probability of each branch to
best order the code. (For an {\tt if}, fall-through is most efficient. Why?)

This isn't quite an optimization, but you can use {\tt
  \_\_builtin\_expect(expr, value)} to help gcc, if you know the runtime behaviour...
  
  In the Linux Kernel:
  \begin{lstlisting}[language=C]
#define likely(x)       __builtin_expect((x),1)
#define unlikely(x)     __builtin_expect((x),0)
  \end{lstlisting}



\end{frame}


\begin{frame}
\frametitle{Architecture-Specific}

{\tt gcc} can also generate code tuned to particular
processors. 

You can specify this using {\tt
  -march} and {\tt -mtune}. ({\tt -march} implies {\tt -mtune}).


This will enable specific instructions that not all CPUs support (e.g. SSE4.2).
For example, {\tt -march=corei7}.

\noindent
Good to use on your local machine or your cloud servers, not ideal for code you ship to others.


\end{frame}





\end{document}

