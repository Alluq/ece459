\input{configuration}

\title{Lecture 19 --- Performance Case Studies}

\author{Patrick Lam \\ \small \texttt{patrick.lam@uwaterloo.ca}}
\institute{Department of Electrical and Computer Engineering \\
  University of Waterloo}
\date{\today}


\begin{document}

\begin{frame}
  \titlepage

\end{frame}


\part{Firefox}

\begin{frame}
\frametitle{Case Study: Firefox Quantum}

\begin{center}
\includegraphics[width=.6\textwidth]{images/L19-logo-quantum.png}
\end{center}

\end{frame}

\begin{frame}[fragile]
\frametitle{Some Firefox Perf Improvements, per Mike Conley}

\begin{minipage}{.5\textwidth}
\begin{itemize}
\item don't animate out-of-view elements
\item move db init off main thread
\item keep better profiling data
\item parallel painting for macOS
\item lazily instantiate Search Service
\end{itemize}
\end{minipage}\begin{minipage}{.5\textwidth}
\begin{itemize}
\item halve size of the blocklist
\item refactor to reduce main-thread IO
\item don't hold all frames of animated GIFs/APNGs in memory
\item eliminate unnecessary hash table
\item use more modern compiler
\end{itemize}
\end{minipage}

\begin{center}
\url{https://mikeconley.ca/blog/2018/02/14/firefox-performance-update-1/}
\end{center}

\end{frame}

\begin{frame}
\large
\frametitle{We've seen this before:}
\begin{itemize}
\item do less work (or do it sooner/later);
\item use threads (move work off main thread);
\item measure performance;
\end{itemize}
Which of the updates fall into which categories?
\end{frame}

\begin{frame}
\frametitle{Tab Warming}
\begin{center}
\url{https://mikeconley.ca/blog/2018/01/11/making-tab-switching-faster-in-firefox-with-tab-warming/}.
\end{center}

\begin{minipage}[t]{.5\textwidth}
\vspace{0pt}
\includegraphics[width=.95\columnwidth]{images/L19-hot-chocolate.jpg}
\end{minipage}%
\begin{minipage}[t]{.5\textwidth}
\vspace{0pt}
\begin{flushleft}
``Maybe this is my Canadian-ness showing, but I like to think of it almost like coming in from shoveling snow off of the driveway, and somebody inside has \emph{already made hot chocolate for you}, because they knew you’d probably be cold.'' --- Mike Conley
\end{flushleft}
\end{minipage}

\end{frame}

\begin{frame}
\frametitle{How Can Tab Warming Work?}
\Large

Before: Firefox requests paint of newly-active tab,\\
and then waits for the result before switching.

Idea: reduce user-visible latency by predicting an imminent tab switch.

Q: How can we predict the future?

Q': How can we predict which tab will be switched to?

\end{frame}

\begin{frame}
\frametitle{Predicting the future}
\large

A: When the user has a mouse, then the mouse cursor will hover over the next tab.

\vspace*{1em}
\begin{quote}
Assuming a sufficiently long delay between hover and click, the tab switch should be perceived as instantaneous. If the delay was non-zero but still not long enough, we will have nonetheless shaved that time off in eventually presenting the tab to you.

And in the event that we were wrong, and you weren’t interested in seeing the tab, we eventually throw the uploaded layers away.
\end{quote}

Blog post does not report performance numbers.
\end{frame}

\begin{frame}[fragile]
\frametitle{Some Firefox Perf Improvements, per Mike Conley}

\begin{minipage}{.5\textwidth}
\begin{itemize}
\item don't animate out-of-view elements
\item move db init off main thread
\item keep better profiling data
\item parallel painting for macOS
\item lazily instantiate Search Service
\end{itemize}
\end{minipage}\begin{minipage}{.5\textwidth}
\begin{itemize}
\item halve size of the blocklist
\item refactor to reduce main-thread IO
\item don't hold all frames of animated GIFs/APNGs in memory
\item eliminate unnecessary hash table
\item use more modern compiler
\end{itemize}
\end{minipage}

\begin{center}
\url{https://mikeconley.ca/blog/2018/02/14/firefox-performance-update-1/}
\end{center}

\end{frame}

\part{Single-Thread Performance}



\begin{frame}
\frametitle{Single-Thread Performance}

``Can you run faster just by trying harder?''

\begin{center}
\includegraphics[width=0.7\textwidth]{images/theflash.jpg}
\end{center}

\end{frame}


\begin{frame}
\frametitle{Single-Thread Performance}
\Large
\begin{changemargin}{2cm}
Performance improvements to date have \\
used parallelism
to improve throughput. 

Decreasing latency is trickier---\\
often requires domain-specific
tweaks. 

Today: one example of decreasing latency: \\
\hspace*{2em} Stream VByte.
\end{changemargin}

\end{frame}


\begin{frame}
\frametitle{I have a cunning plan...}

\Large
\begin{changemargin}{2cm}
Even Stream VByte uses parallelism:\\
\hspace*{2em}vector instructions. 

But there
are sequential improvements, \\
e.g. Stream VByte takes care to be predictable
for the branch predictor.
\end{changemargin}

\end{frame}
\begin{frame}
\frametitle{Inverted Indexes (like it's CS 137 again!)}

\Large
\begin{changemargin}{1cm}
\vspace*{-1em}
Abstractly: store a sequence of small integers.


Why Inverted indexes?

\hspace*{1cm}allow fast lookups by term;\\
\hspace*{1cm}support boolean queries combining terms.

\end{changemargin}

\end{frame}
\begin{frame}
\frametitle{Dogs, cats, cows, goats. In ur documents.}
\Large
\begin{changemargin}{2cm}
\begin{center}
\begin{tabular}{r|l}
docid & terms \\ \hline
1 & dog, cat, cow\\
2 & cat\\
3 & dog, goat\\
4 & cow, cat, goat\\
\end{tabular}
\end{center}
\end{changemargin}

\end{frame}


\begin{frame}
\frametitle{Inverting the Index}

\large\begin{changemargin}{2cm}
Here's the index and the inverted index:
\begin{center}
\begin{tabular}{r|l}
docid & terms \\ \hline
1 & dog, cat, cow\\
2 & cat\\
3 & dog, goat\\
4 & cow, cat, goat\\
\end{tabular} \hspace*{2em}
\begin{tabular}{r|l}
term & docs \\ \hline
dog & 1, 3 \\
cat & 1, 2, 4 \\
cow & 1, 4 \\
goat & 3, 4
\end{tabular}
\end{center}

Inverted indexes contain many small integers.

Deltas typically small if doc ids are sorted.
\end{changemargin}

\end{frame}


\begin{frame}
\frametitle{Storing inverted index lists: VByte}

\Large\begin{changemargin}{2cm}
\vspace*{-2em}
VByte uses a variable number of bytes\\
to store integers.  

Why? Most integers are
small,\\
especially on today's 64-bit processors.
\end{changemargin}
\end{frame}


\begin{frame}
\frametitle{How VByte Works}

\large\begin{changemargin}{1cm}
VByte works like this:

\begin{itemize}
\item $x$ between 0 and $2^7-1$ (e.g. $17 = 0b10001$):\\
\hspace*{1em}$0xxx xxxx$, e.g. $0001 0001$;
\item $x$ between $2^7$ and $2^{14}-1$ (e.g. $1729 = 0b110 11000001$):\\
\hspace*{1em}                   $1xxx xxxx/0xxx xxxx$ (e.g. $1100 0001/0000 1101$);\\
\item $x$ between $2^{14}$ and $2^{21}-1$: \\
\hspace*{1em}$0xxx xxxx/1xxx xxxx/1xxx xxxx$;
\item etc.
\end{itemize}

Control bit, or high-order bit, is:\\
\hspace*{2em}0 once done representing the int,\\
\hspace*{2em}1 if more bits remain.
\end{changemargin}

\end{frame}



\begin{frame}
\frametitle{Why VByte Helps}

\large\begin{changemargin}{2cm}
Isn't dealing with variable-byte integers harder?\\
\hspace*{2em} $\bullet$~ Yup!

But perf improves: \\
\hspace*{2em} $\bullet$~  We are using fewer bits! 

We fit more information into RAM and
cache, and can get higher throughput. (think inlining)

Storing and reading 0s isn't good use of resources. 

However, a naive algorithm to decode VByte \\
gives branch mispredicts.
\end{changemargin}
\end{frame}

\begin{frame}
\frametitle{Stream VByte}
\vspace*{-2em}
\large\begin{changemargin}{2cm}
Stream VByte: a variant of VByte using SIMD.


Science is incremental. \\
Stream VByte builds on earlier work---\\
\hspace*{2em}masked VByte, {\sc varint}-GB, {\sc varint}-G8IU. 

Innovation in Stream VByte:\\
\hspace*{2em}\emph{store the control and data streams separately}.
\end{changemargin}

\end{frame}

\begin{frame}
\frametitle{Control Stream}
\vspace*{-4em}
\large\begin{changemargin}{2cm}
Stream VByte's control stream uses two bits per integer to represent the size of the integer:
\begin{center}
\vspace*{-3em}
\begin{tabular}{ll@{~~~~~~~~}ll}
00 & 1 byte & 10 & 3 bytes\\
01 & 2 bytes & 11 & 4 bytes
\end{tabular}
\end{center}
\end{changemargin}

\end{frame}


\begin{frame}
\frametitle{Decoding Stream VByte}

\vspace*{-4em}
\large\begin{changemargin}{2cm}
Per decode iteration:\\
\hspace*{2em} reads 1 byte from the control stream,\\
\hspace*{2em} and 16 bytes of data.


Lookup table on control stream byte: decide how many
bytes it needs out of the 16 bytes it has read.

SIMD instructions: \\
\hspace*{2em}shuffle the bits each into their own integers. 

Unlike VByte, \\
Stream VByte uses all 8 bits
of data bytes as data.
\end{changemargin}

\end{frame}



\begin{frame}
\frametitle{Stream VByte Example}

\large\begin{changemargin}{2cm}
Say control stream contains $0b1000~1100$. \\
Then the data stream
contains the following sequence of integer sizes: $3, 1, 4, 1$. 

Out of the 16 bytes read,
this iteration uses 9 bytes; \\
\hspace*{2em} $\Rightarrow$ it advances the data pointer by 9. 

The SIMD
``shuffle'' instruction puts decoded integers from data stream at known positions in the
128-bit SIMD register.

Pad the first 3-byte integer with 1 byte, then
the next 1-byte integer with 3 bytes, etc. 
\end{changemargin}
\end{frame}

\begin{frame}
\frametitle{Stream VByte: Shuffling the Bits}
\vspace*{-1em}
\large\begin{changemargin}{1cm}
Say the data input is:\\
{\tt 0xf823~e127~2524~9748~1b..~....~....~....}. 

The 128-bit output is:\\
{\tt 0x00f8~23e1/0000~0027/2524 9748/0000/001b}\\
/s denote separation
between outputs. 

Shuffle mask is precomputed and
read from an array.
\end{changemargin}
\end{frame}

\begin{frame}[fragile]
\frametitle{SIMD Instructions}

\vspace*{-1em}
\large\begin{changemargin}{1cm}
The core of the implementation uses\\
three SIMD instructions:
\begin{lstlisting}[language=C]
  uint8_t C = lengthTable[control];
  __m128i Data = _mm_loadu_si128 ((__m128i *) databytes);
  __m128i Shuf = _mm_loadu_si128(shuffleTable[control]);
  Data = _mm_shuffle_epi8(Data, Shuf);
  databytes += C; control++;
\end{lstlisting}
\end{changemargin}
\end{frame}


\begin{frame}
\frametitle{But Does It Work?!}

\vspace*{-1em}
\large\begin{changemargin}{1cm}
Stream VByte performs better than previous techniques on a realistic input.


Why?

\begin{itemize}
\item control bytes are sequential:\\
\hspace*{1em} CPU can always prefetch the next control byte, \\
\hspace*{1em} because
its location is predictable;
\item data bytes are sequential \\
\hspace*{1em}and loaded at high throughput;
\item shuffling exploits the instruction set: \\
\hspace*{1em}takes 1 cycle;
\item control-flow is regular \\
\hspace*{1em}(tight loop which retrieves/decodes control
\& data;\\
\hspace*{1em}no conditional jumps).
\end{itemize}
\end{changemargin}
\end{frame}



\end{document}

