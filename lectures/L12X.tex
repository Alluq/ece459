\include{header}

\begin{document}

\lecture{12 --- OpenMP}{\term}{Patrick Lam}

We are going to go and present OpenMP in more detail. This will help you 
use it in Assignment 2. I didn't present these notes in class except as a series
of examples. These notes, though, may help you out by serving as a readable API guide.

OpenMP is a portable, easy-to-use parallel programming API. It combines
compiler directives with runtime library routines and uses environment variables
for setup. Note that you can detect its presence in your code with {\tt \#ifdef \_OPENMP}.

For exhaustive documentation, see \url{http://www.openmp.org/mp-documents/OpenMP3.1.pdf}.

\paragraph{Directive Format.} All OpenMP directives follow this pattern:
  \begin{center}
    {\tt \#pragma omp }{\it directive-name [clause [[,] clause]*]}
  \end{center}

There are {\bf 16} OpenMP directives. Each directive applies to the immediately-following
statement, which is either a single statement or a compound statement {\bf \{  $\cdots$ \}}.

Most clauses have a \emph{list} as an argument. 
A list is a comma-separated list of list items. For C and C++, a list item 
is simply a variable name.

\section*{Data Terminology}

OpenMP includes three keywords for variable scope and storage:
\squishlist
        \item private;
        \item shared; and
        \item threadprivate.
\squishend

\paragraph{Private Variables.} You can declare private variables
with the {\tt private} clause. This creates new storage, on a per-thread
basis, for the variable---it does not copy variables. The scope
of the variable extends from the start of the region where the variable
exists to the end of that region; the variable is destroyed afterwards.

Some Pthread pseudocode for private variables:

  \begin{lstlisting}[language=C]
void* run(void* arg) {
    int x;
    // use x
}
  \end{lstlisting}

\paragraph{Shared Variables.} The opposite of a private variable
is a {\tt shared} variable. All threads have access to the same block
of data when accessing such a variable.

The relevant Pthread pseudocode is:
  \begin{lstlisting}[language=C]
int x;

void* run(void* arg) {
    // use x
}
  \end{lstlisting}

\paragraph{Thread-Private Variables.} Finally, OpenMP supports 
{\tt threadprivate} variables. This is like a {\tt private} variable
in that each thread makes a copy of the variable. However, the scope 
is different. Such variables are accessible to the thread in any parallel region.

This example will make things clearer. The OpenMP code:
  \begin{lstlisting}[language=C]
int x;
#pragma omp threadprivate(x)
  \end{lstlisting}
  maps to this Pthread pseudocode:
  \begin{lstlisting}[language=C]
int x;
int x[NUM_THREADS];

void* run(void* arg) {
  // use x[pthread_self()]
}
  \end{lstlisting}

A variable may not appear in {\bf more than one clause} on the same
directive. (There's an exception for {\tt firstprivate} and {\tt
  lastprivate}, which we'll see later.) By default, variables
declared in regions are private; those outside regions are
shared. (An exception: anything with dynamic storage is
shared).

\section*{Directives}
We'll talk about the different OpenMP directives next. These are the key
language features you'll use to tell OpenMP what to parallelize.

\subsection*{Parallel}
  \begin{center}
    {\tt \#pragma omp }{\bf parallel} {\it [clause [[,] clause]*]}
  \end{center}

This is the most basic directive in OpenMP.
It tells OpenMP to form a team of threads and start parallel execution.
The thread that enters the region becomes the {\bf master} (thread 0).

Allowed Clauses: {\bf if, num\_threads, default, private, firstprivate,
    shared, copyin, reduction}.

Figure~\ref{fig:parallel} illustrates what {\tt parallel} does.
By default, the number of threads used is set globally, either automatically
or manually. After the parallel block, the thread team sleeps until it's needed again.

  \tikzstyle{brace} = [decorate,decoration={brace},thick]
  \tikzstyle{transition} = [rectangle, draw=blue!50, fill=blue!20, thick,
                            text width=6.75cm, align=center]
  \tikzstyle{thread} = [->, >=stealth', very thick]
  \tikzstyle{master} = [->, >=stealth', very thick, red]

  \begin{figure}
\begin{center}
  \begin{tikzpicture}[decoration={brace}]

    \node[transition] (begin) {{\tt \#pragma omp parallel}\\begin block};
    \node[transition] (end) [below=of begin] {end block};

    \draw[master]   (   0,   1.55) to (   0,   0.55);

    \draw[master]   (-3.5, -0.525) to (-3.5, -1.525);
    \draw[thread]   (-2.5, -0.525) to (-2.5, -1.525);
    \draw[thread]   (-1.5, -0.525) to (-1.5, -1.525);
    \draw[thread]   (-0.5, -0.525) to (-0.5, -1.525);
    \draw[thread]   ( 0.5, -0.525) to ( 0.5, -1.525);
    \draw[thread]   ( 1.5, -0.525) to ( 1.5, -1.525);
    \draw[thread]   ( 2.5, -0.525) to ( 2.5, -1.525);
    \draw[thread]   ( 3.5, -0.525) to ( 3.5, -1.525);

    \draw[master]   (   0,  -2.08) to (   0,  -3.08);

    \draw[decorate, thick, red] (-4,  -3.08) -- (-4,  -2.08)
      node[red, anchor=east, midway, outer sep=0.1cm] {Master Thread};
    \draw[decorate, thick]      (-4, -1.525) -- (-4, -0.525)
      node[anchor=east, midway, outer sep=0.1cm] {Thread Team};
    \draw[decorate, thick, red] (-4,   0.55) -- (-4,   1.55)
      node[red, anchor=east, midway, outer sep=0.1cm] {Master Thread};
\end{tikzpicture}
\end{center}
\caption{\label{fig:parallel}Visual view of {\tt parallel} directive.}
\end{figure}

\paragraph{Code Example: Parallel.}
  \begin{lstlisting}[language=C]
#pragma omp parallel
{
    printf("Hello!");
}
  \end{lstlisting}

  If the number of threads is 4, this produces:
  \begin{lstlisting}[language=C]
Hello!
Hello!
Hello!
Hello!
  \end{lstlisting}

\paragraph{{\tt if} and {\tt num\_threads} Clauses.} Directives take clauses.
The first one we'll see (besides the variable scope clauses) are {\tt if} and
{\tt num\_threads}.

  \begin{center}
    {\bf if}({\it primitive-expression})    
  \end{center}
The {\tt if} clause allows you to control at runtime whether or not to run
multiple threads or just one thread in its associated parallel section.
If {\it primitive-expression} evaluates to {\tt 0}, i.e. {\tt false}, then
only one thread will execute in the parallel section. (It's what you would expect.)

\newpage
If the parallel section is going to run multiple threads (e.g. {\tt if} expression
is true; or if there is no {\tt if} expression), we can then specify how many threads.
  \begin{center}
    {\bf num\_threads}({\it integer-expression})    
  \end{center}

This spawns at most {\bf num\_threads}, depending on the number of
threads available.  It can only guarantee the number of threads
requested if {\bf dynamic adjustment} for number of threads is off and
enough threads aren't busy.

\paragraph{{\tt reduction} Clause.} We saw reductions last time. Here's
another look.
  \begin{center}
    {\bf reduction}({\it operator:list})
  \end{center}

{\bf Operators (and their associated initial value)}
  \begin{center}
    \begin{tabular}{l r | l r | l r | l r | l r}
      + & (0) & -  &  (0) &    $\mid$ & (0) & \&\& & (1) & max & MAX\\
      * & (1) & \& & (\~{}0) & \^{} & (0) &   $\mid\mid$ & (0) & min & MIN\\ 
    \end{tabular}
  \end{center}

Each thread gets a \emph{private} copy of the variable.
The variable is initialized by OpenMP (so you don't need to do anything else).
At the end of the region, OpenMP updates your result using the operator.

\paragraph{{\tt reduction} Clause Pthreads Pseudocode.}
  \begin{lstlisting}
void* run(void* arg) {
    variable = initial value;
    // code inside block---modifies variable
    return variable;
}

// ... later in master thread (sequentially):
variable = initial value
for t in threads {
    thread_variable
    pthread_join(t, &thread_variable);
    variable = variable (operator) thread_variable;
}
  \end{lstlisting}
\newpage
\subsection*{(For) Loop Directive} Inside a parallel section, we can
specify a for loop clause, as follows.

  \begin{center}
    {\tt \#pragma omp }{\bf for} {\it [clause [[,] clause]*]}
  \end{center}

    Iterations of the loop will be distributed among the
      current team of threads.
    This clause only supports simple ``for'' loops with invariant bounds (bounds do
      not change during the loop).
    Loop variable is implicitly private; OpenMP sets the
      correct values.

  Allowed Clauses: {\bf private, firstprivate, lastprivate, reduction, schedule,
    collapse, ordered, nowait}.

\paragraph{{\tt schedule} Clause.}

  \begin{center}
    {\bf schedule}({\it kind[, chunk\_size]})
  \end{center}

    The {\bf chunk\_size} is the number of iterations a thread
      should handle at a time. {\bf kind} is one of:
      \begin{itemize}
        \item {\bf static}:  divides the number of iterations into chunks and assigns each thread
        a chunk in round-robin fashion (before the loop executes).

        \item {\bf dynamic}: divides the number of iterations into chunks and assigns each
        available thread a chunk, until there are no chunks left.
        \item {\bf guided}:  same as dynamic, except {\bf chunk\_size} represents the minimum
        size. This starts by dividing the loop into large chunks, and decreases the
        chunk size as fewer iterations remain.
        \item {\bf auto}: obvious (OpenMP decides what's best for you).
        \item {\bf runtime}: also obvious; we'll see how to adjust this later.
      \end{itemize}

\paragraph{{\tt collapse} and {\tt ordered} Clauses.}

  \begin{center}
    {\bf collapse}({\it n})
  \end{center}

    This collapses {\it n} levels of loops. Obviously, this only has
    an effect if $n \ge 2$; otherwise, nothing happens. Collapsed loop
    variables are also made private.

  \begin{center}
    {\bf ordered}
  \end{center}
  
    This enables the use of {\tt ordered} directives inside loop, which we'll see below.

\subsection*{Ordered directive}
  \begin{center}
    {\tt \#pragma omp }{\bf ordered}
  \end{center}

    To use this directive, the containing loop must have an {\bf ordered} clause.
    OpenMP will ensure that the ordered directives are executed the same
      way the sequential loop would (one at a time).
    Each iteration of the loop may execute {\bf at most one} ordered
      directive.

Let's see what that means by way of two examples.

\paragraph{Invalid Use of Ordered.} This doesn't work.

{\small
  \begin{lstlisting}
void work(int i) {
  printf("i = %d\n", i);
}
...
int i;
#pragma omp for ordered
for (i = 0; i < 20; ++i) {

  #pragma omp ordered
  work(i);

  // Each iteration of the loop has 2 "ordered" clauses!
  #pragma omp ordered 
  work(i + 100);
}
  \end{lstlisting}
}

\paragraph{Valid Use of Ordered.} This does.
{\small  \begin{lstlisting}
void work(int i) {
  printf("i = %d\n", i);
}
...
int i;
#pragma omp for ordered
for (i = 0; i < 20; ++i) {
  if (i <= 10) {
    #pragma omp ordered
    work(i);
  }
  if (i > 10) {
    // two ordered clauses are mutually-exclusive
    #pragma omp ordered
    work(i+100);
  }
}
  \end{lstlisting}
}

{\bf Note:} if we change {\tt i \textgreater\- 10} to {\tt i \textgreater\- 9}, 
the use becomes invalid because the iteration $i=9$ contains two {\tt ordered}
directives.

  \paragraph{Tying It All Together.} Here's a larger example.

  \begin{lstlisting}
#include <omp.h>
#include <stdio.h>

int main(int argc, char *argv[])
{
    int j, k, a;
    #pragma omp parallel num_threads(2)
    {
        #pragma omp for collapse(2) ordered private(j,k) \
                        schedule(static,3)
        for (k = 1; k <= 3; ++k)
            for (j = 1; j <= 2; ++j) {
                #pragma omp ordered
                printf("t[%d] k=%d j=%d\n",
                       omp_get_thread_num(),
                       k, j);
            }
    }
    return 0;
}
  \end{lstlisting}

\paragraph{Output of Previous Example.} And here's what it does.

{\small
  \begin{lstlisting}
    t[0] k=1 j=1
    t[0] k=1 j=2
    t[0] k=2 j=1
    t[1] k=2 j=2
    t[1] k=3 j=1
    t[1] k=3 j=2
  \end{lstlisting}
}

  {\bf Note:} the output will be determinstic; still, the program will run two threads as
  long as the thread limit is at least 2.

\subsection*{Parallel Loop Directive} This directive is shorthand:

  \begin{center}
    {\tt \#pragma omp }{\bf parallel for} {\it [clause [[,] clause]*]}
  \end{center}

We could equally well write:
{\small
  \begin{lstlisting}
#pragma omp parallel
{
    #pragma omp for
    {

    }
}
  \end{lstlisting}
}
but the single directive is shorter; this idiom happens a lot.

Allowed Clauses: everything allowed by {\tt parallel} and {\tt for}, except
  {\bf nowait}.

\subsection*{Sections}
Another OpenMP parallelism idiom is sections.
  \begin{center}
    {\tt \#pragma omp }{\bf sections} {\it [clause [[,] clause]*]}
  \end{center}

  Allowed Clauses: {\bf private, firstprivate, lastprivate, reduction, nowait}.

Each {\bf sections} directive must contain one or more {\bf section} directive:
  \begin{center}
    {\tt \#pragma omp }{\bf section}
  \end{center}
  
Sections distributed among current team of threads.
They statically limit parallelism to the number of
      sections which are lexically in the code.

\paragraph{Parallel Sections.} Another common idiom.
  \begin{center}
    {\tt \#pragma omp }{\bf parallel sections} {\it [clause [[,] clause]*]}
  \end{center}

  As with parallel for, this is basically shorthand for:
  
  \begin{lstlisting}
#pragma omp parallel
{
    #pragma omp sections
    {

    }
}
  \end{lstlisting}

  Allowed Clauses: everything allowed by {\tt parallel} and {\tt sections}, except
  {\bf nowait}.
\newpage
  \subsection*{Single} When we'd be otherwise running many threads, we can state
that some particular code block should be run by only one method.

  \begin{center}
    {\tt \#pragma omp }{\bf single}
  \end{center}
    Only a single thread executes the region following the directive.
    The thread is not guaranteed to be the master thread.

  Allowed Clauses: {\bf private, firstprivate, copyprivate, nowait}.
 Must not use {\bf copyprivate} with {\bf nowait}

  \subsection*{Barrier}

  \begin{center}
    {\tt \#pragma omp }{\bf barrier}
  \end{center}

     Waits for all the threads in the team to reach the barrier before
      continuing. In other words, this constitutes a synchronization point.
     Loops, sections, and single all have an implicit barrer at the end of their
      region (unless you use {\bf nowait}).
     Note that, although it's always good practice to put statements following a {\tt \#pragma omp} inside a compound block ({\tt \{ \ldots \}}), a barrier inside an {\tt if} statement must be {\tt \{  \}} separated.

     This mechanism is analogous to {\tt pthread\_barrier} in Pthreads.

  \subsection*{Master} Sometimes we do want to guarantee that only the master
thread runs some code.
  \begin{center}
    {\tt \#pragma omp }{\bf master}
  \end{center}

    This is similar to the {\bf single} directive, except that the master thread (and only the master thread) is guaranteed to enter this region.
    No implied barriers. 

Also, no clauses.

  \subsection*{Critical} We turn our attention to synchronization constructs.
First, let's examine critical.
  \begin{center}
    {\tt \#pragma omp }{\bf critical} {\it [(name)]}
  \end{center}

    The enclosed region is guaranteed to only run one thread at a time
      (on a per-name basis).
    This is the same as a block of code in Pthreads surrounded by a {\tt mutex} lock
      and unlock.

\subsection*{Other Directives}
  We'll get into these next lecture.
  \begin{itemize}
    \item {\bf task}
    \item {\bf taskyield}
    \item {\bf taskwait}
    \item {\bf flush}
  \end{itemize}

\subsection*{More Scoping Clauses: {\tt firstprivate}, {\tt lastprivate}, {\tt copyin}, {\tt copyprivate} and {\tt default}}
Besides the {\tt shared}, {\tt private} and {\tt threadprivate}, OpenMP also 
supports {\tt firstprivate} and {\tt lastprivate}, which work like this.

\newpage
Pthreads pseudocode for {\tt firstprivate} clause:
  \begin{lstlisting}
int x;

void* run(void* arg) {
    int thread_x = x;
    // use thread_x
}
  \end{lstlisting}

Pthread pseudocode for the {\tt lastprivate} clause:
  \begin{lstlisting}
int x;

void* run(void* arg) {
    int thread_x;
    // use thread_x
    if (last_iteration) {
        x = thread_x;
    }
}
  \end{lstlisting}
In other words, {\tt lastprivate} makes sure that the variable {\tt x}
has the same value as if the loop executed sequentially.

{\tt copyin} is like firstprivate, but for threadprivate variables.

Pthreads pseudocode for {\tt copyin}:
  \begin{lstlisting}[language=C]
int x;
int x[NUM_THREADS];

void* run(void* arg) {
  x[thread_num] = x;
  // use x[thread_num]
}
  \end{lstlisting}

The {\tt copyprivate} clause is only used with {\tt single}.
It copies the specified private variables from the thread to all other
threads. It cannot be used with {\tt nowait}.

\paragraph{Defaults.} {\bf default(shared)} makes all variables shared; 
{\bf default(none)} prevents sharing by default (creating compiler errors if
you treat a variable as shared.)

\section*{Runtime Library Routines}

To use the runtime library and call OpenMP functions (rather than
using pragmas), you need to {\tt \#include <omp.h>}.

  \begin{itemize}
    \item {\tt int omp\_get\_num\_procs();}: return the number of processors in the system.
    \item {\tt int omp\_get\_thread\_num();}: return the thread number of the currently executing thread (the master thread will return 0).
    \item {\tt int omp\_in\_parallel();}: returns true if currently in a parallel region.
    \item {\tt int omp\_get\_num\_threads();}: return the number of threads in the current team.
  \end{itemize}

\paragraph{Locks in OpenMP.}
OpenMP provides two types of locks:
 
  \begin{itemize}
    \item Simple: cannot be acquired if it is already held by the task trying to acquire it.
    \item Nested: can be acquired multiple times by the same task before being released (like Java).
  \end{itemize}

 Lock usage is similar to Pthreads:

  \begin{center}
    \begin{tabular}{r | r}
      {\tt omp\_init\_lock} & {\tt omp\_init\_nest\_lock}\\
      {\tt omp\_destroy\_lock} & {\tt omp\_destroy\_nest\_lock}\\
      {\tt omp\_set\_lock} & {\tt omp\_set\_nest\_lock}\\
      {\tt omp\_unset\_lock} & {\tt omp\_unset\_nest\_lock}\\
      {\tt omp\_test\_lock} & {\tt omp\_test\_nest\_lock}\\
    \end{tabular}
  \end{center}

\paragraph{Timing.} You can measure how long things take, which is useful for domain-specific
profiling.

  \begin{itemize}
    \item {\tt double omp\_get\_wtime();}: elapsed wall clock time in seconds (since some time in the past).
    \item {\tt double omp\_get\_wtick();}: precision of the timer.
  \end{itemize}

\paragraph{Other Routines.}
Wight see these in later lectures. Included for
  completeness:

  \begin{itemize}
    \item {\tt int omp\_get\_level();}
    \item {\tt int omp\_get\_active\_level();}
    \item {\tt int omp\_get\_ancestor\_thread\_num(int level);}
    \item {\tt int omp\_get\_team\_size(int level);}
    \item {\tt int omp\_in\_final();}
  \end{itemize}

\section*{Internal Control Variables}

OpenMP uses internal variables to control how it handles threads.
These can be set with clauses, runtime routines, environment
      variables, or just from defaults.
Routines will be represented as all-lower-case, environment variables
      as all-upper-case.

\begin{center}
    Clause \textgreater\- Routine \textgreater\- Environment Variable \textgreater\- Default Value
\end{center}

All values (except 1) are implementation defined.

\paragraph{Operation of Parallel Regions.} We can control how parallel regions work
with the following variables.

  {\bf dyn-var}
  \begin{itemize}
    \item is dynamic adjustment of the number of threads enabled?
    \item {\bf Set by:} {\tt OMP\_DYNAMIC omp\_set\_dynamic}
    \item {\bf Get by:} {\tt omp\_get\_dynamic}
  \end{itemize}

  {\bf nest-var}
  \begin{itemize}
    \item is nested parallelism enabled?
    \item {\bf Set by:} {\tt OMP\_NESTED omp\_set\_nested}
    \item {\bf Get by:} {\tt omp\_get\_nested}
    \item Default value: {\tt false}
  \end{itemize}

  {\bf thread-limit-var}
  \begin{itemize}
    \item maximum number of threads in the program
    \item {\bf Set by:} {\tt OMP\_NUM\_THREADS omp\_set\_num\_threads}
    \item {\bf Get by:} {\tt omp\_get\_max\_threads}
  \end{itemize}

  {\bf max-active-levels-var}
  \begin{itemize}
    \item Maximum number of nested active parallel regions
    \item {\bf Set by:} {\tt OMP\_MAX\_ACTIVE\_LEVELS}\\
      \hspace{3.8em}{\tt omp\_set\_max\_active\_levels}
    \item {\bf Get by:} {\tt omp\_get\_max\_active\_levels}
  \end{itemize}

\paragraph{Operation of Parallel Regions/Loops.} These apply to both
parallel regions and loops.

  {\bf nthreads-var}
  \begin{itemize}
    \item number of threads requested for parallel regions.
    \item {\bf Set by:} {\tt OMP\_NUM\_THREADS omp\_set\_num\_threads}
    \item {\bf Get by:} {\tt omp\_get\_max\_threads}
  \end{itemize}

  {\bf run-sched-var}
  \begin{itemize}
    \item {\bf schedule} that the runtime schedule clause uses for loops.
    \item {\bf Set by:} {\tt OMP\_SCHEDULE omp\_set\_schedule}
    \item {\bf Get by:} {\tt omp\_get\_schedule}
  \end{itemize}

\paragraph{Program Execution.} We can also control program execution.

  {\bf bind-var}
  \begin{itemize}
    \item Controls binding of threads to processors.
    \item {\bf Set by:} {\tt OMP\_PROC\_BIND}
  \end{itemize}

  {\bf stacksize-var}
  \begin{itemize}
    \item Controls stack size for threads.
    \item {\bf Set by:} {\tt OMP\_STACK\_SIZE}
  \end{itemize}

  {\bf wait-policy-var}
  \begin{itemize}
    \item Controls desired behaviour of waiting threads.
    \item {\bf Set by:} {\tt OMP\_WAIT\_POLICY}
  \end{itemize}

\section*{Summary}

  \begin{itemize}
    \item Main concepts:
      \begin{itemize}
        \item {\bf parallel}
        \item {\bf for} ({\bf ordered})
        \item {\bf sections}
        \item {\bf single}
        \item {\bf master}
      \end{itemize}
    \item Synchronization:
      \begin{itemize}
        \item {\bf barrier}
        \item {\bf critical}
        \item {\bf atomic}
      \end{itemize}
    \item Data sharing: {\bf private}, {\bf shared}, {\bf threadprivate}
    \item You now should be able to use OpenMP effectively with a reference.
  \end{itemize}

\paragraph{Reference Card.}

  \url{http://openmp.org/mp-documents/OpenMP3.1-CCard.pdf}


\end{document}
