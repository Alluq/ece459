\include{header}

\begin{document}

\lecture{25 --- System-Level Profiling, Profiler Guided Optimization}{\term}{Patrick Lam \& Jeff Zarnett}

      \section*{System-level profiling}
Most profiling tools interrogate the CPU in more detail than {\tt
  gprof} and friends. These tools are typically aware of the whole
system, but may focus on one application, and may have both
per-process and system-wide modes. We'll discuss a couple of these
tools here, highlighting conceptual differences between these
applications.

\paragraph{Solaris Studio Performance Analyzer.} This 
tool\footnote{You can find a high-level description at
  \url{http://www.oracle.com/technetwork/server-storage/solarisstudio/documentation/oss-performance-tools-183986.pdf}}
supports {\tt gprof}-style profiling (``clock-based profiling'') as
well as kernel-level profiling through DTrace (described later). At
process level, it collects more process-level data than {\tt gprof},
including page fault times and wait times. It also can read CPU
performance counters (e.g. the number of executed floating point adds
and multiplies).  As a Sun application, it also works with Java
programs.

Since locks and concurrency are important, modern tools, including the
Studio Performance Analyzer, can track the amount of time spent
waiting for locks, as well as statistics about MPI message
passing. More on lock waits below, when we talk about WAIT.

\paragraph{VTune.} Intel and AMD both provide profiling
tools; Intel's VTune tool costs money, while AMD's CodeAnalyst tool is
free software.

Intel uses the term ``event-based sampling'' to refer to sampling 
which fires after a certain number of CPU events occur, and ``time-based
sampling'' to refer to the {\tt gprof}-style sampling (e.g. every 100ms).
VTune can also correlate the behaviour of the counters with other
system events (like disk workload). Both of these sampling modes
also include the behaviour of the operating system and I/O in their
counts.

VTune also supports an instrumentation-based profiling approach,
which measures time spent in each procedure (same type of
data as {\tt gprof}, but using a different collection scheme).

VTune will also tell you what it thinks the top problems with your
software are. However, if you want to understand what it's saying,
you do actually need to understand the architecture.

\paragraph{CodeAnalyst.} AMD also provides a profiling tool. Unlike Intel's tool, AMD's tool is
free software (the Linux version is released under the GPL), so that,
for instance, Mozilla suggests that people include CodeAnalyst
profiling data when reporting Firefox performance
problems~\footnote{\url{https://developer.mozilla.org/Profiling_with_AMD_CodeAnalyst}}.

CodeAnalyst is a system-wide profiler. It supports drilling down into
particular programs and libraries; the only disadvantage of being
system-wide is that the process you're interested in has to execute
often enough to show up in the profile. It also uses debug symbols to
provide meaningful names; these symbols are potentially supplied over
the Internet.

Like all profilers, it includes a sampling mode, which it calls ``Time-based
profiling'' (TBP). This mode works on all processors. The other modes are 
``Event-based profiling'' (EBP) and ``Instruction-based sampling'' (IBS);
these modes use hardware performance counters.

AMD's CodeAnalyst documentation points out that your sampling interval
needs to be sufficiently high to capture useful data, and that you need to 
take samples for enough time. The default sampling rate is once every
millisecond, and they suggest that programs should run for at least 15 seconds
to get meaningful data.

The EBP mode works like VTune's event-based sampling: after a certain
number of CPU events occur, the profiler records the system state.
That way, it knows where e.g. all the cache misses are occuring.
A caveat, though, is that EBP can't exactly identify the guilty statement,
because of ``skid'': in the presence of out-of-order execution, guilt
gets spread to the adjacent instructions.

To improve the accuracy of the profile information, CodeAnalyst uses
AMD hardware features to watch specific x86 instructions and ``ops'',
their associated backend instructions. This is the IBS mode\footnote{Available
on AMD processors as of the K10 family---typically manufactured in 2007+; see \url{http://developer.amd.com/assets/AMD_IBS_paper_EN.pdf}. Thanks to Jonathan Thomas for pointing this out.} of
CodeAnalyst.  AMD provides an
example\footnote{\url{http://developer.amd.com/cpu/CodeAnalyst/assets/ISPASS2010_IBS_CA_abstract.pdf}}
where IBS tracks down the exact instruction responsible for data
translation lookaside buffer (DTLB) misses, while EBP indicates four
potential guilty instructions.


\paragraph{oprofile.} This free software is a sampling-based tool which uses the Linux Kernel Performance
Events API to access CPU performance counters. It tracks the
currently-running function (or even the line of code) and can, in
system-wide mode, work across processes, recording data for every
active application.

Webpage: \url{http://oprofile.sourceforge.net}.

You can run oprofile either in system-wide mode (as root) or per-process. To run it in system-wide mode:
  \begin{lstlisting}
% sudo opcontrol --vmlinux=/usr/src/linux-3.2.7-1-ARCH/vmlinux
% echo 0 | sudo tee /proc/sys/kernel/nmi_watchdog
% sudo opcontrol --start
Using default event: CPU_CLK_UNHALTED:100000:0:1:1
Using 2.6+ OProfile kernel interface.
Reading module info.
Using log file /var/lib/oprofile/samples/oprofiled.log
Daemon started.
Profiler running.
  \end{lstlisting}

Or, per-process:
\begin{lstlisting}
[plam@lynch nm-morph]$ operf ./test_harness
operf: Profiler started

Profiling done.
\end{lstlisting}%$

Both of these invocations produce profiling output. You can read the profiling output
by running {\tt opreport} and giving it your executable.

  \begin{lstlisting}
% sudo opreport -l ./test    
CPU: Intel Core/i7, speed 1595.78 MHz (estimated)
Counted CPU_CLK_UNHALTED events (Clock cycles when not
halted) with a unit mask of 0x00 (No unit mask) count 100000
samples  %        symbol name
7550     26.0749  int_math_helper
5982     20.6596  int_power
5859     20.2348  float_power
3605     12.4504  float_math
3198     11.0447  int_math
2601      8.9829  float_math_helper
160       0.5526  main
  \end{lstlisting}
  If you have debug symbols ({\tt -g}) you can also get better data:
  {\small
  \begin{lstlisting}
% sudo opannotate --source --output-dir=/path/to/annotated-source /path/to/mybinary
  \end{lstlisting}
  }

Use {\tt opreport} by itself for a whole-system view.
    You can also reset and stop the profiling.
  \begin{lstlisting}
% sudo opcontrol --reset 
Signalling daemon... done
% sudo opcontrol --stop
Stopping profiling.
  \end{lstlisting}

\paragraph{perf.} This uses the same base data as oprofile, but provides a better
(git-like) interface. Once again, it is an interface to the Linux kernel's built-in
sample-based profiling using CPU counters. It works per-process, per-CPU, or system-wide.
It can report the cost of each line of code.

Webpage: \url{https://perf.wiki.kernel.org/index.php/Tutorial}

\noindent
Here's a usage example on some old assignment code from last year:
\begin{lstlisting}[basicstyle=\scriptsize]
[plam@lynch nm-morph]$ perf stat ./test_harness

 Performance counter stats for './test_harness':

       6562.501429 task-clock                #    0.997 CPUs utilized          
               666 context-switches          #    0.101 K/sec                  
                 0 cpu-migrations            #    0.000 K/sec                  
             3,791 page-faults               #    0.578 K/sec                  
    24,874,267,078 cycles                    #    3.790 GHz                     [83.32%]
    12,565,457,337 stalled-cycles-frontend   #   50.52% frontend cycles idle    [83.31%]
     5,874,853,028 stalled-cycles-backend    #   23.62% backend  cycles idle    [66.63%]
    33,787,408,650 instructions              #    1.36  insns per cycle        
                                             #    0.37  stalled cycles per insn [83.32%]
     5,271,501,213 branches                  #  803.276 M/sec                   [83.38%]
       155,568,356 branch-misses             #    2.95% of all branches         [83.36%]

       6.580225847 seconds time elapsed
\end{lstlisting} %$

\noindent
perf can tell you which instructions are taking time, or which lines
of code; compile with {\tt -ggdb} to enable source code viewing.

\begin{lstlisting}
% perf record ./test_harness
% perf annotate
\end{lstlisting}

\noindent
{\tt perf annotate} is interactive. Play around with it.

\paragraph{DTrace.} 
DTrace~\cite{Cantrill:2004:DIP:1247415.1247417} is an instrumentation-based
system-wide profiling tool designed to be used on production
systems. It supports custom queries about system behaviour: when you
are debugging system performance, you can collect all sorts of data
about what the system is doing. The two primary design goals were in
support of use in production: 1) avoid overhead when not tracing and
2) guarantee safety (i.e. DTrace can never cause crashes).

DTrace runs on Solaris and some BSDs. There is a Linux port, which may be usable.
I'll try to install it on {\tt ece459-1}.

\paragraph{Probe effect.} ``Wait! Don't `instrumentation-based' 
and `production systems' not go together?'' For instance, Valgrind incurs a 100$\times$
slowdown. Ouch.

Nope! DTrace was designed to have zero overhead when inactive. It does
this by dynamically rewriting the code to insert instrumentation when
requested. So, if you want to instrument all calls to the {\tt open}
system call, then DTrace is going to replace the instruction at the
beginning of {\tt open} with an unconditional branch to the
instrumentation code, execute the profiling code, then return to your
code. Otherwise, the code runs exactly as if you weren't looking.

\paragraph{Safety.} As I've mentioned before, crashing a production
system is a big no-no. DTrace is therefore designed to never cause a
system crash. How? The instrumentation you write for DTrace 
must conform to fairly strict constraints.

\paragraph{DTrace system design.} The DTrace framework supports instrumentation
\emph{providers}, which make \emph{probes} (i.e.  instrumentation
points) available; and \emph{consumers}, which enable probes as
appropriate. Examples of probes include system calls, arbitrary kernel
functions, and locking actions. Typically, probes apply at function entry or exit points.
DTrace also supports typical
sampling-based profiling in the form of timer-based probes; that is,
it executes instrumentation every 100ms. This is tantamount to
sampling.

You can specify a DTrace clause using probes, predicates, and a set of
action statements. The action statements execute when the condition
specified by the probe holds and the predicate evaluates to true. D
programs consist of a sequence of clauses.

\paragraph{Example.} Here's an example of a DTrace query from~\cite{Cantrill:2004:DIP:1247415.1247417}.

{ \begin{verbatim}
    syscall::read:entry {
            self->t = timestamp;
    }

    syscall::read:return
    /self->t/ {
            printf("%d/%d spent %d nsecs in read\n"
                pid, tid, timestamp - self->t);
    }
    \end{verbatim}
}

The first clause instruments all entries to the system call {\tt read}
and sets a thread-local variable {\tt t} to the current time. The
second clause instruments returns from {\tt read} where the
thread-local variable {\tt t} is non-zero, calling {\tt printf} to print out
the relevant data.

The D (DTrace clause language) design ensures that clauses cannot loop indefinitely
(since they can't loop at all), nor can they execute unsafe code;
providers are responsible for providing safety guarantees. Probes
might be unsafe because they might interrupt the system at a critical
time. Or, action statements could perform illegal writes. DTrace won't 
execute unsafe code.

\paragraph{Workflow.} Both the USENIX article~\cite{Cantrill:2004:DIP:1247415.1247417} and the
ACM Queue article~\cite{300ps} referenced above contain example usages of DTrace.
In high-level terms: first identify a problem; then, use
standard system monitoring tools, plus custom DTrace queries,
to collect data about the problem (and resolve it).

\section*{WAIT}
Another approach which recently appeared in the research literature is
the WAIT tool out of IBM. Unfortunately, this tool is not free and not
generally available. Let's talk about it anyways.

Like DTrace, WAIT is suitable for use in production environments. It
uses hooks built into modern Java Virtual Machines (JVMs) to analyze
their idle time. It performs a sampling-based analysis of the
behaviour of the Java VM. Note that its samples are quite infrequent;
they suggest that taking samples once or twice a minute is enough.  At
each sample, WAIT records the state of each of the threads, which
includes its call stack and participation in system locks. This data
enables WAIT to compute (using expert rules) an abstract ``wait
state''.  The wait state indicates what the process is currently doing
or waiting on, e.g. ``disk'', ``GC'', ``network'', or ``blocked''.

\paragraph{Workflow.} You run your application, collect data (using a
script or manually), and upload the data to the server. The server provides
a report which you use to fix the performance problems.
The report indicates processor utilization (idle, your application, GC, 
etc); runnable threads; waiting threads (and why they are waiting); 
thread states; and a stack viewer.

The paper presents six case studies where WAIT helped solve performance
problems, including deadlocks, server underloads, memory leaks, database
bottlenecks, and excess filesystem activity.

\subsection*{Other Applications of Profiling.} 
Profiling applies to languages beyond C/C++ and Java, of course. If you are
profiling an interpreted language, you'll need a specific tool to get useful
results. For Python, you can use {\tt cProfile}; it is a standard implementation
of profiling, from what I can see.


Here's a short tangent. Many of the concepts that we've seen for code
also apply to web pages. Google's Page Speed
tool\footnote{\url{http://code.google.com/speed/page-speed/}}, in
conjunction with Firebug, helps profile web pages, and provides
suggestions on how to make your web pages faster. Note that Page Speed
includes improvements for the web page's design, e.g. not requiring
multiple DNS lookups; leveraging browser caching; or combining images;
as well as traditional profiling for the JavaScript on your pages.


\section*{Profiler Guided Optimization (POGO)}

In 2015 we were fortunate enough to have a guest lecture from someone at Microsoft actually in the room to give the guest lecture on the subject of Profile Guided Optimization (or POGO). In subsequent years, I was not able to convince him to fly in just for the lecture. Now there's a pandemic, so that's a big nope. Anyway, let's talk about the subject, which is by no means restricted to Rust.

The compiler does static analysis of the code you've written and makes its best guesses about what is likely to happen. The canonical example for this is branch prediction: there is an if-else block and the compiler will then guess about which is more likely and optimize for that version. Consider three examples, originally from~\cite{pogo} but replaced with some Rust equivalents:

\begin{lstlisting}[language=Rust]
fn which_branch(a: i32, b: i32) {
    if a < b {
        println!("Case one.");
    } else {
        println!("Case two.");
    }
}
\end{lstlisting}

Just looking at this, which is more likely, \texttt{a < b} or \texttt{a >= b}? Assuming there's no other information in the system the compiler can believe that one is more likely than the other, or having no real information, use a fallback rule. This works, but what if we are wrong? Suppose the compiler decides it is likely that \texttt{a} is the larger value and it optimizes for that version. However, it is only the case 5\% of the time, so most of the time the prediction is wrong. That's unpleasant. But the only way to know is to actually run the program.

\begin{multicols}{2}
\begin{lstlisting}[language=Rust]
trait Polite {
    fn greet(&self) -> String;
}

struct Kenobi {
    /* Stuff */
}
impl Polite for Kenobi {
    fn greet(&self) -> String {
        return String::from("Hello there!");
    }
}
\end{lstlisting}
\columnbreak
\begin{lstlisting}[language=Rust]
struct Grievous {
    /* Things */
}
impl Polite for Grievous {
    fn greet(&self) -> String {
        return String::from("General Kenobi.");
    }
}

fn devirtualization(thing: &Polite) {
    println!("{}", thing.greet());
}

\end{lstlisting}
\end{multicols}



There are similar questions raised for the other two examples. What is the ``normal'' type for some reference \texttt{thing}? It could be of either type \texttt{Kenobi} or \texttt{Grievous}. If we do not know, the compiler cannot do devirtualization (replace this virtual call with a real one). If there was exactly one type that implements the \texttt{Polite} trait we wouldn't have to guess. But are we much more likely to see \texttt{Kenobi} than \texttt{Grievous}?

\begin{lstlisting}[language=Rust]
fn match_thing(x: i32) -> i32 {
    match x {
        0..10 => 1,
        11..100 => 2,
        _ => 0
    }
}
\end{lstlisting}

 Same thing with \texttt{x}: what is its typical value? If we know that, it is our prediction. Actually, in a match block with many options, could we rank them in descending order of likelihood?

There exists a solution to this, and it is that we can give hints to the compiler, but that's a manual process. Automation is a good thing and this lecture is about that. These sorts of things already exist for Java! The Java HotSpot virtual machine will update its predictions on the fly. There are some initial predictions and if they turn out to be wrong, the Just In Time compiler will replace it with the other version. That's neat! I don't know for certain but I suspect the .NET runtime will do the same for something like C\#. But this is Rust and we don't have the runtime to reduce the overhead: the compiler runs and it does its job and that's it; the program is never updated with newer predictions if more data becomes known.

Solving this problem is the goal of POGO. It is taking the data from some actual runs of the program and using that to inform the predictions. This necessitates a multi-step compile: first compile the code, run it to collect data, then recompile the code using the data we collected. Let's expand on all three steps.

Step one is to generate an executable with instrumentation. Ask to compile with instrumentation enabled, which also says what directory to put it in: \texttt{-Cprofile-generate=/tmp/pgo-data}. The compiler inserts a bunch of probes into the generated code that are used to record data. Three types of probe are inserted: function entry probes, edge probes, and value probes.  A function entry probe, obviously, counts how many times a particular function is called. An edge probe is used to count the transitions (which tells us whether an if branch is taken or the else condition). Value probes are interesting; they are used to collect a histogram of values. Thus, we can have a small table that tells us the frequency of what is given in to a \texttt{match} statement. When this phase is complete, there is an instrumented executable and an empty database file where the training data goes~\cite{pogo}. 

Step two is training day: run the instrumented executable through real-world scenarios. Ideally you will spend the training time on the performance-critical sections. It does not have to be a single training run, of course, data can be collected from as many runs as desired. Keep in mind that the program will run a lot slower when there's the instrumentation present. 

Still, it is important to note that you are not trying to exercise every part of the program (this is not unit testing); instead it should be as close to real-world-usage as can be accomplished. In fact, trying to use every bell and whistle of the program is counterproductive; if the usage data does not match real world scenarios then  the compiler has been given the wrong information about what is important. Or you might end up teaching it that almost nothing is important... 

According to the docs about it\footnote{\url{https://doc.rust-lang.org/rustc/profile-guided-optimization.html}}, the output \texttt{.profraw} files require a little bit of processing before they're ready to go. When the program is running, the recording of data is done as quickly as possible with little regard for making it neat. Think of it like taking notes furiously during a lecture and then later revisiting them to organize them a bit. The tool for doing this is \texttt{llvm-profdata} and it will organize the data into a \texttt{.profdata} file. We can merge multiple runs as needed into a single file that will be used for input. 

Step three is a recompile. This time, in addition to the source files, the (merged) training data is fed into the compiler for a second compile, and this data is applied to (hypothetically) produce a better output executable than could be achieved by static analysis alone.


It is not necessary to do all three steps for every build. Old training data can be re-used until the code base has diverged significantly enough from the instrumented version. According to~\cite{pogo}, the recommended workflow is for one developer to perform these steps and check the training data into source control so that other developers can make use of it in their builds.

The Intel Developer Zone explains the process in a handy infographic\footnote{Source: \url{https://software.intel.com/en-us/node/522721}} :

\begin{center}
	\includegraphics[width=0.5\textwidth]{images/pogo-workflow.jpg}
\end{center}



Or, here, a complete set of steps for actually running it if our program is all in \texttt{main.rs}, from the docs:

{\scriptsize
\begin{verbatim}
# STEP 1: Compile the binary with instrumentation
rustc -Cprofile-generate=/tmp/pgo-data -O ./main.rs

# STEP 2: Run the binary a few times, maybe with common sets of args.
#         Each run will create or update `.profraw` files in /tmp/pgo-data
./main mydata1.csv
./main mydata2.csv
./main mydata3.csv

# STEP 3: Merge and post-process all the `.profraw` files in /tmp/pgo-data
llvm-profdata merge -o ./merged.profdata /tmp/pgo-data

# STEP 4: Use the merged `.profdata` file during optimization. All `rustc`
#         flags have to be the same.
rustc -Cprofile-use=./merged.profdata -O ./main.rs
\end{verbatim}
}

What does it mean for the executable to be better? We have already looked at an example about how to predict branches. Predicting it correctly will be faster than predicting it incorrectly, but this is not the only thing.  The algorithms will aim for speed in the areas that are ``hot'' (performance critical and/or common scenarios). The algorithms will alternatively aim to minimize the size of code of areas that are ``cold'' (not heavily used). It is recommended in~\cite{pogo} that less than 5\% of methods should be compiled for speed.

It is possible that we can combine multiple training runs and we can manually give some suggestions of what scenarios are important. Obviously the more a scenario runs in the training data, the more important it will be, as far as the POGO optimization routine is concerned, but multiple runs can be merged with user assigned weightings.

\subsection*{Behind the Scenes}

In the optimize phase, the training data is used to do the following optimizations( which I will point out are based on C and \CPP programs and not necessarily Rust, but the principles should work because the Rust compiler's approach to this is based on that of LLVM/Clang)~\cite{pogo2}:

\begin{multicols}{2}
\begin{enumerate}
\item Full and partial inlining
\item Function layout
\item Speed and size decision
\item Basic block layout 
\item Code separation
\item Virtual call speculation
\item Switch expansion
\item Data separation
\item Loop unrolling
\end{enumerate}
\end{multicols}

For the most part we should be familiar with the techniques that are listed as being other compiler optimizations we have previously discussed. The new ones are (3) speed and size decision, which we have just covered; and items (4) and (5) which relate to how to pack the generated code in the binary.

According to~\cite{pogo2} the majority of the performance gains relate to the inlining decisions. These decisions are based on the call graph path profiling: the behaviour of function \texttt{foo} may be very different when calling it from \texttt{bar} than it is when calling it from function \texttt{baz}. Let's look at this call graph from~\cite{pogo2}:

\begin{center}
	\includegraphics[width=0.25\textwidth]{images/callpaths.png}
\end{center}

Quick analysis of this code would have us find all the ways in which the functions might call each other. In total, there are 14 paths in this code, seven of which get us to function \texttt{Foo}. Consider another diagram showing the relationships between functions, in which the numbers on the edges represent the number of invocations~\cite{pogo2}:

\begin{center}
	\includegraphics[width=0.4\textwidth]{images/callpaths2.png}
\end{center}

When considering what to do here, POGO takes the view like this~\cite{pogo2}:

\begin{center}
	\includegraphics[width=0.4\textwidth]{images/callpaths3.png}
\end{center}

Each part of the call path is considered separately, remembering that we want to inline where it makes sense for speed, but otherwise leave it alone because of code size increases. Inlining \texttt{bar} into \texttt{bat} makes sense, but not inlining \texttt{bar} into \texttt{goo} (because that increases the code size without significant performance benefits). It also makes sense for \texttt{baz} to get inlined into \texttt{bar}. This is illustrated below~\cite{pogo2}:

\begin{center}
	\includegraphics[width=0.4\textwidth]{images/callpaths4.png}
\end{center}

Packing the blocks is also done based on this call graph profiling. The most common cases will be put next to each other, and, where possible, subsequent steps are put next to each other. The more we can pack related code together, the fewer page faults we get by jumping to some other section, causing a cache miss... If the function being called is in the same page as the call, it has achieved ``page locality'' (and that is the goal!). This is represented visually~\cite{pogo2}:

\begin{center}
	\includegraphics[width=0.6\textwidth]{images/blocklayout.png}
\end{center}

According to the author, the ``dead'' code goes in its own special block. I don't think they actually mean truly dead code, the kind that is compile-time determined to be unreachable, but instead they mean code that never gets invoked in any of the training runs.

So, to sum up, the training data is used to identify what branches are likely to be taken, inlines code where that is a performance increase, and tries to pack the binary code in such a way as to reduce cache misses/page faults. How well does it work?

\subsection*{Benchmark Results}

This table, condensed from~\cite{pogo2} summarizes the gains to be made. The application under test is a standard benchmark suite (Spec2K) (admittedly, C rather than Rust, but the goal is to see if the principle of POGO works and not just a specific implementation):

\begin{center}
\begin{tabular}{l|l|l|l|l|l}
	\textbf{Spec2k:} & \textbf{sjeng} & \textbf{gobmk} & \textbf{perl} & \textbf{povray} & \textbf{gcc}\\ \hline
	\textbf{App Size:} &  {Small} & {Medium} & {Medium} & {Medium} & {Large} \\ \hline
	\textbf{Inlined Edge Count} & 50\% & 53\% & 25\% & 79\% & 65\% \\ \hline
	\textbf{Page Locality} & 97\% & 75\% & 85\% & 98\% & 80\% \\ \hline
	\textbf{Speed Gain} & 8.5\% & 6.6\% & 14.9\% & 36.9\% & 7.9\% \\ 
\end{tabular}
\end{center}

There are more details in the source as to how many functions are used in a typical run and how many things were inlined and so on. But we get enough of an idea from the last row of how much we are speeding up the program, plus some information about why. We can speculate about how well the results in a synthetic benchmark translate to real-world application performance, but at least from this view it does seem to be a net gain.


\input{bibliography.tex}

\end{document}
