\include{header}

\begin{document}

\lecture{18 --- Cache Coherency}{\term}{Patrick Lam}

\section*{Cache Coherency}

\begin{center}
    \includegraphics[scale=0.7]{images/coherency}

\hfill ---Wikipedia
\end{center}

We talked about memory ordering and fences last time.  Today we'll
look at what support the architecture provides for memory ordering, in
particular in the form of cache coherence. Since this isn't an
architecture course, we'll look at this material more from the point
of view of a user, not an implementer.

Cache coherency means that:
  \begin{itemize}
    \item the values in all caches are consistent; and
    \item to some extent, the system behaves as if all CPUs are using shared memory.
  \end{itemize}

\paragraph{Cache Coherence Example.} We will use this example to illustrate
different cache coherence algorithms and how they handle the same situation.

Initially in main memory: {\tt x = 7}.

  \begin{enumerate}
    \item {\tt CPU1} reads x, puts the value in its cache.
    \item {\tt CPU3} reads x, puts the value in its cache.
    \item {\tt CPU3} modifies {\tt x := 42}
    \item {\tt CPU1} reads x \ldots ~from its cache?
    \item {\tt CPU2} reads x. Which value does it get?
  \end{enumerate}

Unless we do something, {\tt CPU1} is going to read invalid data.

\paragraph{High-Level Explanation of Snoopy Caches.} The simplest way to ``do something''
is to use snoopy caches. The setup is as follows:

  \begin{itemize}
    \item Each CPU is connected to a simple bus.
    \item Each CPU ``snoops'' to observe if a memory location is read or written
      by another CPU.
    \item We need a cache controller for every CPU.
  \end{itemize}

Then:
  \begin{itemize}
    \item Each CPU reads the bus to see if any memory operation is relevant. If
      it is, the controller takes appropriate action.
  \end{itemize}

\subsection*{Write-Through Caches}
Let's put that into practice using write-through caches, the simplest type of
cache coherence.
  \begin{itemize}
    \item All cache writes are done to main memory.
    \item All cache writes also appear on the bus.
    \item If another CPU snoops and sees it has the same location in
      its cache, it will either {\it invalidate} or {\it update} the
      data.\\ \qquad (We'll be looking at invalidating.)
  \end{itemize}
Normally, when you write to an invalidated location, you bypass the
cache and go directly to memory (aka {\bf write no-allocate}).

\paragraph{Write-Through Protocol.}
The protocol for implementing such caches looks like this.
There are two possible states, {\bf valid} and {\bf invalid}, for each cached memory location.
Events are either from a processor ({\bf Pr}) or the {\bf Bus}.
We then implement the following state machine.

  \begin{center}
    \begin{tabular}{llll}
      {\bf State} & {\bf Observed} & {\bf Generated} & {\bf Next State}\\
      Valid   & PrRd  &       & Valid\\
      Valid   & PrWr  & BusWr & Valid\\
      Valid   & BusWr &       & Invalid\\
      Invalid & PrWr  & BusWr & Invalid\\
      Invalid & PrRd  & BusRd & Valid\\
    \end{tabular}
  \end{center}

\paragraph{Example.} For simplicity (this isn't an architecture course), assume all cache
      reads/writes are atomic. Using the same example as before:

Initially in main memory: {\tt x = 7}.

  \begin{enumerate}
    \item {\tt CPU1} reads x, puts the value in its cache. {(valid)}
    \item {\tt CPU3} reads x, puts the value in its cache. {(valid)}
    \item {\tt CPU3} modifies {\tt x := 42}. {(write to memory)}
      \begin{itemize}
        \item {{\tt CPU1} snoops and marks data as invalid.}
      \end{itemize}
    \item {\tt CPU1} reads x, {from main memory. (valid)}
    \item {\tt CPU2} reads x, {from main memory. (valid)}
  \end{enumerate}

\subsection*{Write-Back Caches}
Let's try to improve performance. What if, in our example, {\tt CPU3} writes to {\tt x} 3 times?

\paragraph{Main goal.} Delay the write to memory as long as possible. 
At minimum, we have to add a ``dirty'' bit, which indicates the our data has not yet been written to memory.
Let's do that.

\paragraph{Write-Back Implementation.}
     The simplest type of write-back protocol (MSI) uses 3 states instead of 2:
      \begin{itemize}
        \item {\bf Modified}---only this cache has a valid copy; 
          main memory is {\bf out-of-date}.
        \item {\bf Shared}---location is unmodified, 
           up-to-date with main
          memory; 
          \quad may be present in other caches (also up-to-date).
        \item {\bf Invalid}---same as before.
      \end{itemize}
      
The initial state for a memory location, upon its first read, is ``shared''.
The implementation will only write the data to memory if another
processor requests it.
During write-back, a processor may read the data from the bus.

\paragraph{MSI Protocol.} Here, bus write-back (or flush) is {\bf BusWB}.
Exclusive read on the bus is {\bf BusRdX}.

\begin{center}
    \begin{tabular}{llll}
      {\bf State} & {\bf Observed} & {\bf Generated} & {\bf Next State}\\
      Modified   & PrRd   &        & Modified\\
      Modified   & PrWr   &        & Modified\\
      Modified   & BusRd  & BusWB  & Shared\\
      Modified   & BusRdX & BusWB  & Invalid\\
      Shared     & PrRd   &        & Shared\\
      Shared     & BusRd  &        & Shared\\
      Shared     & BusRdX &        & Invalid\\
      Shared     & PrWr   & BusRdX & Modified\\
      Invalid    & PrRd   & BusRd  & Shared\\
      Invalid    & PrWr   & BusRdX & Modified\\
    \end{tabular}
\end{center}

\paragraph{MSI Example.} Using the same example as before:

Initially in main memory: {\tt x = 7}.

  \begin{enumerate}
    \item {\tt CPU1} reads x from memory. {(BusRd, shared)}
    \item {\tt CPU3} reads x from memory. {(BusRd, shared)}
    \item {\tt CPU3} modifies {\tt x = 42}:
      \begin{itemize}
        \item {Generates a BusRdX.}
        \item {{\tt CPU1} snoops and invalidates x.}
      \end{itemize}
    \item {\tt CPU1} reads x:
      \begin{itemize}
        \item {Generates a BusRd.}
        \item {{\tt CPU3} writes back the data and sets x to shared.}
        \item {{\tt CPU1} reads the new value from the bus as shared.}
      \end{itemize}
    \item {\tt CPU2} reads x from memory. {(BusRd, shared)}
  \end{enumerate}

\subsection*{An Extension to MSI: MESI}
    The most common protocol for cache coherence is MESI.
    This protocol adds yet another state:
      \begin{itemize}
        \item {\bf Modified}---only this cache has a valid copy; 
 main memory is {\bf out-of-date}.
        \item {\bf Exclusive}---only this cache has a valid copy; 
 main memory is {\bf up-to-date}.
        \item {\bf Shared}---same as before.
        \item {\bf Invalid}---same as before.
      \end{itemize}

    MESI allows a processor to modify data exclusive to it, without
    having to communicate with the bus.  MESI is safe. The key is that
    if memory is in the E state, no other processor has the data.

\subsection*{MSEIF: Even More States!}

    MESIF (used in latest i7 processors):
      \begin{itemize}
        \item {\bf Forward}---basically a shared state; but, current
          cache is the only one that will respond to a request to
          transfer the data.
      \end{itemize}

    Hence: a processor requesting data that is already shared or exclusive will
      only get one response transferring the data. This permits more efficient usage of the bus.

\subsection*{Cache coherence vs flush}

\begin{quote}
Cache coherency seems to make sure my data is consistent. Why do I have
to have something like flush or fence?
\end{quote}

Sadly, no. Cache coherence isn't enough. Writes may be to registers rather than
memory, and those won't be coherent. Use fences or flushes.

\begin{quote}
Well, I read that {\tt volatile} variables aren't stored in registers,
    so then am I okay?
\end{quote}

Again, sadly, no. Recall that {\tt volatile} in C was only designed to:
      \begin{itemize}
        \item Allow access to memory mapped devices.
        \item Allow uses of variables between {\tt setjmp} and {\tt longjmp}.
        \item Allow uses of {\tt sig\_atomic\_t} variables in signal handlers.
      \end{itemize}

Remember, things can also be reordered by the compiler, and {\tt
  volatile} doesn't prevent reordering. Also, it's likely your
variables could be in registers the majority of the time, except in
critical areas.

\paragraph{Coherence summary.}
We saw four cache coherence protocols, from MSI through MESIF. There
are many other protocols for cache coherence, each with their own
trade-offs.

Recall: OpenMP flush acts as a {\bf memory barrier/fence} so the
      compiler and hardware don't reorder reads and writes.
    \begin{itemize}
     \item Neither cache coherence nor {\tt volatile} will save you.
    \end{itemize}

\input{bibliography.tex}

\end{document}
