\input{configuration}

\title{Lecture 17 --- Critical Paths, Data \& Task Parallelism }

\author{Patrick Lam \\ \small \texttt{patrick.lam@uwaterloo.ca}}
\institute{Department of Electrical and Computer Engineering \\
  University of Waterloo}
\date{\today}


\begin{document}

\begin{frame}
  \titlepage

 \end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Critical Paths}

  

  Should be familiar with critcal paths from other courses (Gantt charts).\\[1em]

  Consider the following diagram (edges are tasks):

\begin{center}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,column sep=2.2cm,
                    semithick,initial text=]
  \node[initial,block] (s0) {Start};
  \node[block] (s1) [right=of s0] {S1};
  \node[block] (s2) [right=of s1] {S2};
  \node[bw] (s3) [right=of s2] {Finish};

  \path (s0) edge node {A} (s1)
        (s1) edge node {B} (s2)
        (s2) edge node {D} (s3)
        (s0) edge[bend left=30] node {C} (s2);
\end{tikzpicture}
\end{center}

  \begin{itemize}
    \item B depends on A, C has no dependencies, and D depends on B and C.
    \item Can execute A-then-B in parallel with C.
    \item Keep dependencies in mind when calculating speedups for more
      complex programs.
  \end{itemize}
  
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Data and Task Parallelism}

  
     \structure{Data parallelism} is performing \emph{the same} operations on
      different input.\\

     {\bf Example:} doubling all elements of an array.\\[1em]

     \structure{Task parallelism} is performing \emph{different} operations
      on different input.

    {\bf Example:} playing a video file: one thread decompresses
      frames, another renders.

  
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Data Parallelism: Single Instruction, Multiple Data}

  
    We'll discuss SIMD in more detail later. An overview:
    \begin{itemize}
    \item You can load a bunch of data and perform 
      arithmetic.
    \item Intructions process multiple data items simultaneously.
      (Exact number is hardware-dependent).
    \end{itemize}
    For x86-class CPUs, MMX and SSE extensions provide SIMD instructions.
  
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{SIMD Example}

  
  Consider the following code:

  \begin{lstlisting}
void vadd(double * restrict a, double * restrict b, 
          int count) {
  for (int i = 0; i < count; i++)
    a[i] += b[i];
}    
  \end{lstlisting}

    In this scenario, we have a regular operation over block data.\\[1em]

    We could use threads, but we'll use SIMD.

  
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{SIMD Example---Assembly without SIMD}

  
If we compile this without SIMD instructions on a 32-bit x86, (flags {\tt -m32 -march=i386 -S}) we might get this:

  \begin{lstlisting}
loop:
  fldl  (%edx)
  faddl (%ecx)
  fstpl (%edx)
  addl  8, %edx
  addl  8, %ecx
  addl  1, %esi
  cmp   %eax, %esi
  jle   loop
  \end{lstlisting}

   Just loads, adds, writes and increments.
  
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{SIMD Example---Assembly with SIMD}

  
 Instead, compiling to SIMD instructions\\ ({\tt -m32 -mfpmath=sse -march=prescott}) gives:

  \begin{lstlisting}
loop:
  movupd (%edx),%xmm0
  movupd (%ecx),%xmm1
  addpd  %xmm1,%xmm0
  movpd  %xmm0,(%edx)
  addl   16,%edx
  addl   16,%ecx
  addl   2,%esi
  cmp    %eax,%esi
  jle    loop
  \end{lstlisting}

  \begin{itemize}
    \item Now processing two elements at a time on the same core.
    \item Also, no need for stack-based x87 code.
  \end{itemize}
  
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{SIMD Overview}

  
  \begin{itemize}
    \item Operations \emph{packed}: operate on multiple data elements 
      at~the~same~time.
    \vfill
    \item On modern 64-bit CPUs, SSE has 16 128-bit registers.
    \vfill
    \item Very good if your data can be \emph{vectorized} and performs math.
    \vfill
    \item Usual application: image/video processing.
    \vfill
    \item We'll see more SIMD as we get into GPU programming: GPUs excel
       at these types of applications.
  \end{itemize}
  

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Task-Based Patterns: Overview}

\Large
  
  \begin{itemize}
    \item We'll now look at thread and process-based~parallelization.
    \item Although threads and processes differ, we don't care for now.
  \end{itemize}
  

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Pattern 1: Multiple Independent Tasks}

  
    Only useful to maximize system utilization.
  \begin{itemize}
    \item Run multiple tasks on the same system \\ ~~~(e.g. database and web server).\\[1em]
     If one is memory-bound and the other is I/O-bound, for example, you'll
      get maximum utilization out of your resources.\\[1em]
     {\bf Example:} cloud computing, each task is independent and can
      tasks can spread themselves over different nodes.\\[1em]
    \item Performance can increase linearly with the number of threads.
  \end{itemize}
  
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Pattern 2: Multiple Loosely-Coupled Tasks}

  
    Tasks aren't quite independent, so there needs to be some inter-task
      communication (but not much).
  \begin{itemize}

    \item Communication might be from the tasks to a controller or status
      monitor.
  \end{itemize}

    Refactoring an application can help with latency. \\
For instance: split off
      the CPU-intensive computations into a separate thread---your application may
      respond more quickly.\\[1em]
     {\bf Example:} A program (1) receives/forwards packets and (2) logs them. You
      can split these two tasks into two threads, so you can still
      receive/forward while waiting for disk. This will increase the
      \structure{throughput} of the system.
  
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Pattern 3: Multiple Copies of the Same Task}

  
    Variant of multiple independent tasks: run multiple copies of the same task (probably on different data).

  \begin{itemize}
    \item No communcation between different copies.
  \end{itemize}

  Again, performance should increase linearly with number of tasks.\\[1em]

  {\bf Example:} In a rendering application, each thread can be
      responsible for a frame (gain \structure{throughput}; same
      \structure{latency}).

  
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Pattern 4: Single Task, Multiple Threads}

  
  Classic vision of ``parallelization''.\\[1em]

  {\bf Example:} Distribute array processing over multiple 
      threads---each thread computes results for a subset of the array.

  \begin{itemize}
    \item Can decrease \structure{latency} (and increase
      \structure{throughput}), as we saw with \structure{Amdahl's Law}.
    \item Communication can be a problem, if the data is not nicely partitioned.
    \item Most common implementation is just creating threads and joining them,
      combining all results at the join.
  \end{itemize}
  
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Pattern 5: Pipeline of Tasks}

  
    Seen briefly in computer architecture.

    \begin{itemize}
    \item Use multiple stages; each thread handles a stage.\\[1em]
     {\bf Example:} a program that handles network packets: (1) accepts
      packets, (2) processes them, and (3) re-transmits them. Could set up the threads such that each packet goes through the threads.
    \vfill
    \item Improves \structure{throughput}; may increase \structure{latency} as
      there's communication between threads.
    \vfill
    \item In the best case, you'll have a linear speedup.\\[1em]

     Rare, since the runtime of the stages will vary, and the slow
      one will be the bottleneck (but you could have 2 instances of the
      slow stage).
  \end{itemize}
  
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Pattern 6: Client-Server}

  
    To execute a large computation, the server supplies work to many
      clients---as many as request it.\\[1em]

    Client computes results and returns them to the server.\\[1em]
   {\bf Examples:} botnets, {\tt SETI@Home}, GUI application (backend
      acts as the server).\\[1em]
   Server can arbitrate access to shared resources (such as network
      access) by storing the requests and sending them out.\\[1em]

   \begin{itemize}
    \item Parallelism is somewhere between single task, multiple threads and
      multiple loosely-coupled tasks  
  \end{itemize}
  
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Pattern 7: Producer-Consumer}

  
    Variant on the pipeline and client-server models.\\
    Producer generates work, and consumer performs work.\\[1em]
    {\bf Example:} producer which generates rendered frames;
      consumer which orders these frames and writes them to disk.\\[1em]
    Any number of producers and consumers.\\[1em]

    \begin{itemize}
    \item This approach can improve \structure{throughput} and also reduces
      design complexity
    \end{itemize}
  
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Combining Strategies}

  
    Most problems don't fit into one category, so it's often best to combine
      strategies.\\[1em]

    For instance, you might often start with a pipeline, and then use
      multiple threads in a particular pipeline stage to handle one piece of
      data.\\[1em]
    Tip: estimate to see what divisions of strategies would work best
      (might have to do more iterations of Amdahl's law depending on the amount
      of strategies you can use).
  

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Midterm Questions from 2011 (1)}


  For each of the following situations, {\bf name an appropriate parallelization
  pattern and the granularity at which you would apply it, explain the necessary
  communication, and explain why your pattern is appropriate}.
  \begin{itemize}
  \item build system, e.g. parallel make
    \begin{itemize}
      \item<2-> Multiple independent tasks, at a per-file granularity
    \end{itemize}
  \item optical character recognition system
    \begin{itemize}
      \item<2-> Pipeline of tasks
      \item<2-> 2 tasks - finding characters and analyzing them
    \end{itemize}
  \end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Midterm Questions from 2011 (2)}


  {\bf Give a concrete example} where you would use the following
  parallelization patterns. {\bf Explain} the granularity at which you'd apply
  the pattern.
  \begin{itemize}
    \item single task, multiple threads:
    \begin{itemize}
      \item<2-> Computation of a mathematical function with independent
        sub-formulas.
    \end{itemize}
    \item producer-consumer (no rendering frames, please):
    \begin{itemize}
      \item<2-> Processing of stock-market data:
        a server might generate raw financial data (quotes) for a
        particular security. The server would be the producer. Several clients
        (or consumers) may take the raw data and use them in different ways, e.g. by computing means, generating charts, etc.
    \end{itemize}
  \end{itemize}
 
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{How To Parallelize Code: Strategy}
  
  
  Four-step outline:
  \begin{enumerate}
    \item Profile the code.
    \item Look at hotspots; find and optimize dependencies; parallelize dependency chains; change the algorithm if you can.
    \item Estimate benefits.
    \item If not good enough, step back and try higher level of abstraction.
  \end{enumerate}
  Always try to minimize synchronization.
  

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Low-level Implementation Tactic: Thread Pools}
  
  
    Instead of creating threads, destroying them and recreating them, you
      can use a \structure{thread pool}.
    \begin{itemize}
    \item It creates $n$ threads; you just {\tt push} work onto them.
    \end{itemize}

  \begin{center}
    \includegraphics[scale=0.4]{images/thread-pool}
  \end{center}

  \begin{itemize}
    \item Only question is: How many threads should you create? (Experiment to find out).
    \item Implementation from GLib: {\tt GThreadPool}.
  \end{itemize}
  

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}

